{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Table of Contents <ol> <li>Introduction</li> <li>Terminology     <ul> <li>General Terminology         <ul> <li>Programming Languages</li> <li>Big Data</li> <li>Data Engineering</li> <li>Data Analytics</li> <li>Data Visualization</li> <li>Data Science</li> </ul> </li> <li>Fabric Terminology         <ul> <li>Get Data</li> <li>Store Data</li> <li>Prepare Data</li> <li>Analyze and Train Data</li> <li>OneLake</li> </ul> </li> <li>Finance Terminology         <ul> <li>Financial and Accounting Terms</li> <li>Finance KPIs</li> </ul> </li> <li>Calendar Terminology         <ul> <li>Calendar Systems and Accounting Period</li> </ul> </li> </ul> </li> <li>Fabric Tools     <ul> <li>Lakehouse</li> <li>SQL Analytics Endpoint</li> <li>Notebook</li> <li>Semantic Model</li> <li>Warehouse</li> <li>SQL Database</li> <li>Dataflow</li> <li>Pipelines</li> <li>ML Model</li> <li>Reports</li> </ul> </li> <li>Programming Tools     <ul> <li>Git</li> </ul> </li> <li>PowerBI     <ul> <li>Best Dashboard Design Practices</li> <li>VertiPaq Engine</li> </ul> </li> <li>Architecture     <ul> <li>Star Schema Design</li> <li>Dimension Design</li> <li>Fact Design</li> <li>Snowflake Schema</li> <li>Data Model Relationships</li> <li>Naming Convention</li> </ul> </li> <li>Automation     <ul> <li>STRAVIS Automation</li> <li>SAP Automation</li> </ul> </li> <li>Inforiver</li> <li>References</li> </ol>"},{"location":"introduction/","title":"Introduction","text":"<p>This documentation contains information on the various tools that are used in the Finance department. To use this documentation, go to the Table of Contents to begin navigating your way through. To access the Table of Contents from any point in the documentation, click on the HOYA Surgical Optics logo.</p> <p>In each page, the left navigation bar shows you all the pages in the topic. The right navigation bar shows you the sections in the current page, and the section that you are currently reading. </p>"},{"location":"architecture/dimension_design/","title":"Dimension Design","text":""},{"location":"architecture/dimension_design/#denormalization-vs-normalization","title":"Denormalization vs. Normalization","text":"<ul> <li> <p>It is almost alway sthe case that dimension tables should be denormalized. </p> </li> <li> <p>Normalization: term used to describe data that's stored in a way that reduces repetitious data</p> </li> <li> <p>Denormalization: term used to define where precomputed redundant data exists.</p> <ul> <li> <p>Redundant data exists typically due to the storage of hierarchies, meaning that hierarchies are flattened.</p> </li> <li> <p>E.g.: a product dimension could store subcategory (and its related attributes) and category (and its related attributes).</p> </li> </ul> </li> <li> <p>Because dimensions are generally small (wen compared to fact tables), the cost of storing redundant data is almost always outweighed by the improved query performance and usability.</p> </li> </ul>"},{"location":"architecture/dimension_design/#snowflake-dimensions","title":"Snowflake Dimensions","text":"<ul> <li> <p>One exception to denormalization is to design a snowflake dimension. </p> <ul> <li>A snowflake dimension is normalized, and it stores the dimension data across several related tables.</li> </ul> </li> </ul> Snowflake Dimension (Microsoft, 2025) <ul> <li> <p>Implement a snowflake dimension when:</p> <ul> <li> <p>The dimension is extremely large and storage costs outweigh the need for high query performance. </p> </li> <li> <p>You need keys to relate the dimension to higher-grain facts. For example, the sales fact table stores rows at product level, but the sales target fact table stores rows at subcategory level.</p> </li> <li> <p>You need to track historical changes at higher levels of granularity.</p> </li> </ul> </li> <li> <p>A hierarhcy in a Power BI semantic model can only be based on columns from a single semantic model table. Therefore, a snowflake dimension should deliver a denormalized result by using a view that joins the snowflake tables together.</p> </li> </ul>"},{"location":"architecture/dimension_design/#hierarchies","title":"Hierarchies","text":"<ul> <li> <p>Hierarchies enable exploring data at distinct levels of summarization. </p> <ul> <li>For example, the initial view of a matrix visual might show yearlly sales, and the report consumer can choose to drill down to reveal quarterly and monthly sales.</li> </ul> </li> <li> <p>3 ways to store a hierarchy in a dimension:</p> <ul> <li> <p>Columns from a single, denormalized dimension.</p> </li> <li> <p>A snowflake dimension, which comprises multiple related tables.</p> </li> <li> <p>A parent-child (self-referencing) relationship in a dimension.</p> </li> </ul> </li> <li> <p>Hierarchies can be balanced, unbalanced, or ragged.</p> </li> </ul>"},{"location":"architecture/dimension_design/#balanced-hierarchies","title":"Balanced Hierarchies","text":"<ul> <li> <p>Most common type of hierarchy.</p> </li> <li> <p>A balanced hierarchy has the same number of levels.</p> <ul> <li>E.g.: A calendar hierarchy in a date dimension that comprises leels for year, quarter, month, and date.</li> </ul> </li> <li> <p>The following diagram depicts a balanced hierarchy for sales regions. It comprises two levels, which are sales region group and sales region.</p> <p> Balanced Hierarchy (Microsoft, 2025) </p> </li> <li> <p>Levels of a balanced hierarchy are either based on columns from a single, denormalized dimension, or from tables that form a snowflake dimension.</p> <ul> <li>When based on a single, denormalized dimension, the columns that represent the higher levels contain redundant data.</li> </ul> </li> <li> <p>For balanced hierarchies, facts always relate to a single level of the hierarchy, which is typically the lowest level.</p> <ul> <li> <p>The facts can be aggregated to the highest level of hierarchy. </p> </li> <li> <p>Facts can relate to any level, which is determined by the grain of the fact table. </p> </li> <li> <p>E.g.: the sales fact table might be stored at the date level, while the sales target fact table might be stored at quarter level.</p> </li> </ul> </li> </ul>"},{"location":"architecture/dimension_design/#unbalanced-hierarchies","title":"Unbalanced hierarchies","text":"<ul> <li> <p>An unbalanced hierarchy has levels based on a parent-child relationship. </p> <ul> <li>The number of levels in an unbalanced hierarchy is determined by the dimension rows, and not specific dimension table columns.</li> </ul> </li> <li> <p>A common example of an unbalanced hierarchy is an employe hierarchy where each row in an employee dimension relates to a reporting manager row in the same table.</p> <ul> <li> <p>In this case, any employee can be a manager with reporting employees.</p> </li> <li> <p>Naturally, some branches of the hierarchy will have more levels than others.</p> </li> </ul> </li> <li> <p>The following diagram comprises four levels, and each member in the hierarchy is a salesperson. Salespeople have a different number of ancestors in the hierarchy according to who they report to.</p> <p> Unbalanced Hierarchy (Microsoft, 2025) </p> </li> <li> <p>Facts always relate to the dimension grain.</p> <ul> <li> <p>E.g.: sales facts relate to different salespeople, who have different reporting structures.</p> <ul> <li> <p>The dimension table would have a surrogate key (named <code>Salesperson_SK</code>) and a <code>ReportsTo_Salespersom_FK</code> foreign key column, which referecnes the primary key column.</p> </li> <li> <p>Each salesperson without anyone to manage isn't necessarily at the lowest level of any branch of the hierarchy. </p> </li> <li> <p>When they're not at the lowest level, a salesperson might sell products and have reporting salespeople who also sell products. So, the rollup of fact data must consider the individual salesperson and all their descendants.</p> </li> </ul> </li> </ul> </li> <li> <p>Querying parent-child hierarchies can be complex and slow, especially for large dimensions.</p> <ul> <li>In this case naturalizing the hierarchy is recommended. In this instance, naturalize means to transform and store the hierarchy levels in the dimension as columns.</li> </ul> </li> </ul>"},{"location":"architecture/dimension_design/#ragged-hierarchies","title":"Ragged Hierarchies","text":"<ul> <li> <p>A ragged hierarchy occurs because the parent of a member in the hierarchy exists at a level that's not immediately above it. </p> <ul> <li>In these cases, missing level values repeat the value of the parent.</li> </ul> </li> <li> <p>Consider an example of a balanced geography hierarchy. A ragged hierarchy exists when a country/region has no states or provinces. </p> <ul> <li>For example, New Zealand has neither states nor provinces. So, when you insert the New Zealand row, you should also store the country/region value in the <code>StateProvince</code> column. </li> </ul> <p> Ragged Hierarchy (Microsoft, 2025) </p> </li> </ul>"},{"location":"architecture/dimension_design/#calendar-and-time","title":"Calendar and Time","text":"<ul> <li> <p>Almost without exception, fact tables store measures at specific points in time. To support analysis by date (and possibly time), there must be calendar (date and time) dimensions.</p> </li> <li> <p>It's uncommon that a source system would have calendar dimension data, so it must be generated in the data warehouse. Typically, it's generated once, and if it's a calendar dimension, it's extended with future dates when needed.</p> </li> </ul>"},{"location":"architecture/dimension_design/#date-dimension","title":"Date Dimension","text":"<ul> <li> <p>Most common dimension used for analysis. It stores one row per date, and it supports the common requirement to filter or group by specific periods of dates, like years, quarters, or months.</p> <ul> <li>Date dimension should not go down to the time of day. If a day analysis is required, you should have both a date dimension and a time dimension. </li> </ul> </li> <li> <p>The natural key of the date dimension should use the date data type. The surrogate key should store the date by using <code>YYYYMMDD</code> format and the int data type.</p> <ul> <li>Storing <code>YYYYMMDD</code> as an int data type is not only efficient and sorted numerically, but it also conforms to the unambiguous International Standards Organization (ISO) 8601 date format.</li> </ul> </li> </ul>"},{"location":"architecture/dimension_design/#conformed-dimensions","title":"Conformed Dimensions","text":"<ul> <li> <p>Conformed dimensions relate to many fact tables, and so they're shared by multiple stars in a dimenional model. They deliver consistency and can help you to reduce ongoing development and maintenance.</p> </li> <li> <p>For example, it's typical that fact tables store at least one date dimension key (because activity is almost always recorded by date and/or time). For that reason, a date dimension is a common conformed dimension. You should therefore ensure that your date dimension includes attributes relevant for the analysis of all fact tables.</p> </li> <li> <p>The following diagram shows the <code>Sales</code> fact table and the <code>Inventory</code> fact table. Each fact table relates to the <code>Date</code> dimension and <code>Product</code> dimension, which are conformed dimensions.</p> <p> Conformed Dimensions (Microsoft, 2025) </p> </li> </ul>"},{"location":"architecture/dimension_design/#role-playing-dimensions","title":"Role-playing Dimensions","text":"<ul> <li> <p>When a dimension is referenced multiple times in a fact table, it's known as a role-playing dimension.</p> </li> <li> <p>For example, when a sales fact table has order date, ship date, and delivery date dimension keys, the date dimension relates in three ways. Each way represents a distinct role, yet there's only one physical date dimension.</p> </li> <li> <p>The following diagram depicts a <code>Flight</code> fact table. The <code>Airport</code> dimension is a role-playing dimension because it's related twice to the fact table as the <code>Departure Airport</code> dimension and the <code>Arrival Airport</code> dimension.</p> <p> Role-playing Dimensions (Microsoft, 2025) </p> </li> </ul>"},{"location":"architecture/dimension_design/#junk-dimensions","title":"Junk Dimensions","text":"<ul> <li> <p>Useful when there are many independent dimensions, especially when they comprise a few attributes (perhaps one), and when these attributes have low cardinality (few values). </p> </li> <li> <p>The objective of a junk dimension is to consolidate many small dimensions into a single dimension. This design approach can reduce the number of dimensions, and decrease the number of fact table keys and thus fact table storage size. </p> <ul> <li>They also help to reduce Data pane clutter because they present fewer tables to users.</li> </ul> </li> <li> <p>A junk dimension table typically stores the Cartesian product of all dimension attribute values, with a surrogate key attribute.</p> </li> <li> <p>Good candidates include flags and indicators, order status, and customer demographic states (gender, age group, and others).</p> </li> <li> <p>The following diagram depicts a junk dimension named <code>Sales Status</code> that combines order status values and delivery status values.</p> <p> Junk Dimension (Microsoft, 2025) </p> </li> </ul>"},{"location":"architecture/dimension_design/#degenerate-dimensions","title":"Degenerate Dimensions","text":"<ul> <li> <p>A degenerate dimension can occur when the dimension is at the same grain as the related facts.</p> <ul> <li>A common example of a degenerate dimension is a sales order number dimension that relates to a sales fact table. Typically, the invoice number is a single, non-hierarchical attribute in the fact table. So, it's an accepted practice not to copy this data to create a separate dimension table.</li> </ul> </li> <li> <p>The following diagram depicts a <code>Sales Order</code> dimension that's a degenerate dimension based on the <code>SalesOrderNumber</code> column in a sales fact table. This dimension is implemented as a view that retrieves the distinct sales order number values.</p> <p> Degenerate Dimension (Microsoft, 2025) </p> </li> <li> <p>From a Power BI semantic modeling perspective, a degenrate dimension can be created as a separate table using Power Query.</p> </li> </ul>"},{"location":"architecture/dimension_design/#outrigger-dimensions","title":"Outrigger Dimensions","text":"<ul> <li> <p>Outrigger dimension: when a dimension table relates to other dimension tables.</p> <ul> <li>An outrigger dimension can help to conform and reuse definitions in the dimensional model.</li> </ul> </li> <li> <p>For example, you could create a geography dimension that stores geographic locations for every postal code. That dimension could then be referenced by your customer dimension and salesperson dimension, which would store the surrogate key of the geography dimension. That way, customers and salespeople could then be analyzed by using consistent geographic locations.</p> </li> <li> <p>The following diagram depicts a <code>Geography</code> dimension that's an outrigger dimension. It doesn't relate directly to the <code>Sales</code> fact table. Instead, it's related indirectly via the <code>Customer</code> dimension and the Salesperson dimension.</p> <p> Outrigger Dimension (Microsoft, 2025) </p> </li> <li> <p>Consider that the date dimension can be used as an outrigger dimension when other dimension table attributes store dates. For example, the birth date in a customer dimension could be stored by using the surrogate key of the date dimension table.</p> </li> </ul>"},{"location":"architecture/dimension_design/#multivalued-dimensions","title":"Multivalued Dimensions","text":"<ul> <li> <p>Multivalued dimension: when a dimension attribute must store multiple values.</p> </li> <li> <p>To implement a multivalued dimension, create a bridge table.</p> <ul> <li>A bridge table stores a many-to-many relationship between entities.</li> </ul> </li> <li> <p>For example, consider there's a salesperson dimension, and that each salesperson is assigned to one or possibly more sales regions. In this case, it makes sense to create a sales region dimension. That dimension stores each sales region only once. A separate table, known as the bridge table, stores a row for each salesperson and sales region relationship. Physically, there's a one-to-many relationship from the salesperson dimension to the bridge table, and another one-to-many relationship from the sales region dimension to the bridge table. Logically, there's a many-to-many relationship between salespeople and sales regions.</p> </li> <li> <p>In the following diagram, the <code>Account</code> dimension table relates to the <code>Transaction</code> fact table. Because customers can have multiple accounts and accounts can have multiple customers, the <code>Customer</code> dimension table is related via the <code>Customer Account</code> bridge table.</p> <p> Multivalued Dimension (Microsoft, 2025) </p> </li> </ul> <p> Star Schema Design</p> <p>Fact Design </p>"},{"location":"architecture/fact_design/","title":"Fact Design","text":""},{"location":"architecture/fact_design/#fact-table-size","title":"Fact Table Size","text":"<ul> <li> <p>Fact tables vary in size. THeir size corresponds to the dimensionality, granularity, number of measures, and amount of history.</p> </li> <li> <p>In comparison to dimension tables, fact tables are more narrow (fewer columns) but big or even immense in terms of rows (in excess of billions).</p> </li> </ul>"},{"location":"architecture/fact_design/#fact-table-types","title":"Fact Table Types","text":""},{"location":"architecture/fact_design/#transaction-fact-tables","title":"Transaction Fact Tables","text":"<ul> <li> <p>Stores business events or transactions.</p> </li> <li> <p>Each row stores facts in terms of dimension keys and measures, and optionally other attributes. </p> <ul> <li>All the data is fully known when inserted, and it never changes.</li> </ul> </li> <li> <p>Tyoically, transaction fact tables store facts at the lowest possible level of granularity, and they contain measures that are additive across all dimensions. A sales fact table that stores every sales order line is a good example of a transaction fact table.</p> </li> </ul>"},{"location":"architecture/fact_design/#periodic-snapshot-fact-tables","title":"Periodic Snapshot Fact Tables","text":"<ul> <li> <p>Stores measurements at a predefined time, or specific intervals.</p> <ul> <li> <p>It provides a summary of key metrics or performance indicators over time, and so it's useful for trend analysis and monitoring change over time. </p> </li> <li> <p>Measures are always semi-additive.</p> </li> </ul> </li> <li> <p>An inventory fact table is a good example of a periodic snapshot table. It's loaded every day with the end-of-day stock balance of every product.</p> </li> <li> <p>Periodic snapshot tables can be used instead of a transaction fact table when recording large volumes of transactions is expensive, and it doesn't support any useful analytic requirement. For example, there might be millions of stock movements in a day (which could be stored in a transaction fact table), but your analysis is only concerned with trends of end-of-day stock levels.</p> </li> </ul>"},{"location":"architecture/fact_design/#accumulating-snapshot-fact-tables","title":"Accumulating Snapshot Fact Tables","text":"<ul> <li> <p>Stores measurements that accumulate across a well-defined period or workflow.</p> <ul> <li>It often records the state of a business process at distinct stages or milestones, which might take days, weeks, or even months to complete.</li> </ul> </li> <li> <p>A fact row is loaded soon after the first event in a process, and then the row is updated in a predictable sequence every time a milestone event occurs. Updates continue until the process completes.</p> </li> </ul>"},{"location":"architecture/fact_design/#measure-types","title":"Measure Types","text":"<ul> <li>Measures are typically numeric, and commonly additive. However, some measures can't always be added. These measures are categorized as either semi-additive or non-additive.</li> </ul>"},{"location":"architecture/fact_design/#additive-measures","title":"Additive Measures","text":"<ul> <li> <p>Can be summed across any dimension.</p> </li> <li> <p>E.g.: order quantity and sales revenue are additive measures (providing revenue is recorded for a single currency).</p> </li> </ul>"},{"location":"architecture/fact_design/#semi-additive-measures","title":"Semi-additive Measures","text":"<ul> <li> <p>Can be summed across certain dimensions only.</p> </li> <li> <p>Some examples:</p> <ul> <li> <p>Any measure in a periodic snapshot fact table can't be summed across other time periods. For example, you shouldn't sum the age of an inventory item sampled nightly, but you could sum the age of all inventory items on a shelf, each night.</p> </li> <li> <p>A stock balance measure in an inventory fact table can't be summed across other products.</p> </li> <li> <p>Sales revenue in a sales fact table that has a currency dimension key can't be summed across currencies.</p> </li> </ul> </li> </ul>"},{"location":"architecture/fact_design/#non-additive-measures","title":"Non-additive Measures","text":"<ul> <li> <p>Can't be summed across any dimension.</p> <ul> <li>E.g.: a temperature reading, which by its nature doesn't make sense to add to other readings.</li> </ul> </li> <li> <p>Other examples include rates, like unit prices, and ratios. However, it's considered a better practice to store the values used to compute the ratio, which allows the ratio to be calculated if needed. For example, a discount percentage of a sales fact could be stored as a discount amount measure (to be divided by the sales revenue measure). Or, the age of an inventory item on the shelf shouldn't be summed over time, but you might observe a trend in the average age of inventory items.</p> </li> <li> <p>While some measures can't be summed, they're still valid measures. They can be aggregated by using count, distinct count, minimum, maximum, average, and others. Also, non-additive measures can become additive when they're used in calculations. For example, unit price multiplied by order quantity produces sales revenue, which is additive.</p> </li> </ul>"},{"location":"architecture/fact_design/#factless-fact-tables","title":"Factless Fact Tables","text":"<ul> <li> <p>When a fact table doesn't contain any measure columns.</p> </li> <li> <p>A factless fact table typically records events or occurrences, like students attending class. From an analytics perspective, a measurement can be achieved by counting fact rows.</p> </li> </ul>"},{"location":"architecture/fact_design/#aggregate-fact-tables","title":"Aggregate Fact Tables","text":"<ul> <li>Represents a rollup of a base fact table to a lower dimensionality and/or higher granularity. Its purpose is to accelerate query performance for commonly queried dimensions.</li> </ul> <p> Dimension Design</p> <p>Snowflake Schema </p>"},{"location":"architecture/relationships/","title":"Data Model Relationships","text":""},{"location":"architecture/relationships/#what-is-a-relationship-in-a-data-model","title":"What is a Relationship in a Data Model?","text":"<ul> <li> <p>Relationship: allows you to filter or group data from one table by columns in a second table. To do this, you match equivalent columns (or keys) from each table.</p> </li> <li> <p>Relationships are integral for you to shift from a set of disconnected data points to a functional model of a business process.</p> </li> </ul>"},{"location":"architecture/relationships/#types-of-relationships","title":"Types of relationships","text":"<ul> <li> <p>Relationships are represented by arrows between tables in your model view.</p> <ul> <li>You create a relationship between two columns, specifying the direction of the relationship and whether it's active, or not. </li> </ul> </li> <li> <p>These in-memory data structures are conceptually similar to a SQL index, but differ in that they're a result of how the VertiPaq compresses and stores data in memory.</p> </li> <li> <p>The diagram below depicts the typical types of relationships in a Power BI data model.</p> <p> Relationships (Tabular Editor, 2024) </p> </li> </ul>"},{"location":"architecture/relationships/#one-to-one-relationships","title":"One-to-One Relationships","text":"<ul> <li> <p>Each record in the first table can have only one matching record in the second table, and each record in the second table can only have one matching record in the first table.</p> <ul> <li>Usually, information like this is stored in the same table, so this type of relationship is not common.</li> </ul> </li> <li> <p>Use cases: </p> <ul> <li> <p>Divide a table with many fields</p> </li> <li> <p>Isolate part of a table for security reasons</p> </li> <li> <p>Store information that applies only to a subset of the main table</p> </li> </ul> </li> <li> <p>When this relationship is identified, both tables must share a common field.</p> </li> </ul>"},{"location":"architecture/relationships/#one-to-many-relationships","title":"One-to-Many Relationships","text":"<ul> <li> <p>A one-to-many relationship in database design represents a connection where one entity can be linked to multiple instances of another entity, while each instance of the second enity can be linked to one instance of the first.</p> </li> <li> <p>Example: an order tracking database that includes a Customers table and an Orders table. A customer can place any number of orders. It follows that for any customer represented in the Customers table, there might be many orders represented in the Orders table. </p> </li> <li> <p>To represent this relationship in database design:</p> <ul> <li> <p>Take the primary key on the \"one\" side of the relationship and add it as an additional field or fields to the table on the \"many\" side of the relationship.</p> </li> <li> <p>In the previous example, you add a new field - the ID field from the Customers table - to the Orders table and name it Customer ID.</p> <ul> <li>Access can then use the Customer ID number in the Orders table to locate the correct customer for each order.</li> </ul> </li> </ul> </li> </ul>"},{"location":"architecture/relationships/#many-to-many-relationship","title":"Many-to-Many Relationship","text":"<ul> <li> <p>Occurs when one or more records in one table are associated with one or more records in another table.</p> </li> <li> <p>Example:</p> <ul> <li> <p>Relationship between a Products table and an Orders table.</p> </li> <li> <p>A single order can include more than one product. On the other hand, a single product can appear on many orders. Therefore, for each record in the orders table, there can be many records in the Products table. In addition, for each record in the Products table, there can be many records in the Orders table.</p> </li> </ul> </li> <li> <p>Note that to detect existing many-to-many relationships between your tables, it is important that you consider both sides of the relationship.</p> </li> <li> <p>To represent a many-to-many relationship:</p> <ul> <li> <p>A third table must be created, often called a junction table, that breaks down the many-to-many relationship into two one-to-many relationships.</p> </li> <li> <p>You insert the primary key from each of the two tables into the third table.</p> <ul> <li>As a result, the thirs table records each occurrence, or instance, of the relationship.</li> </ul> </li> <li> <p>From the example, the Orders table and the Products table have a many-to-many relationship that is defined by creating two one-to-many relationships to the Order Details table. One order can have many products, and each product can appear on many orders.</p> </li> </ul> </li> </ul>"},{"location":"architecture/relationships/#questions-to-ask-when-creating-relationships","title":"Questions to Ask When Creating Relationships","text":"<ul> <li> <p>Is the cardinality what you expect? Most relationships should be one-to-many from the dimension to the fact table. If you intend to use a different cardinality, such as many-to-many or one-to-one, ensure that you are aware of the consequences for your model performance and query results.</p> </li> <li> <p>Is the directionality what you expect? Most relationships should be uni-directional from the dimension to the fact table. If you intend to use bi-directional relationships, ensure that you avoid ambiguity and are aware of the consequences for model performance and query results.</p> </li> <li> <p>Is the relaionship active? Most physical relationships should be active, unless you are using role-playing dimensions and activating the relationship by using the USERRELATIONSHIP DAX function inside of CALCULATE.</p> </li> <li> <p>Are you missing keys?: Relationships should ensure referential integrity to be valid, meaning that all of the values in the fact table (on the \"to\" or \"many\" side of a one-to-many relationship) have a corresponding value in the dimension table (on the \"from\" or \"one\" side of a one-to-many relationship). If you're missing keys, </p> </li> </ul>"},{"location":"architecture/relationships/#creating-relationships-in-power-bi-desktop","title":"Creating Relationships in Power BI Desktop","text":"<ol> <li> <p>Open up Power BI Desktop, and connect to the semantic model that you wish to create relationships for.</p> </li> <li> <p>From the left pane, click on Model view.</p> <p> </p> </li> <li> <p>From here, you can create a relationship in Power BI Desktop in three ways:</p> <ul> <li> <p>Drag-and-drop in model view: Select the model view, find the tables, and drag the \"from\" ket column to the \"to\" key column. You can double-click any relationship to edit its properties.</p> </li> <li> <p>Relationship editor: Select Manage Relationships from the top of the model view, and select New... to create a new relationship. You can also manage existing relationships.</p> </li> <li> <p>Automatically detect relationships: Power BI Desktop will by default automatically detect relationships between similarly named and typed columns. You can adjust this behaviour from the Power BI Desktop options. Disabling automatic relationship detection is generally recommended when you're developing your semanic model by using Power BI Desktop, because it can result in unexpected relationships when you add new columns or modify the name and data types of existing columns.</p> </li> </ul> </li> <li> <p>Create relationships, ideally using the star schema concept.</p> </li> </ol>"},{"location":"architecture/relationships/#creating-relationships-in-power-bi-service","title":"Creating Relationships in Power BI Service","text":"<ol> <li> <p>In Power BI Service, open the workspace that you wish to create your semantic model in. If there is no warehouse available to build the semantic model on, create the warehouse.</p> </li> <li> <p>Select the Reporting tab of the ribbon and choose New semantic model. This enables you to create a new semantic model using only speciic tables and views from your data warehouse, for use by data teams and the business to build reports.</p> </li> <li> <p>Navigate back to your workspace and confirm that you see your new semantic model. Notice that the type is Semantic Model, as opposed to the Semantic model (default) that is automatically created when you create a data warehouse.</p> <ul> <li>The default semantic model inherits the business logic from the parent Lakehouse or Warehouse. A semantic model that you create yourself is a custom model that you can design and modify according to your specific needs and preferences.</li> </ul> </li> <li> <p>Select Open data model from the ribbon.</p> </li> <li> <p>Follow step 3 of Creating Relationships in Power BI Desktop.</p> </li> </ol> <p> Snowflake Schema</p> <p>Naming Convention </p>"},{"location":"architecture/snowflake/","title":"Snowflake Schema","text":"<ul> <li> <p>A multi-dimensional dat amodel that is an extension of star schema, where dimension tables are broken down into subdimensions. </p> </li> <li> <p>Commonly used for business intelligence and reporting in OLAP data warehouses, data marts, and relational databases.</p> </li> <li> <p>It's called a snowflake schema because its entity-relationship diagram (ERD) looks like a snowflake, as seen below.</p> <p> Snowflake Schema (Databricks) </p> </li> <li> <p>The snowflake structure materialized when thedimensions of a star schema are detailed and highly structured having several levels of relationship and the child tables have multiple parent tables.</p> <ul> <li>The snowflake effect affects only the dimension tables and does not affect the fact tables.</li> </ul> </li> <li> <p>The dimension tables are normalized into multiple related tables, creating a hierarchical or \"snowflake\" structure.</p> </li> <li> <p>The fact table is still located at the center of the schema, surrounded by the dimension tables. However, each dimension table is further broken down into multiple related tables, creating a hierarchical structure that resembles a snowflake.</p> </li> </ul>"},{"location":"architecture/snowflake/#what-is-snowflaking","title":"What is Snowflaking?","text":"<ul> <li> <p>It is the result of further expansion and normalization of the dimension table. </p> <ul> <li> <p>In other words, a dimension table is said to be snowflaked if the low-cardinality sttribute of the dimensions has been divided into separate normalized tables.</p> </li> <li> <p>These tables are then joined to the original dimension table with referential constraints (foreign key constraint).</p> </li> </ul> </li> </ul>"},{"location":"architecture/snowflake/#when-to-use-snowflake-schemas","title":"When to Use Snowflake Schemas","text":"<ul> <li> <p>Storage optimization is critical.</p> </li> <li> <p>Data consistency and maintainability are priorities.</p> </li> <li> <p>there are complex, hierarchical dimensions.</p> </li> </ul>"},{"location":"architecture/snowflake/#snowflake-schema-vs-star-schema","title":"Snowflake Schema vs. Star Schema","text":"<ul> <li> <p>Like star schemas, snowflake schemas have a central fact table which is connected to multiple dimension tables via foreign keys. However, the main difference is that they are more normalized than star schemas.</p> </li> <li> <p>Snowflake schemas offer more storage efficiency, due to their tighter adherence to high normalization standards, but query performance is not as good as with more denormalized data models. Denormalized data models like star schemas have more data redundancy (duplication of data), which makes query performance faster at the cost of duplicated data.</p> </li> </ul>"},{"location":"architecture/snowflake/#characteristics-of-snowflake-schema","title":"Characteristics of Snowflake Schema","text":"<ul> <li> <p>The snowflake schema uses small disk space.</p> </li> <li> <p>It is easy to implement the dimension that is added to the schema.</p> </li> <li> <p>There are multiple tables, so performance is reduced.</p> </li> <li> <p>The dimension table consists of two or more sets of attributes that define information at different grains.</p> </li> <li> <p>The sets of attributes of the same dimension table are populated by different source systems.</p> </li> </ul>"},{"location":"architecture/snowflake/#features-of-the-snowflake-schema","title":"Features of the Snowflake Schema","text":"<ul> <li> <p>Normalization: The snowflake schema is a normalized design, which means that data is organized into multiple related tables. This reduces data redundancy and improves data consistency.</p> </li> <li> <p>Hierarchical Structure: The snowflake schema has a hierarchical structure that is organized around a central fact table. The fact table contains the measures or metrics of interest, and the dimension tables contain the attributes that provide context to the measures.</p> </li> <li> <p>Multiple Levels: The snowflake schema can have multiple levels of dimension tables, each related to the central fact table. This allows for more granular analysis of data and enables users to drill down into specific subsets of data.</p> </li> <li> <p>Joins: The snowflake schema typically requires more complex SQL queries that involve multiple tables joins. This can impact performance, especially when dealing with large data sets.</p> </li> <li> <p>Scalability: The snowflake schema is scalable and can handle large volumes of data. However, the complexity of the schema can make it difficult to manage and maintain.</p> </li> </ul>"},{"location":"architecture/snowflake/#advantages-of-snowflake-schema","title":"Advantages of Snowflake Schema","text":"<ul> <li> <p>It provides structured data which reduces the problem of data integrity.</p> </li> <li> <p>It uses small disk space because data are highly structured</p> </li> </ul>"},{"location":"architecture/snowflake/#disadvantages-of-snowflake-schema","title":"Disadvantages of Snowflake Schema","text":"<ul> <li> <p>Snowflaking reduces space consumed by dimension tables but compared with the tables entire data warehouse the saving is usually insignificant.</p> </li> <li> <p>Avoid snowflaking or normalization of a dimension table, unless required and appropriate.</p> </li> <li> <p>Do not snowflake hierarchies of dimension table into separate tables. Hierarchies should belong to the dimension table only and should never be snowflakes.</p> </li> <li> <p>Multiple hierarchies that can belong to the same dimension have been designed at the lowest possible detail.</p> </li> </ul> <p> Fact Design</p> <p>Data Model Relationships </p>"},{"location":"architecture/star_schema/","title":"Star Schema Design","text":"<ul> <li> <p>A dimensional modelling design technique adopted by relational data warehouses.</p> </li> <li> <p>It's a recommended design approach to take when creating a Fabric Warehouse.</p> </li> <li> <p>A star schema comprises fact tables and dimension tables.</p> <ul> <li> <p>Dimension tables: describe the entities relevant to the organization and analytics requirements. Broadly, they represent the things that you model. Things could be products, people, places, or any other concept, including date and time.</p> </li> <li> <p>Fact tables: store measurements associated with observations or events. They can store sales orders, stock balances, exchange rates, temperature readings, and more. Fact tables contain dimension keys together with granular values that can be aggregated.</p> </li> </ul> </li> <li> <p>It is optimized for analytic query workloads, and is thus considered a prerequisite for enterprise Power BI semantic models.</p> <ul> <li> <p>Analytic queries are concerned with filtering, grouping, sorting, and summarizing data.</p> </li> <li> <p>Fact data is summarized within the context of filters and groupings of the related dimension tables.</p> </li> </ul> </li> <li> <p>The reason why it's called a star schema is because a fact table forms the center of a star while the related dimension tables form the points of the star.</p> <p> Star Schema (Microsoft, 2025) </p> </li> <li> <p>A star schema often contains multipe fact tables, and therefore multiple stars.</p> </li> <li> <p>A well-desgned star schema delivers high performance (relational) queries because of fewer table joins, and the higher likelihood of useful indexes. Also, a star schema often requires low maintenance as the data warehouse design evolves. For example, adding a new column to a dimension table to support analysis by a new attribute is a relatively simple task to perform. As is adding new facts and dimensions as the scope of the data warehouse evolves.</p> </li> </ul>"},{"location":"architecture/star_schema/#dimension-table-structure","title":"Dimension Table Structure","text":"<ul> <li> <p>To easily identify dimension tables, their names will be prefixed with <code>Dim</code>.</p> </li> <li> <p>To describe the structure of a dimension table, consider the following example of a salesperson dimension table named <code>Dim_Salesperson</code>. This example applies good design practices.</p> </li> </ul> SQL<pre><code>CREATE TABLE Dim_Salesperson\n(\n    --Surrogate key\n    Salesperson_SK INT NOT NULL,\n\n    --Natural key(s)\n    EmployeeID VARCHAR(20) NOT NULL,\n\n    --Dimension attributes\n    FirstName VARCHAR(20) NOT NULL,\n    &lt;...&gt;\n\n    --Foreign key(s) to other dimensions\n    SalesRegion_FK INT NOT NULL,\n    &lt;...&gt;\n\n    --Historical tracking attributes (SCD type 2)\n    RecChangeDate_FK INT NOT NULL,\n    RecValidFromKey INT NOT NULL,\n    RecValidToKey INT NOT NULL,\n    RecReason VARCHAR(15) NOT NULL,\n    RecIsCurrent BIT NOT NULL,\n\n    --Audit attributes\n    AuditMissing BIT NOT NULL,\n    AuditIsInferred BIT NOT NULL,\n    AuditCreatedDate DATE NOT NULL,\n    AuditCreatedBy VARCHAR(15) NOT NULL,\n    AuditLastModifiedDate DATE NOT NULL,\n    AuditLastModifiedBy VARCHAR(15) NOT NULL\n);\n</code></pre>"},{"location":"architecture/star_schema/#primary-key","title":"Primary Key","text":"<ul> <li> <p>Primary Key: a column or columns in a database table with values  that uniquely identify each row or record.</p> <ul> <li>E.g.: an employee ID column could be a primary key in a table of employee information.</li> </ul> </li> <li> <p>Characteristics:</p> <ul> <li> <p>No null values</p> </li> <li> <p>No duplicate values</p> </li> <li> <p>May take the form of an existing column in a table (natural key) or be added as a new column (surrogate key)</p> </li> <li> <p>May inclue a single column or multiple columns (as a composite primary key)</p> </li> </ul> </li> <li> <p>Create a Microsoft Fabric Warehouse table with a primary key:</p> SQL<pre><code>CREATE TABLE PrimaryKeyTable (c1 INT NOT NULL, c2 INT);\n\nALTER TABLE PrimaryKeyTable ADD CONSTRAINT PK_PrimaryKeyTable PRIMARY KEY NONCLUSTERED (c1) NOT ENFORCED;\n</code></pre> </li> </ul>"},{"location":"architecture/star_schema/#surrogate-key","title":"Surrogate Key","text":"<ul> <li> <p>In the example, the surrogate key is <code>Salesperson_SK</code>.</p> </li> <li> <p>Surrogate Key: a single column unique-identifier that's generated and stored in the dimension table. </p> <ul> <li>It's a primary key column used to relate to other tables in the dimensional model.</li> </ul> </li> <li> <p>Surrogate keys strive to insulate the data warehouse from changes in source data. They also deliver many other benefits, allowing you to:</p> <ul> <li> <p>Consolidate multiple data sources (avoiding clash of duplicate identifiers).</p> </li> <li> <p>Consolidate multi-column natural keys into a more efficient, single-column key.</p> </li> <li> <p>Limit fact table width for storage optimization (by selecting the smallest possible integer data type).</p> </li> </ul> </li> <li> <p>A surrogate key column is a recommended practice, even when a natural key (described next) seems an acceptable candidate. You should also avoid giving meaning to the key values (except for date and time dimension keys, as described later).</p> </li> </ul>"},{"location":"architecture/star_schema/#natural-keys","title":"Natural Keys","text":"<ul> <li> <p>In the sample dimension table, there is also a natural key, which is named <code>EmployeeID</code>.</p> </li> <li> <p>A natural key is the key stored in the source system.</p> </li> <li> <p>It allows relating the dimension data to its source system, which is typically done by an Extract, Load, Transform (ETL) process to load the dimension table.</p> </li> <li> <p>Sometimes dimensions don't have a natural key. That oculd be the case for your date dimension or lookup dimensions, or when you generate dimension data by normalizing a flat file.</p> </li> </ul>"},{"location":"architecture/star_schema/#dimension-attributes","title":"Dimension Attributes","text":"<ul> <li> <p>The sample dimension table also has dimension attributes, like the <code>FirstName</code> column.</p> </li> <li> <p>Dimension attributes provide context to the numeric data stored in related fact tables. They're typically text columns that are used in analytics queries to filter and group (slice and dice), but not to be aggregated themselves. </p> </li> </ul>"},{"location":"architecture/star_schema/#foreign-keys","title":"Foreign Keys","text":"<ul> <li> <p>The sample dimension table also has a foreign key, which is named <code>SalesRegion_FK</code>.</p> </li> <li> <p>Other dimension tables can reference a foreign key, and their presence in a dimension table is a special case. </p> <ul> <li>It indicates that the table is related to another dimension table, meaning that it might form a part of a snowflake dimension or it's related to an outrigger dimension.</li> </ul> </li> </ul>"},{"location":"architecture/star_schema/#fact-table-structure","title":"Fact Table Structure","text":"<ul> <li> <p>Fact tables include measures, which are typically numeric columns, like sales order qunatity. Analytics queries summarize measures (by using sum, count, average, and other functions) within the context of dimension filters and groupings.</p> </li> <li> <p>Fact tables also include dimension keys, which determine the dimensionality of the facts. The dimension key values determine the granularity of the facts, which is the atomic level by which facts are defined.</p> </li> <li> <p>To easily identify fact tables, their names will be prefixed with <code>Fact_</code>.</p> </li> <li> <p>To describe the structure of a fact table, consider the following example of a sales fact table named <code>Fact_Sales</code>. This example applies good design practices. </p> </li> </ul> SQL<pre><code>CREATE TABLE Fact_Sales\n(\n    --Dimension keys\n    OrderDate_Date_FK INT NOT NULL,\n    ShipDate_Date_FK INT NOT NULL,\n    Product_FK INT NOT NULL,\n    Salesperson_FK INT NOT NULL,\n    &lt;...&gt;\n\n    --Attributes\n    SalesOrderNo INT NOT NULL,\n    SalesOrderLineNo SMALLINT NOT NULL,\n\n    --Measures\n    Quantity INT NOT NULL,\n    &lt;...&gt;\n\n    --Audit attributes\n    AuditMissing BIT NOT NULL,\n    AuditCreatedDate DATE NOT NULL,\n    AuditCreatedBy VARCHAR(15) NOT NULL,\n    AuditLastModifiedDate DATE NOT NULL,\n    AuditLastModifiedBy VARCHAR(15) NOT NULL\n);\n</code></pre>"},{"location":"architecture/star_schema/#primary-key_1","title":"Primary Key","text":"<ul> <li> <p>The sample fact table does not have a primary key. </p> <ul> <li> <p>It doesn't typically serve a useful purpose, and it would unnecessarily increase the table storage size.</p> </li> <li> <p>A primary key is often implied by the set of dimension keys and attributes.</p> </li> </ul> </li> </ul>"},{"location":"architecture/star_schema/#dimension-key","title":"Dimension Key","text":"<ul> <li> <p>The sample fact table has various dimension keys, which determine the dimensionality of the fact table. </p> </li> <li> <p>Dimension keys are references to the surrogate keys (or higher-level attributes) in the related dimensions.</p> </li> <li> <p>A fact table can reference a dimension multiple times. In this case, it's known as a role-playing dimension. In this example, the fact table has the <code>OrderDate_Date_FK</code> and <code>ShipDate_Date_FK</code> dimension keys. Each dimension key represents a distinct role, yet there's only one physical date dimension.</p> </li> <li> <p>It is good practice to set each dimension key as <code>NOT NULL</code>.</p> </li> </ul>"},{"location":"architecture/star_schema/#attributes","title":"Attributes","text":"<ul> <li> <p>In this example, attribute columns store sales order information.</p> </li> <li> <p>Attributes provide additional information and set the granularity of fact data, but they're neither dimension keys nor dimension attributes, nor measures.</p> </li> <li> <p>For analysis purposes, an attribute could form a degenerate dimension.</p> </li> </ul>"},{"location":"architecture/star_schema/#measures","title":"Measures","text":"<ul> <li> <p>The <code>Quantity</code> column in the sample fact table is an example of a measure.</p> </li> <li> <p>Measure columns are typically numeric and commonly additive (meaning they can be summed, and summarized by using other aggregations).</p> </li> </ul>"},{"location":"architecture/star_schema/#audit-attributes","title":"Audit Attributes","text":"<ul> <li> <p>The sample fact table also has various audit attributes. Audit attributes are optional.</p> </li> <li> <p>They allow you to track when and how fact records were created or modified, and they can include diagnostic or troubleshooting information raised during Extract, Transform, and Load (ETL) processes.</p> <ul> <li>For example, you'll want to track who (or what process) updated a row, and when. Audit attributes can also help diagnose a challenging problem, like when an ETL process stops unexpectedly.</li> </ul> </li> </ul> <p>Dimension Design </p>"},{"location":"automation/sap_automation/","title":"SAP Automation","text":""},{"location":"automation/sap_automation/#overview","title":"Overview","text":"<ul> <li> <p>The three main files to take note of are: <code>main.py</code>, <code>automation.py</code>, and <code>gui.py</code>. </p> <ul> <li> <p><code>main.py</code>: Initializes and centers the main GUI window and launches the SAP Frame interface, handling any uncaught errors via logging.</p> </li> <li> <p><code>automation.py</code>: COntains the core automation logic to interact with SAP GUI using SAP Scripting tool.</p> </li> <li> <p><code>gui.py</code>: Implements the graphical user interface that collects user inputs, validates them, and triggers the SAP automation in a background thread.</p> </li> </ul> </li> </ul>"},{"location":"automation/sap_automation/#overview_1","title":"Overview","text":"<ul> <li> <p>Purpose: To automate the report extraction from the SAP interface based on user input.</p> </li> <li> <p>Users can choose between downloading ZFD107 or FBL3N using the GUI window.</p> </li> <li> <p>Requires SAP to already be logged in. Since it is using a scripting tool, the automation can run in the background, and is also much faster than manually downloading.</p> </li> </ul>"},{"location":"automation/sap_automation/#user-interface","title":"User Interface","text":"<ul> <li>On the start page, the user can choose whether they want to download FBL3N or ZFD107. </li> </ul>"},{"location":"automation/sap_automation/#fbl3n","title":"FBL3N","text":"<ul> <li> <p>Users can choose a start date and end date, and can input the path that they want to download the file to.</p> </li> <li> <p>The file will be downloaded with the name: <code>FBL3N_{current_date}.xlsx</code>. </p> </li> <li> <p>The download path should not have \"\", and should be the absolute path to the folder to download the file.</p> </li> </ul>"},{"location":"automation/sap_automation/#zfd107","title":"ZFD107","text":"<ul> <li> <p>Users can input the period that they want to download, as well as the absolute path to download the file to.</p> </li> <li> <p>The same rules as FBL3N apply for inputting the download path.</p> </li> <li> <p>The period should be in the format: YYYYMM.</p> </li> <li> <p>The file will be downloaded as <code>ZFD107_YYYY_MM.xlsx</code>.</p> </li> </ul>"},{"location":"automation/sap_automation/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Download the <code>SAP_Automation.zip</code> folder. Unzip the folder.</p> </li> <li> <p>In Powershell, navigate to your SAP_Automation folder using the code:</p> PowerShell<pre><code>cd relative\\path\\to\\SAP_Automation\n</code></pre> </li> <li> <p>Download all the requirements in the <code>requirements.txt</code> folder using the following code:</p> PowerShell<pre><code>pip install -r requirements.txt\n</code></pre> </li> <li> <p>Run <code>main.py</code> either using VSCode, or run the code below in Powershell:</p> PowerShell<pre><code>python main.py`\n</code></pre> </li> <li> <p>Use the GUI to download the required files.</p> </li> </ol> <p> STRAVIS Automation</p>"},{"location":"automation/stravis_automation/","title":"STRAVIS Automation","text":"<ul> <li> <p>The four main files to take note of are: <code>main.py</code>, <code>automation.py</code>, <code>gui.py</code>, and <code>config.py</code>.</p> <ul> <li> <p><code>main.py</code>: Initializes and centers the main GUI window and launches the STRAVISFrame interface, handling any uncaught errors via logging.</p> </li> <li> <p><code>gui.py</code>: Implements the graphical user interface that collects user inputs (period and folder path), validates them, and triggers the STRAVIS automation in a background thread.</p> </li> <li> <p><code>automation.py</code>: Contains the core automation logic to interact with the STRAVIS GUI using screen coordinates and <code>pyautogui</code> for navigating, filtering, and downloading data files.</p> </li> <li> <p><code>config.py</code>: Stores all hardcoded screen coordinates used by <code>pyautogui</code> to interact with specific UI elements in the STRAVIS system.</p> </li> </ul> </li> </ul>"},{"location":"automation/stravis_automation/#overview","title":"Overview","text":"<ul> <li> <p>Purpose: To automate the report extraction from the STRAVIS interface based on user input.</p> </li> <li> <p>Creates a folder for the period indicated, and automatically downloads Excel files.</p> </li> <li> <p>Requires STRAVIS to be logged in and accessible on screen. The mouse cannot be moved, as this automation works by automated clicks on the screen.</p> </li> </ul>"},{"location":"automation/stravis_automation/#user-interface","title":"User Interface","text":"<ul> <li> <p>The app uses a <code>Tkinter</code> GUI with:</p> <ul> <li> <p>Input for Period ('YYYY.MM')</p> </li> <li> <p>Input for Base Folder (where a fiscal year subfolder will be created)</p> </li> </ul> </li> </ul>"},{"location":"automation/stravis_automation/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Download the <code>STRAVIS_Automation.zip</code> folder. Unzip the folder.</p> </li> <li> <p>In Powershell, navigate to your STRAVIS_Automation folder using the code:</p> PowerShell<pre><code>cd relative\\path\\to\\STRAVIS_Automation\n</code></pre> </li> <li> <p>Once all the configurations are done, package it into an app.</p> <ol> <li> <p>Make sure PyInstaller is installed. To do so, use the following code:</p> PowerShell<pre><code>pip install PyInstaller\n</code></pre> </li> <li> <p>Run the following code in Powershell:</p> PowerShell<pre><code>python -m PyInstaller automation_{your_name}.py -n STRAVIS_Automation_{your_name}.exe\n</code></pre> </li> <li> <p>Once the code has run, navigate to the <code>\\dist</code> folder in STRAVIS_Automation from your Files application. The <code>.exe</code> file should be stored there. </p> <ul> <li>Open this file to get the automation app.</li> </ul> </li> </ol> </li> </ol>"},{"location":"automation/stravis_automation/#configurations","title":"Configurations","text":"<ol> <li> <p>Create a copy of 4 files: <code>main.py</code>, <code>gui.py</code>, <code>automation.py</code>, and <code>config.py</code>.</p> <ul> <li>Naming convention for all the files: <code>{file_name}_{your_name}.py</code>.</li> </ul> </li> </ol>"},{"location":"automation/stravis_automation/#configpy","title":"<code>config.py</code>","text":"<ol> <li> <p>Open your own version of <code>config_{your_name}.py</code>.</p> </li> <li> <p>Using <code>click_test.py</code>, find all the coordinates to click for each item in the dictionary.</p> <ul> <li> <p>To test:</p> <ul> <li> <p>Create a new file called <code>test_config_{your_name}.py</code>. You will use this file as your workspace to test the automated clicks.</p> </li> <li> <p>Type <code>from config_{your_name} import *</code>.</p> </li> <li> <p>In the subsequent lines, copy the lines of automation that you want to test from <code>automation.py</code> and paste it into the document.</p> </li> <li> <p>If there are input variables being used, you can change them to any variables that you want to test with.</p> </li> </ul> </li> </ul> </li> <li> <p>Once all the automated clicks work, save the file, and your <code>config_{your_name}.py</code> is working.</p> </li> </ol>"},{"location":"automation/stravis_automation/#automationpy","title":"<code>automation.py</code>","text":"<ol> <li> <p>Open your own version of <code>automation_{your_name}.py</code>.</p> </li> <li> <p>In Line 12, change the code from <code>from config import *</code> to <code>from config_{your_name} import *</code>.</p> </li> <li> <p>Save the file.</p> </li> </ol>"},{"location":"automation/stravis_automation/#guipy","title":"<code>gui.py</code>","text":"<ol> <li> <p>Open your own version of <code>gui_{your_name}.py</code>.</p> </li> <li> <p>In Line 5, change the code from <code>from automation import STRAVISAutomation</code> to <code>from automation_{your_name} import STRAVISAutomation</code>.</p> </li> <li> <p>Save the file.</p> </li> </ol>"},{"location":"automation/stravis_automation/#mainpy","title":"<code>main.py</code>","text":"<ol> <li> <p>Open your own version of <code>main_{your_name}.py</code>.</p> </li> <li> <p>In Line 2, change the code from <code>from gui import STRAVISFrame</code> to <code>from gui_{your_name} import STRAVISFrame</code>.</p> </li> <li> <p>Save the file.</p> </li> </ol> <p>SAP Automation </p>"},{"location":"fabric_tools/dataflow/","title":"Dataflow","text":"<ul> <li> <p>Because most of our files are currently stored in Sharepoint, this is the default method used to bring data into Fabric workspaces.</p> </li> <li> <p>Power Query is used to do the data transformation, thus it is a low-code solution.</p> </li> </ul>"},{"location":"fabric_tools/dataflow/#dataflow-gen1-vs-gen2","title":"Dataflow Gen1 vs. Gen2","text":"<ul> <li> <p>In the current workspaces, there are 2 different dataflows being used: Dataflow Gen1 and Dataflow Gen2.</p> </li> <li> <p>The only difference between the two dataflows is that Gen2 allows for a different source and destination (e.g. warehouse, lakehouse, etc.), but Gen1 only allows for one destination, which is a semantic model.</p> </li> </ul>"},{"location":"fabric_tools/dataflow/#creating-a-dataflow-sharepoint-files","title":"Creating a Dataflow (Sharepoint Files)","text":"<ol> <li> <p>In the workspace that you want to create a dataflow, click on + New Item.</p> </li> <li> <p>Under Get data, click on the Dataflow Gen2 tile. </p> </li> <li> <p>Name your Dataflow Gen2, and select Create.</p> <ul> <li> <p>If you wish to enable Git integration, select teh checkbox below the Name textbox.</p> </li> <li> <p>For the accurate Naming Convention, refer to Architecture: Naming Convention.</p> </li> </ul> </li> <li> <p>Under the Home tab, click on the Get Data button, and a dropdown should appear.</p> <p> </p> </li> <li> <p>Click on More.... Under New Sources, select Sharepoint folder. If it is not under New sources, you can simply search for it in the search bar.</p> <ul> <li>If you want to bring in data from other sources besides Sharepoint files, you can select oter sources besides Sharepoint folder here.</li> </ul> <p> </p> </li> <li> <p>Enter the desired Sharepoint Site URL. </p> <ul> <li> <p>To get the Sharepoint site URL:</p> <ul> <li> <p>Open a page in Sharepoint.</p> </li> <li> <p>From a page in Sharepoint, you can usually get the site address by selecting Home in the navigation pane, or the icon for the site at the top. Copy the address from your web browser's address bar and paste into the Sharepoint Site URL text box.'</p> </li> </ul> </li> </ul> <p> Sharepoint Site URL (Microsoft, 2025) </p> </li> <li> <p>Click Next.</p> </li> <li> <p>In the Preview folder data popup, click Create.</p> </li> <li> <p>Create 3 groups, \"NAVIGATION\", \"TRANSAFORMATION\", and \"FINAL\".</p> <ul> <li> <p>To do so, right click on any empty space in the Queries pane, and select New group.</p> </li> <li> <p>NAVIGATION: for all the files used to navigate to the destination.</p> </li> <li> <p>TRANSFORMATION: for all the \"Transform file from ___\" folders created when combining files.</p> <ul> <li>If you are not combining files, this folder can stay empty.</li> </ul> </li> <li> <p>FINAL: for all the final files that will be sent to the data destination.</p> </li> </ul> </li> <li> <p>Rename this query to from \"Query\" to \"Source\". To do this, in the Queries pane, right click on the query and select Rename.</p> </li> <li> <p>Right click on the query and click Enable staging to disable staging.</p> <ul> <li>Please remember to disable staging for all files that will not be sent to the data destination. Only files that will be sent to the data destination can have staging enabled.</li> </ul> </li> <li> <p>In the Formula textbox, the current code should be <code>SharePoint.Files(\"your/sharepoint/site/url\", [ApiVersion = 15])</code>. Change this code to <code>SharePoint.Contents(\"your/sharepoint/site/url\", [ApiVersion = 15])</code>.</p> </li> <li> <p>Put the Source query under the NAVIGATION folder.</p> <ul> <li>Your Queries pane should now look like this.</li> </ul> <p> </p> </li> <li> <p>Reference the Source query by right clicking on the Source query and selecting Reference.</p> <ul> <li> <p>From here, navigate to the folder where all your files are located.</p> </li> <li> <p>Generally, if there are multiple folders where the files are located, a good practice would be to stop at the parent folder where all the individual folders are located. Then, reference the parent folder and navigate to each of the desired folders. A query can be referenced multiple times, thus if you have multiple folders to navigate to, just reference the parent folder multiple times.</p> </li> <li> <p>To navigate into the folder, simply click on the [Table] Link under the Content column in the table.</p> </li> <li> <p>All of these queries go into the NAVIGATION folder.</p> </li> <li> <p>Rename all these queries to the names of the folders that you are navigating to.</p> </li> </ul> </li> <li> <p>A good practice would be to rename all the invidual steps to what you are currently doing. This can be done by right clicking the individual steps and selecting Rename. All the steps are under the Query settings pane, under Applied steps.</p> </li> <li> <p>Once you are in the folder, reference the query again. This time, move the referenced query to the FINAL folder. Name this new referenced query to the table name that you want to save it as in your data destination.</p> </li> <li> <p>If you have multiple files that you want to combine together, follow the steps below for the new query:</p> <ul> <li> <p>Change the data type of the column to Binary. Ensure that the items are all files, and not folders. There will be an error if they are folders. If it is successful, the [Table] links will change to [Binary].</p> <ul> <li> <p>To change the data type, click on the data type beside the Content column header.</p> <p> </p> </li> <li> <p>Alternatively, go to the Transform pane, select the column, and click on Data type: Any and change to Binary in the dropdown.</p> <p> </p> </li> </ul> </li> <li> <p>Click on the Combine files button on the right side of the Content column header.</p> <p> </p> </li> <li> <p>If there are multiple sheets, select the sheet that you want to combine from the left pane in the Combine files popup. Once done, select OK.</p> </li> <li> <p>Move the \"Transform file from {Table Name}\" to the TRANSFORMATION folder.</p> </li> </ul> </li> <li> <p>Do the necessary data transformations to the query. </p> <ul> <li> <p>If this data is fed into EDITable, make sure that the general structure of the table remains the same. This means that only basic transformations such as removing empty rows and changing data types can be done.</p> </li> <li> <p>If you want to make changes that will change each individual combined files, do the transformations in the Transform Sample file file in the \"Transform file from {Table Name}\" folder.</p> <ul> <li> <p>Once the changes are made, remember to refresh the query, which is available under the Home pane.</p> </li> <li> <p>The changes made in the Transform Sample file file should now be reflected in the main query.</p> </li> </ul> </li> <li> <p>If you want to make changes to the overall main query, simply do the transformations in the main query.</p> </li> </ul> </li> <li> <p>Once all the transformations are done, add the data detsination. Under the Query settings pane, press the + sign beside Data destination.</p> <p> </p> </li> <li> <p>Select the new destination type that you wish to bring the table to.</p> </li> <li> <p>Find the exact destination that you want to bring the table to in the Choose destination target popup. Click Next once done.</p> </li> <li> <p>Unselect Use Automatic Settings, set update method to Replace, and schema options on publish to Dynamic schema.</p> </li> <li> <p>For column mapping, if a warning appears that some column names contain unsupported characters, click the Fix it button.</p> </li> <li> <p>Check that all Source types are defined, the source type cannot be Any. </p> </li> <li> <p>Once all the columns are selected, click Save settings.</p> </li> <li> <p>Click Publish once all the configurations (steps 15 - 25) are done for all queries. </p> </li> </ol>"},{"location":"fabric_tools/dataflow/#maintaining-dataflows","title":"Maintaining Dataflows","text":"<ul> <li> <p>A scheduled refresh can be set on the dataflow such that the data in the dataflow will be refreshed periodically.</p> </li> <li> <p>By using pipelines, a refresh can be scheduled when data is added to the Sharepoint folder.</p> </li> <li> <p>A manual refresh can be done by clicking the refresh button on the dataflow.</p> <p> </p> </li> </ul> <p> SQL Database</p> <p>Pipelines </p>"},{"location":"fabric_tools/lakehouse/","title":"Lakehouse","text":""},{"location":"fabric_tools/lakehouse/#creating-a-lakehouse","title":"Creating a Lakehouse","text":"<p>To decide between the different data storage options, refer to Warehouse, SQL Database, or Lakehouse?</p> <p>There are 2 ways to create a Lakehouse. However, the second method is more straightforward, and usually preferred.</p> <ol> <li> <p>Data Engineering homepage</p> <ul> <li>A Lakehouse can be created through the Lakehouse card under the New section in the homepage.</li> </ul> <p> Lakehouse Explorer in Fabric (Microsoft, 2025) </p> </li> <li> <p>Workspace view</p> <ul> <li>A Lakehouse can be created in the workspace view under 'Store Data'</li> </ul> <p> Lakehouse card in Workspace view </p> </li> </ol>"},{"location":"fabric_tools/lakehouse/#creating-a-lakehouse-in-data-engineering-homepage","title":"Creating a Lakehouse in Data Engineering homepage","text":"<ol> <li> <p>Browse to the Data Engineering homepage</p> </li> <li> <p>Under the New section, locate the Lakehouse card and select it to get started with the creation process.</p> </li> <li> <p>Enter the name for the Lakehosue and a sensitivity label if required, and select Create.</p> </li> <li> <p>Once the Lakehouse is created, you land on the Lakehouse explorer page where you can get started and load data.</p> </li> </ol>"},{"location":"fabric_tools/lakehouse/#creating-a-lakehouse-in-workspace-view","title":"Creating a Lakehouse in Workspace view","text":"<ol> <li> <p>Navigate to the workspace that you want to create a Lakehouse.</p> </li> <li> <p>In the top left hand corner, click on the New Item button. A New Item pane should pop up on the right half of the screen.</p> </li> <li> <p>Navigate to the Store Data section of the 'New Item' pane.</p> </li> <li> <p>Click on Lakehouse, and it should prompt you to name the Lakehouse, and select Create once the Lakehouse is named.</p> <ul> <li>To enable schema support for the Lakehouse, check the box nect to Lakehouse schemas (Public Preview).</li> </ul> <p> </p> </li> <li> <p>Once the Lakehouse is created, you should end up in the Lakehouse explorer.</p> </li> </ol>"},{"location":"fabric_tools/lakehouse/#lakehouse-schemas","title":"Lakehouse Schemas","text":"<ul> <li> <p>Once the Lakehouse is created, the default schema named dbo can be found under Tables. This schema can't be changed or removed.</p> </li> <li> <p>To create a new schema, hover over Tables in Lakehouse Explorer, select ..., and choose New schema. Enter the schema name and select Create. The schema will be listed under Tables in alphabetical order.</p> <p> </p> <ul> <li>To use Python to store a table in a specific schema, use the following code:</li> </ul> PySpark Python<pre><code>df.write.mode(\"overwrite\").saveAsTable(\"yourSchemaName.yourTableName\")\n</code></pre> <ul> <li>Alternatively, you can drag and drop tables between schemas in the Lakehouse Explorer page.</li> </ul> <p> </p> </li> <li> <p>To reference an entire schema from another Fabric Lakehouse or external storage, hover over Tables in Lakehouse Explorer, select ..., and choose New schema shortcut. Then, select a schema on another Lakehouse, or a folder with Delta tables on your external storage. This will create a new schema with the referenced tables.</p> </li> </ul>"},{"location":"fabric_tools/lakehouse/#loading-data-into-lakehouse","title":"Loading data into Lakehouse","text":"<p>In our current workspaces, these methods are most commonly used:</p> <ul> <li> <p>Notebook code: In the Finance dataflows, the following code is used:</p> PySpark Python<pre><code>yourDataframeName.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"targetLakehouse.yourTableName\")\nprint(\"yourTableName saved to Lakehouse\")\nspark.catalog.clearCache()\nspark.sql(\"REFRESH TABLE targetLakehouse.yourTableName\")\n</code></pre> </li> <li> <p>Copy tool in pipelines: Connect to various data sources and land the data in its original format or convert it into a Delta table.</p> </li> <li> <p>Dataflows Gen 2: Create dataflows to import data, transform it, and publish it into your lakehouse.</p> <ul> <li>Use this if you are importing data from a Sharepoint folder.</li> </ul> </li> <li> <p>Shortcut: Create shortcuts that connect to existing data in your lakehouse without needing to copy it.</p> </li> </ul>"},{"location":"fabric_tools/lakehouse/#interacting-with-lakehouses","title":"Interacting with Lakehouses","text":"<ul> <li> <p>Lakehouse explorer: This is the main Lakehouse interaction page. You can load data into the Lakehouse, explore data in the Lakehouse using the object explorer, and various other things.</p> </li> <li> <p>Notebooks: Notebooks can be used to write code to read, transform and write directly to Lakehouse as tables and/or folders.</p> </li> </ul>"},{"location":"fabric_tools/lakehouse/#lakehouse-explorer","title":"Lakehouse Explorer","text":"Lakehouse Explorer in Fabric (Microsoft, 2025) <ul> <li> <p>Table Section: All tables are stored here.</p> <ul> <li> <p>Organised and governed to facilitate efficient data processing and analysis.</p> </li> <li> <p>You can select a table to preview, inspect the table schema, access underlying files, and execute various other actions related to the data.</p> </li> </ul> </li> <li> <p>Unidentified Area: Displays folders or files present in the managed area that lack associated tables in SyMS.</p> <ul> <li> <p>If this happens when you create new tables: Refresh the Unidentified folder, and the tables should appear under the Tables folder.</p> </li> <li> <p>If unsupported files (e.g.: images, audio) are uploaded to the managed area, they will not be automatically detected and linked to tables.</p> <ul> <li>Prompts the user to either remove these files from the managed area or transfer them to the File Section for further processing. </li> </ul> </li> </ul> </li> <li> <p>File Section: Unmanaged area of the lakehouse and can be considered a \"landing zone\" for raw data ingested from various sources.</p> <ul> <li> <p>The data often requires additional processing. </p> </li> <li> <p>Displays folder-level objects only. To view file-level objects, you need to utilize the Main View area.</p> </li> </ul> </li> </ul>"},{"location":"fabric_tools/lakehouse/#main-view-area","title":"Main View Area","text":"<ul> <li> <p>This is where most of the data interaction occurs. The view changes depending on what you select.</p> </li> <li> <p>The object explorer only displays the folder hierarchy of the lake, but the main view area is used to navigate the files, preview files and tables, and various other tasks.</p> </li> </ul>"},{"location":"fabric_tools/lakehouse/#table-preview","title":"Table Preview","text":"<ul> <li> <p>Some functions can be carried out in the preview area to interact with the data:</p> <ul> <li> <p>Sort columns in ascending or descending order with a simple click.</p> </li> <li> <p>Filter data by substring or by selecting from a list of available values in the table.</p> </li> <li> <p>Resize columns to tailor your data view to suit your preferences.</p> </li> </ul> </li> </ul>"},{"location":"fabric_tools/lakehouse/#file-preview","title":"File Preview","text":"<ul> <li>Files in various formats can be previewed. This is supported for the following file types: bmp, css, csv, gif, html, jpeg, jpg, js, json, md, mjs, png, ps1, py, svg, ts, tsx, txt, xml,yaml.</li> </ul>"},{"location":"fabric_tools/lakehouse/#shortcut","title":"Shortcut","text":"<ul> <li> <p>Shortcuts can be created between lakehouses and/or warehouses, such that data does not need to be copied.</p> </li> <li> <p>You can quickly make large amounts of data available in your lakehouse locally without the latency of copying data from the source.</p> </li> </ul>"},{"location":"fabric_tools/lakehouse/#creating-a-shortcut","title":"Creating a Shortcut","text":"Create lakehouse Shortcut (Microsoft, 2025) <ol> <li> <p>Open Lakehouse Explorer in the lakehouse that you want to copy.</p> </li> <li> <p>Select the ... symbol next to the Tables or Files section and select New shortcut.</p> <ul> <li> <p>The Tables section of the lakehouse explorer automatically registers it as a table enabling data access through Spark, SQL analytics endpoint, and default semantic model.</p> </li> <li> <p>After a shortcut is created, you can differenciate a regular file or a table from the shortcut from its properties. The properties have a Shortcut Type parameter that indicates the item is a shortcut.</p> </li> </ul> </li> </ol> <p>SQL Analytics Endpoint </p>"},{"location":"fabric_tools/ml_model/","title":"ML Model","text":"<ul> <li> <p>A machine learning model is a file trained to recognize certain types of patterns.</p> <ul> <li> <p>You train a model over a set of data, and you provide it an algorithm that uses to reason over and learn from that data set. </p> </li> <li> <p>After you train the model, you can use it to reason over data that it never saw before, and make predictions about that data.</p> </li> </ul> </li> <li> <p>In MLflow, a machine learning model can include multiple model versions. </p> <ul> <li>Here, each version can represent a model iteration.</li> </ul> </li> </ul>"},{"location":"fabric_tools/ml_model/#create-a-machine-learning-model","title":"Create a Machine Learning Model","text":"<ul> <li> <p>In MLflow, machine learning models include a standard packaging format. </p> <ul> <li> <p>This format allows use of those models in various downstream tools, including batch inferencing on Apache Spark.</p> </li> <li> <p>The format defines a convention to save a model in different \"flavours\" that different downstream tools can understand.</p> </li> </ul> </li> <li> <p>You can directly create a machine learning model from the Fabric UI. The MLflow API can also directly create the model.</p> </li> </ul>"},{"location":"fabric_tools/ml_model/#steps","title":"Steps","text":"<ol> <li> <p>Create a new data science workspace, or select an existing data science workspace.</p> </li> <li> <p>Create a new workspace or select an existing one.</p> </li> <li> <p>You can create a new item through the workspace</p> <ol> <li> <p>Select your workspace.</p> </li> <li> <p>Select New item.</p> </li> <li> <p>Select ML model under Analyze and train data.</p> </li> </ol> </li> <li> <p>After model creation, you can start adding model versions to track run metrics and parameters. Register or save experiment runs to an existing model.</p> </li> </ol>"},{"location":"fabric_tools/ml_model/#api","title":"API","text":"<ul> <li>You can also create a machine learning model directly from your authroing experience with the <code>mlflow.register_model()</code> API. If a registered machine learning model with the given name doesn't exist, the API creates it automatically.</li> </ul> Python<pre><code>import mlflow\n\nmodel_uri = \"runs:/{}/model-uri-name\".format(run.info.run_id)\nmv = mlflow.register_model(model_uri, \"model-name\")\n\nprint(\"Name: {}\".format(mv.name))\nprint(\"Version: {}\".format(mv.version))\n</code></pre>"},{"location":"fabric_tools/ml_model/#manage-versions","title":"Manage Versions","text":"<ul> <li> <p>A machine learning model contains a collection of model versions for simplified tracking and comparison. Within a model, a data scientist can navigate across various model versions to explore the underlying parameters and metrics.</p> </li> <li> <p>Data scientists can also make comparisons across model versions to identify whether or not newer models might yield better results.</p> </li> </ul>"},{"location":"fabric_tools/ml_model/#track-machine-learning-models","title":"Track machine learning models","text":"<ul> <li> <p>A machine learning model version represents an individual model that is registered for tracking.</p> <p> ML Model Version Tracking (Microsoft, 2025) </p> </li> <li> <p>Each model version includes the following information:</p> <ul> <li> <p>Time created: Date and time of model creation.</p> </li> <li> <p>Run Name: The identifier for the experiment runs used to create this specific model version.</p> </li> <li> <p>Hyperparameters: Hyperparameters are saved as key-value pairs. Both keys and values are strings.</p> </li> <li> <p>Metrics: Run metrics saved as key-value pairs. The value is numeric.</p> </li> <li> <p>Model Schema/Signature: A description of the model inputs and outputs.</p> </li> <li> <p>Logged files: Logged files in any format. For example, you can record images, environment, models, and data files.</p> </li> <li> <p>Tags: Metadata as key-value pairs to runs.</p> </li> </ul> </li> </ul>"},{"location":"fabric_tools/ml_model/#apply-tags-to-ml-models","title":"Apply Tags to ML Models","text":"<ul> <li> <p>MLflow tagging for model versions enables users to attach custom metadata to specific versions of a registered model in the MLflow Model Registry.</p> <ul> <li> <p>These tags, stored as key-value pairs, help organize, track, and differentiate between model versions, making it easier to manage model lifecycles.</p> </li> <li> <p>Tags can be used to denote the model's purpose, deployment environment, or any other relevant information, facilitating more efficient model management and decision-making within teams.</p> </li> </ul> </li> <li> <p>This code demonstrates how to train a RandomForestRegressor model using Scikit-learn, log the model and parameters with MLflow, and then register the model in the MLflow Model Registry with custom tags.</p> Python<pre><code>import mlflow.sklearn\nfrom mlflow.models import infer_signature\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Generate synthetic regression data\nX, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)\n\n# Model parameters\nparams = {\"n_estimators\": 3, \"random_state\": 42}\n\n# Model tags for MLflow\nmodel_tags = {\n    \"project_name\": \"grocery-forecasting\",\n    \"store_dept\": \"produce\",\n    \"team\": \"stores-ml\",\n    \"project_quarter\": \"Q3-2023\"\n}\n\n# Log MLflow entities\nwith mlflow.start_run() as run:\n    # Train the model\n    model = RandomForestRegressor(**params).fit(X, y)\n\n    # Infer the model signature\n    signature = infer_signature(X, model.predict(X))\n\n    # Log parameters and the model\n    mlflow.log_params(params)\n    mlflow.sklearn.log_model(model, artifact_path=\"sklearn-model\", signature=signature)\n\n# Register the model with tags\nmodel_uri = f\"runs:/{run.info.run_id}/sklearn-model\"\nmodel_version = mlflow.register_model(model_uri, \"RandomForestRegressionModel\", tags=model_tags)\n\n# Output model registration details\nprint(f\"Model Name: {model_version.name}\")\nprint(f\"Model Version: {model_version.version}\")\n</code></pre> </li> <li> <p>After applying the tags, you can view them directly on the model version details page. </p> <ul> <li>Additionally, tags can be added, updated, or removed from this page at any time.</li> </ul> <p> ML Model Version Tagging (Microsoft, 2025) </p> </li> </ul>"},{"location":"fabric_tools/ml_model/#compare-and-filter-ml-models","title":"Compare and Filter ML Models","text":"<ul> <li>To compare and evaluate the quality of machine learning model versions, you can compare the parameters, metrics, and metadata between selected versions.</li> </ul>"},{"location":"fabric_tools/ml_model/#visually-compare-ml-models","title":"Visually Compare ML Models","text":"<ul> <li> <p>You can visually compare runs within an existing model. Visual comparison allows easy navigation between, and sorts across, multiple versions.</p> <p> Compare ML Model Runs (Microsoft, 2025) </p> </li> <li> <p>To compare runs, you can:</p> <ol> <li> <p>Select an existing machine learning model that contains multiple versions.</p> </li> <li> <p>Select the View tab, and then navigate to the Model list view. You can also select the option to View model list directly from the details view.</p> </li> <li> <p>You can customize the columns within the table. Expand the Customize columns pane. From there, you can select the properties, metrics, tags, and hyperparameters that you want to see.</p> </li> <li> <p>Lastly, you can select multiple versions, to compare their results, in the metrics comparison pane. From this pane, you can customize the charts with changes to the chart title, visualization type, X-axis, Y-axis, and more.</p> </li> </ol> </li> </ul>"},{"location":"fabric_tools/ml_model/#compare-using-mlflow-api","title":"Compare using MLflow API","text":"Python<pre><code>from pprint import pprint\n\nclient = MlflowClient()\nfor rm in client.list_registered_models():\n    pprint(dict(rm), indent=4)\n</code></pre> <p>For more information, refer to MLflow documentation.</p>"},{"location":"fabric_tools/ml_model/#apply-machine-learning-model","title":"Apply Machine Learning Model","text":"<ul> <li> <p>Once you train a model on a data set, you can apply that model to data it never saw to generate predictions.</p> </li> <li> <p>We call this model use technique scoring or inferencing. For more information about Microsoft Fabric model scoring, see the next section.</p> </li> </ul> <p> Pipelines</p> <p>Reports </p>"},{"location":"fabric_tools/notebook/","title":"Notebook","text":""},{"location":"fabric_tools/notebook/#creating-a-notebook","title":"Creating a Notebook","text":"<p>From the current workspace, click on New Item and then Notebook, which is the same process as creating any new Fabric item.</p>"},{"location":"fabric_tools/notebook/#importing-existing-notebook","title":"Importing Existing Notebook","text":"<ul> <li> <p>You can import one or more existing notebooks from your local computer using the entry in the workspace toolbar. </p> </li> <li> <p>Fabric notebooks recognize the standard Jupyter Notebook .ipynb files, and source files like .py, .scala, and .sql, and create new notebook items accordingly.</p> </li> </ul> Importing a Notebook (Microsoft, 2025)"},{"location":"fabric_tools/notebook/#languages","title":"Languages","text":"<ul> <li> <p>Fabric notebooks currently support four Apache Spark languages:</p> <ul> <li> <p>PySpark (Python)</p> </li> <li> <p>Spark (Scala)</p> </li> <li> <p>Spark SQL</p> </li> <li> <p>SparkR</p> </li> </ul> </li> </ul>"},{"location":"fabric_tools/notebook/#using-multiple-languages","title":"Using Multiple Languages","text":"Using Different Language (Microsoft, 2025) <ul> <li>You can use multiple languages in a notebook by specifying the language magic command at the beginning of a cell. You can also switch the cell language from the language picker. The following table lists the magic commands for switching cell languages.</li> </ul> Magic Command Language Description <code>%%pyspark</code> Python Execute a Python query against Apache Spark Context. <code>%%spark</code> Scala Execute a Scala query against Apache Spark Context. <code>%%sql</code> SparkSQL Execute a SparkSQL query against Apache Spark Context. <code>%%html</code> HTML Execute an HTML query against Apache Spark Context. <code>%%sparkr</code> R Execute an R query against Apache Spark Context."},{"location":"fabric_tools/notebook/#connecting-lakehouses-and-notebooks","title":"Connecting Lakehouses and Notebooks","text":"<ul> <li> <p>You can navigate to different lakehouse in the Lakehouse explorer and set one lakehouse as the default by pinning it.</p> <ul> <li> <p>Your default is then mounted to the runtime working directly, and you can read or write to the default lakehouse using a local house/</p> </li> <li> <p>You must restart the session after pinning a lakehouse or renaming the default lakehouse.</p> </li> </ul> </li> </ul>"},{"location":"fabric_tools/notebook/#add-or-remove-a-lakehouse","title":"Add or Remove a Lakehouse","text":"<ul> <li> <p>Selecting the X icon beside a lakehouse name removes it from the notebook tab, but the lakehouse item still exists in the workplace.</p> </li> <li> <p>Select Add lakehouse to add more lakehouses to the notebook, either by adding an existing one or creating a new lakehouse.</p> </li> </ul>"},{"location":"fabric_tools/notebook/#explore-a-lakehouse-file","title":"Explore a Lakehouse File","text":"<ul> <li>The subfolder and files under the Tables and Files section of the Lake view appear in a content area between the lakehouse list and the notebook content. Select different folders in the Tables and Files section to refresh the content area.</li> </ul>"},{"location":"fabric_tools/notebook/#loading-data-into-lakehouse","title":"Loading Data into Lakehouse","text":"<ul> <li> <p>In the code cell of the notebook, use the following code example to read data from the source and load it into Files, Tables, or both sections of your lakehouse.</p> </li> <li> <p>To specify the location to read from, you can use the relative path if the data is from the default lakehouse of your current notebook. Or, if the data is from a different lakehouse, you can use the absolute Azure Blob File System (ABFS) path. Copy this path from the context menu of the data.</p> </li> </ul>"},{"location":"fabric_tools/notebook/#loading-with-apache-spark-api","title":"Loading with Apache Spark API","text":"Copy Path Menu (Microsoft, 2025) <ul> <li> <p>Copy ABFS path: This option returns the absolute path of the file.</p> </li> <li> <p>Copy relative path for Spark: This option returns the relative path of the file in your default lakehouse.</p> Python<pre><code>df = spark.read.parquet(\"location to read from\") \n\n# Keep it if you want to save dataframe as CSV files to Files section of the default lakehouse\n\ndf.write.mode(\"overwrite\").format(\"csv\").save(\"Files/ \" + csv_table_name)\n\n# Keep it if you want to save dataframe as Parquet files to Files section of the default lakehouse\n\ndf.write.mode(\"overwrite\").format(\"parquet\").save(\"Files/\" + parquet_table_name)\n\n# Keep it if you want to save dataframe as a delta lake, parquet table to Tables section of the default lakehouse\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(delta_table_name)\n\n# Keep it if you want to save the dataframe as a delta lake, appending the data to an existing table\n\ndf.write.mode(\"append\").format(\"delta\").saveAsTable(delta_table_name)\n</code></pre> </li> </ul>"},{"location":"fabric_tools/notebook/#loading-with-pandas-api","title":"Loading with Pandas API","text":"<ul> <li> <p>To support Pandas API, the default lakehouse is automatically mounted to the notebook. The mount point is '/lakehouse/default/'. You can use this mount point to read/write data from/to the default lakehouse. The \"Copy File API Path\" option from the context menu returns the File API path from that mount point. The path returned from the option Copy ABFS path also works for Pandas API.</p> </li> <li> <p>Spark API: Copy ABFS path or Copy relative path for Spark. </p> </li> <li> <p>Pandas API: Copy ABFS Path or Copy Fil API path.</p> </li> </ul> Copy File API Path (Microsoft, 2025) <ul> <li> <p>Copy File API Path: This option returns the path under the mount point of the default lakehouse.</p> Python<pre><code># Keep it if you want to read parquet file with Pandas from the default lakehouse mount point \n\nimport pandas as pd\ndf = pd.read_parquet(\"/lakehouse/default/Files/sample.parquet\")\n\n# Keep it if you want to read parquet file with Pandas from the absolute abfss path \n\nimport pandas as pd\ndf = pd.read_parquet(\"abfss://DevExpBuildDemo@msit-onelake.dfs.fabric.microsoft.com/Marketing_LH.Lakehouse/Files/sample.parquet\")\n</code></pre> </li> </ul>"},{"location":"fabric_tools/notebook/#collaborate-in-a-notebook","title":"Collaborate in a Notebook","text":"Collaborating in a Notebook (Microsoft, 2025) <ul> <li> <p>The Fabric notebook is a collaborative item that supports multiple users editing the same notebook.</p> </li> <li> <p>When you open a notebook, you enter the coediting mode by default, and every notebook edit is automatically saved. If your colleagues open the same notebook at the same time, you see their profile, run output, cursor indicator, selection indicator, and editing trace. By using the collaboration features, you can easily accomplish pair programming, remote debugging, and tutoring scenarios.</p> </li> </ul>"},{"location":"fabric_tools/notebook/#saving-a-notebook","title":"Saving a Notebook","text":"<ul> <li> <p>A notebook in Fabric will by default save automatically after you open and edit it. </p> </li> <li> <p>You can also use Save a copy to clone another copy in the current workspace to another workspace.</p> </li> <li> <p>To save a notebook manually, you can switch to the Manual save option to have a local branch of your notebook item, and then use Save or Ctrl+S to save your changes.</p> <ul> <li>To change to manual saving, you can also go to Edit -&gt; Save options -&gt; Manual.</li> </ul> </li> </ul>"},{"location":"fabric_tools/notebook/#export-a-notebook","title":"Export a Notebook","text":"<ul> <li> <p>You can export your notebook to other standard formats. Synapse notebook can be exported into:</p> <ul> <li> <p>The standard notebook file (.ipynb) that is used for Jupyter notebooks/</p> </li> <li> <p>An HTML file (.html) that can be opened from a browser directly.</p> </li> <li> <p>A Python file (.py).</p> </li> <li> <p>A Latex file (.tex).</p> </li> </ul> </li> </ul> <p>For more information, refer to official documentation: develop and run notebooks and use Python experience on notebook.</p> <p> SQL Analytics Endpoint</p> <p>Semantic Model </p>"},{"location":"fabric_tools/pipelines/","title":"Pipelines","text":"<ul> <li> <p>A data pipeline is a logical grouping of activities that together perform a data ingestion task.</p> </li> <li> <p>Pipelines allow you to manage extract, transform, and load (ETL) activities instead of managing each one individually.</p> </li> </ul>"},{"location":"fabric_tools/pipelines/#create-a-data-pipeline","title":"Create a Data Pipeline","text":"<ol> <li> <p>To create a new pipeline navigate to your workspace, select the + New Item. Under Get Data, select Data pipeline.</p> </li> <li> <p>In the New pipeline dialog, provide a name for your new pipeline and select Create. </p> </li> <li> <p>You'll land in the pipeline canvas area, where you see options to get started.</p> <ul> <li>From here, you can choose what activity you want to do in the pipeline.</li> </ul> </li> </ol>"},{"location":"fabric_tools/pipelines/#adding-activties-to-a-pipeline","title":"Adding Activties to a Pipeline","text":"<p>Only some of the Activities are listed here, refer to Activity Overview for a more comprehensive list.</p> <ol> <li> <p>Create a new pipeline in your workspace.</p> </li> <li> <p>On the Activities tab for the pipeline, browse the activities displayed, scrolling to the right if necessary to see all activties. Select an activity to add it to the pipeline editor.</p> </li> <li> <p>When you add an activity and select it in the pipeline editor canvas, its General setting will appear in the properties pane below the canvas. </p> </li> <li> <p>Each activity also contains custom properties specific to its conifuration on other tabs in the properties pane.</p> </li> </ol>"},{"location":"fabric_tools/pipelines/#general-settings","title":"General Settings","text":"<ul> <li>When you add a new activity to a pipeline and select it, you'll see its properties panes in the area at the bottom of the screen. These properties panes include General, Settings, and sometimes other panes as well.</li> </ul>"},{"location":"fabric_tools/pipelines/#copy-activity-copy-data-assistant","title":"Copy Activity (Copy Data Assistant)","text":"<ol> <li> <p>Open an existing pipeline or create a new data pipeline.</p> </li> <li> <p>Select Copy data on the canvas to open the Copy Assistant tool to get started. Alternatively, select Use copy assistant from the Copy data drop down list under the Activties tab on the ribbon.</p> <p> Use Copy Assistant (Microsoft, 2024) </p> </li> </ol>"},{"location":"fabric_tools/pipelines/#configure-your-source","title":"Configure Your Source","text":"<ol> <li> <p>Select a data source type from the category.</p> </li> <li> <p>Create a connection to your data source by selecting Create new connection.</p> <p> Create new connection (Microsoft, 2024) </p> <ul> <li> <p>After you select Create new connection, fill in the required connection information and then select Next. For the details of connection creation for each type f data source, you can refer to Microsoft: connectors.</p> </li> <li> <p>If you have existing connections, you can select Existing connection and select your connection from the drop-down list.</p> <p> Existing connection (Microsoft, 2024) </p> </li> </ul> </li> <li> <p>Choose the file or folder to be copied in this source configuration step, and then select Next.</p> <p> Choose copy file or folder (Microsoft, 2024) </p> </li> </ol>"},{"location":"fabric_tools/pipelines/#configure-your-destination","title":"Configure Your Destination","text":"<ol> <li> <p>Select a data source type from the category. You can either create a new connection or use an existing one (same as before). The capabilities of Test connection and Edit are available to each selected connection.</p> </li> <li> <p>Configure and map your source data to your destination. Then select Next to finish your destination configurations.</p> </li> </ol>"},{"location":"fabric_tools/pipelines/#review-and-create-your-copy-activity","title":"Review and Create Your Copy Activity","text":"<ol> <li>Review your copy activity settings in the previous steps and select OK to finish. Or you can go back to the previous steps to edit your settings if needed in the tool.</li> </ol>"},{"location":"fabric_tools/pipelines/#semantic-model-refresh-activity","title":"Semantic Model Refresh Activity","text":""},{"location":"fabric_tools/pipelines/#create-the-activity","title":"Create the activity","text":"<ol> <li> <p>Create a new pipeline in your workspace.</p> </li> <li> <p>Before you the Semantic model refresh activity, you need a connection to your Power BI datasets.</p> </li> <li> <p>Search for Semantic model refresh from the home screen card and select it, or select the activity from the Activities bar and add it to the pipeline canvas.</p> <p> Home Screen Card (Microsoft, 2024) </p> <p> Activities Bar (Microsoft, 2024) </p> </li> <li> <p>Select the new Semantic model refresh activity on the pipeline editor canvas if it isn't already selected.</p> </li> </ol>"},{"location":"fabric_tools/pipelines/#choose-tables-and-partitions-to-refresh","title":"Choose tables and partitions to refresh","text":"<ul> <li> <p>You can optimize your semantic model refresh by choosing just the tables that you wish to refresh as opposed to a full model refresh. </p> <ul> <li> <p>You will find the setting to choose optional table and partition refresh under settings.</p> </li> <li> <p>Each of these properties supports parameterization using the pipeline expression builder.</p> </li> </ul> </li> </ul>"},{"location":"fabric_tools/pipelines/#save-and-run-or-schedule-the-pipeline","title":"Save and run or schedule the pipeline","text":"<ul> <li> <p>Although the Semantic model refresh activity is typically used with other activities, it can be run directly as is. </p> </li> <li> <p>After you configure any other activities required for your pipeline, switch to the Home tab at the top of the pipeline editor, and select the save button to save your pipeline. </p> <ul> <li> <p>Select Run to run it directly, or Schedule to schedule it.</p> </li> <li> <p>You can also view the run history here or configure other settings.</p> </li> </ul> </li> </ul>"},{"location":"fabric_tools/pipelines/#dataflow-activity","title":"Dataflow Activity","text":""},{"location":"fabric_tools/pipelines/#creating-the-activity","title":"Creating the Activity","text":"<ol> <li> <p>Search for Dataflow in the pipeline Activities pane, and select it to add it to the pipeline canvas.</p> </li> <li> <p>Select the new Dataflow activity on the canvas if it isn't already selected.</p> <p> Dataflow Settings (Microsoft, 2024) </p> </li> </ol>"},{"location":"fabric_tools/pipelines/#dataflow-activity-settings","title":"Dataflow activity settings","text":"<ul> <li> <p>Select the Settings tab, then select an existing workspace and dataflow to run.</p> </li> <li> <p>Inside the Dataflow parameters section you are able to enter the name of the parameter that you wish to pass as well as the type and value that you wish to pass.</p> </li> </ul>"},{"location":"fabric_tools/pipelines/#invoke-pipeline-activity","title":"Invoke Pipeline Activity","text":""},{"location":"fabric_tools/pipelines/#create-the-activity_1","title":"Create the activity","text":"<ol> <li> <p>Search for invoke pipeline in the pipeline Activities pane, and select it to add it to the pipeline canvas.</p> </li> <li> <p>Select the new Invoke pipeline activity on the canvas if it isn't already selected.</p> </li> </ol>"},{"location":"fabric_tools/pipelines/#invoke-pipeline-settings","title":"Invoke pipeline settings","text":"<ul> <li> <p>Select the Settings tab, and choose an existing pipeline from the invoke pipeline dropdown, or use the + New button to create a new pipeline directly.</p> <ul> <li>You can choose to wait on completion, or continue directly, in which case the invoked pipeline executes in parallel with activities following it within the parent pipeline's execution flow. </li> </ul> </li> <li> <p>Connection: Each Invoke pipeline activity requires a Connection object that is stored in the secure Fabric credentials store. This connection object stores your user token associated with your fabric workspace. If you haven't yet created a new Invoke pipeline activity, you are required to create a new connection object first before you can use the activity.</p> </li> <li> <p>Workspace: Choose the Fabric workspace where the target pipeline is located that you wish to innvoke from your parent pipeline.</p> </li> </ul>"},{"location":"fabric_tools/pipelines/#notebook-activity","title":"Notebook Activity","text":""},{"location":"fabric_tools/pipelines/#creating-the-activity_1","title":"Creating the activity","text":"<ol> <li> <p>Search for Notebook in the pipeline Activities pane, and select it to add it to the pipeline canvas.</p> </li> <li> <p>Select the new Notebook activity on the canvas if it isn't already selected.</p> </li> </ol>"},{"location":"fabric_tools/pipelines/#notebook-settings","title":"Notebook settings","text":"<ul> <li>Select the Settings tab, select an existing notebook from the Notebook dropdown, and optionallt specify any parameters to pass to the notebook.</li> </ul>"},{"location":"fabric_tools/pipelines/#session-tag","title":"Session tag","text":"<ul> <li> <p>In order to minimize the amount of time it takes to execute your notebook job, you could optionally set a session tag. Setting the session tag will instruct Spark to reuse any existing Spark session thereby minimizing the startup time. </p> </li> <li> <p>Any arbitrary string value can be used for the session tag. If no session exists a new one would be created using the tag value.</p> </li> </ul>"},{"location":"fabric_tools/pipelines/#script-activity","title":"Script Activity","text":"<ol> <li> <p>Select on add a pipeline activity and search for Script.</p> </li> <li> <p>Alternately, you can search for Script Activity in the pipeline Activities ribbon at the top, and select it to add it to the pipeline canvas,=.</p> </li> <li> <p>Select the new Script activity on the canvas if it isn't already selected.</p> </li> </ol>"},{"location":"fabric_tools/pipelines/#configure-the-script-activity","title":"Configure the script activity","text":"<ol> <li> <p>Select the Settings tab, and choose an existing connection from the dropdown, or create a new one.</p> </li> <li> <p>After selecting your connection, you can choose either Query to get a data result or NonQuery for any catalog operations.</p> <p> Query or NonQuery (Microsoft, 2024) </p> </li> <li> <p>You can input content into the script expression box. We have multiple ways in which you can input script content into the expression box.</p> <ul> <li>You can add dynamic content by either clicking in the box or clicking on the \"dynamic content\" icon on the right-hand size. A flyout that showcases dynamic content and functions that you can seamlessly use to build your expressions appears.</li> </ul> <p> Script Activity Flyout (Microsoft, 2024) </p> <ul> <li>You can also directly edit your script code in the code editor by selecting the pencil icon on the irght-hand size, as seen in the following screenshot. After you select it, a new dialog box will pop up so that you can seamlessly write and edit your code.</li> </ul> <p> Script Activity Edit Code (Microsoft, 2024) </p> </li> </ol> <p> Dataflow</p> <p>ML Model </p>"},{"location":"fabric_tools/reports/","title":"Reports","text":""},{"location":"fabric_tools/reports/#create-report-in-power-bi-service","title":"Create Report in Power BI Service","text":"<ol> <li> <p>In the data storage that you want to create a report from, using the ribbon and the main home tab, navigate to the New report button. This option provides a native, quick way to create report built on top of the default Power BI semantic model.</p> <p> Home Ribbon (Microsoft, 2025) </p> </li> <li> <p>If no tables have been added to the default Power BI semantic model, the dialog first automatically adds tables, prompting the user to confirm or manually select the tables included in the canonical default semantic model fist, ensuring there's always data first.</p> </li> <li> <p>With a default semantic model that has tables, the New report opens a browser tab to the report editing canvas to a new report that is built on the semantic model. When you save your new report you're prompted to choose a workspace, provided you have write permissions for that workspace.</p> </li> </ol>"},{"location":"fabric_tools/reports/#use-default-power-bi-semantic-model-within-workspace","title":"Use Default Power BI Semantic Model Within Workspace","text":"<ol> <li> <p>Using the default semantic model and action menu in the workspace: In the Microsoft Fabric workspace, navigate to the default Power BI semantic model and select the More menu (...) to create a report in the Power BI service.</p> <p> Create Report (Microsoft, 2025) </p> </li> <li> <p>Select Create report to open the report editing canvas to a new report on the semantic model. When you save your new report, it's saved in the workspace that contains the semantic model as long as you have write permissions on that workspace.</p> </li> </ol>"},{"location":"fabric_tools/reports/#create-reports-in-the-power-bi-desktop","title":"Create Reports in the Power BI Desktop","text":"<ol> <li> <p>Use Data hub menu in the ribbon to get list of all items.</p> </li> <li> <p>Select the warehouse that you would like to connect.</p> </li> <li> <p>On the Connect button, select the dropdown list, and select Connect to SQL endpoint.</p> <p> Data Hub Power BI Desktop (Microsoft, 2025) </p> </li> </ol> <p> ML Model</p>"},{"location":"fabric_tools/semantic_model/","title":"Semantic Model","text":"<ul> <li> <p>Power BI semantic models are a logical description of an analytical domain, with metrics, business friendly terminology, and repreentation, to enable deeper analysis.</p> </li> <li> <p>Typically a star schema with facts that represent a domain, and dimensions that allow you to analyze, or slice and dice the domain to drill down, filter, and calculate different analyses.</p> </li> <li> <p>With the semantic model, the semantic model is created for you automatically, and you choose which tables, relationships, and measures are to be added, and the aforementioned business logic gets inherited from the parent lakehouse or warehouse respectively, jump-starting the downstream analytics experience for business intelligence and analysis with an item in Microsoft Fabric that is managed, optimized, and kept in sync with no user intervention.</p> </li> </ul>"},{"location":"fabric_tools/semantic_model/#direct-lake-mode","title":"Direct Lake Mode","text":"<ul> <li> <p>The technology is based on the idea of consuming parquet-formatted files directly from a data lake, without having a query a Warehouse or SQL analytics endpoint, and without having to import or duplicate data into a Power BI semantic model.</p> </li> <li> <p>Direct Lake mode is the storage mode for default Power BI semantic models, and new Power BI semantic models created in a warehouse or SQL analytics endpoint.</p> </li> <li> <p>Calculated columns are not supported in Direct Lake semantic model.</p> </li> </ul>"},{"location":"fabric_tools/semantic_model/#directquery-vs-import-vs-direct-lake","title":"DirectQuery vs. Import vs. Direct Lake","text":"<ul> <li> <p>DirectQuery Mode: the Power BI engine directly queries the data from the source for each query execution, and the query performance depends on the data retrieval speed. </p> <ul> <li>DirectQuery eliminates the need to copy data, ensuring that any changes in the source are immediately reflected in query results.</li> </ul> </li> <li> <p>Import Mode: the Power BI engine must first copy the data into the memory, at data refresh time. Any changes to the underlying data source are picked up during the next data refresh.</p> <ul> <li>The performance is better because the data is readily available in memory, without having to query the data from the source for each query execution.</li> </ul> </li> <li> <p>Direct Lake Mode: eliminates the Import requirement to copy the data by consuming the data files directly into memory. Because there is no explicit import process, it's possible to pick up any changes at the source as they occur. </p> <ul> <li> <p>Ideal choice for analyzing very arge datasets and datasets with frequent updates at the source. </p> </li> <li> <p>Direct Lake will automatically fallback to Direct Query using the SQL analytics endpoint of the Warehouse or SQL analytics endpoint when Direct Lake exceeds limits for the SKU, or uses features not supported, allowing report users to continue uninterrupted.</p> </li> </ul> </li> </ul>"},{"location":"fabric_tools/semantic_model/#decision-making","title":"Decision-making","text":"Feature / Consideration Import Mode DirectQuery Mode Direct Lake Mode Performance Fastest \u2013 data is cached Slower \u2013 queries data live Fast \u2013 queries Delta Lake directly Data Freshness Stale \u2013 depends on refresh schedule Real-time or near real-time Near real-time with no refresh needed Storage Location Power BI dataset External source (SQL, Fabric Warehouse) Fabric Lakehouse (Delta Lake) Data Volume Limited by model size (1GB \u2013 10GB+) Handles large data volumes Handles large data volumes Transformations Full Power Query support Limited transformation support No Power Query \u2013 relies on pre-modeled data Modeling Flexibility Full DAX and relationships support Limited (e.g., no many-to-many joins) Full DAX and relationships support Offline Availability Fully available offline Requires live connection Requires Fabric environment connectivity Use Cases Dashboards with static/historical data Live dashboards, operational reporting Lakehouse analytics, semi-real-time views Best For Frequent analysis of curated data Monitoring KPIs with up-to-date data High-volume data with fast access needed Refresh Required Yes \u2013 scheduled or manual No \u2013 always live No \u2013 data reflects lake content directly Security Management Row-level security in Power BI Security managed at source Security via Lakehouse and semantic model <ul> <li> <p>Use cases:</p> <ul> <li> <p>Import Mode: Best for curated datasets, fast performance, and dashboards that don\u2019t require constant updates.</p> </li> <li> <p>DirectQuery: Best when real-time or frequently changing data is essential, and the data source supports high concurrency.</p> </li> <li> <p>Direct Lake: Ideal for large-scale, cloud-native analytics with minimal latency from lakehouse storage.</p> </li> </ul> </li> </ul>"},{"location":"fabric_tools/semantic_model/#default-power-bi-semantic-model","title":"Default Power BI Semantic Model","text":"<ul> <li> <p>When you create a warehouse or SQL analytics endpoint, a default Power BI semantic model is created. The default semantic model is represented with the (default) suffix.</p> <ul> <li>You can use Manage default semantic model to choose tables to add.</li> </ul> </li> </ul>"},{"location":"fabric_tools/semantic_model/#access-the-default-power-bi-semantic-model","title":"Access the default Power BI semantic model","text":"<ul> <li>Go to your workspace, and find the semantic model that matches the name of the semantic model that matches the name of the desired lakehouse. The default Power BI semantic model follows the naming convention of the lakehouse.</li> </ul> Finding Default Semantic Model (Microsoft, 2025)"},{"location":"fabric_tools/semantic_model/#creating-a-new-power-bi-semantic-model-in-direct-lake-storage-mode","title":"Creating a new Power BI semantic model in Direct Lake storage mode","text":"<ul> <li> <p>You can create additional Power BI semantic models in Direct Lake mode using SQL analytics endpoint or Warehouse data.</p> </li> <li> <p>These new Power BI semantic models can be edited using Open data model and can be used with other features such as write DAX queries and semantic model row-level security.</p> </li> <li> <p>The New Power BI semantic model button creates a new blank semantic model separate from the default semantic model.</p> </li> </ul>"},{"location":"fabric_tools/semantic_model/#steps-to-create-semantic-model-in-direct-lake-mode","title":"Steps to create semantic model in Direct Lake mode","text":"<ol> <li> <p>Open the lakehouse and select New Powe BI semantic model from the ribbon.</p> </li> <li> <p>Alternatively, open a Warehouse or Lakehouse's SQL analytics endpoint, first select the Reporting ribbon, then select New Power BI semantic model**.</p> </li> <li> <p>Enter a name for the new semantic model, select a workspace to save it in, and pick the tables to include. Then select Confirm.</p> </li> <li> <p>The new Power BI semantic model can be edited in the workspace, where you can add relationships, measures, rename tables and columns, choose how values are displayed in report visuals, and much more. If the model view does not show after creation, check the pop-up blocker of your browser.</p> </li> <li> <p>To edit the Power BI semantic model later, select Open data model from the semantic model context menu or item details page to edit the semantic model further.</p> </li> </ol>"},{"location":"fabric_tools/semantic_model/#creating-a-new-power-bi-semantic-model-in-import-or-directquery-storage-mode","title":"Creating a New Power BI Semantic Model in Import or DirectQuery Storage Mode","text":"<ol> <li> <p>Open Power BI Desktop, sign in, and select OneLake.</p> </li> <li> <p>Choose the SQL analytics endpoint of the lakehouse or warehouse.</p> </li> <li> <p>Select the connect button dropdown list and choose Connect to SQL endpoint.</p> </li> <li> <p>Select import or DirectQuery storage mode and the tables to add to the semantic model.</p> </li> </ol> <p>To create relationships in data model, refer to Architecture: Data Model Relationships.</p>"},{"location":"fabric_tools/semantic_model/#connecting-to-semantic-model-in-power-bi-desktop","title":"Connecting to Semantic Model in Power BI Desktop","text":"<ol> <li> <p>Open Power BI Desktop</p> </li> <li> <p>Open the report that you wish to connect the semantic model to (alternatively, create a blank report).</p> </li> <li> <p>On the Home tab, click on Get Data.</p> <p> </p> </li> <li> <p>In the dropdown, click Power BI semantic models.</p> <p> </p> </li> <li> <p>Click on the semantic model that you wish to connect to. The Connect button should turn green, and there should also be a button to click for a dropdown on the button.</p> <ul> <li>If there is no dropdown, you can simply click on Connect, and skip the next step.</li> </ul> </li> <li> <p>Click on Edit in the dropdown.</p> <p> </p> </li> </ol> <p> Notebook</p> <p>Warehouse </p>"},{"location":"fabric_tools/sql_database/","title":"SQL Database","text":"<ul> <li> <p>SQL Database in Microsoft Fabric is a developer-friendly transactional database, based on Azure SQL Database, that allows you to easily create your operational database in Fabric.</p> </li> <li> <p>A SQL Database in Fabric uses the same SQL Database Engine as Azure SQL Database.</p> </li> <li> <p>SQL Database in Fabric is:</p> <ul> <li> <p>The home in Fabric for OLTP workloads.</p> </li> <li> <p>Easy to configure and manage.</p> </li> <li> <p>Set up for analytics by automatically replicating the data into OneLake near real time.</p> </li> <li> <p>Integrated with development frameworks and analytics.</p> </li> <li> <p>Based on the underlying technology of Mirroring in Fabric</p> </li> <li> <p>Queried in all the same ways as Azure SQL Database, plus a web-based editor in the Fabric portal.</p> </li> </ul> </li> </ul>"},{"location":"fabric_tools/sql_database/#why-sql-database-in-fabric","title":"Why SQL Database in Fabric?","text":"<ul> <li> <p>Data is accessible from other items in Fabric.</p> </li> <li> <p>SQL Database in Fabric creates three items in your Fabric workspace:</p> <ul> <li> <p>Data in your SQL database is automatically replicated of into the OneLake and converted to Parquet, in an analytics-ready format. This enables downstream scenarios like data engineering, data science, and more.</p> </li> <li> <p>A SQL analytics endpoint.</p> </li> <li> <p>A default semantic model.</p> </li> </ul> </li> </ul>"},{"location":"fabric_tools/sql_database/#cross-database-queries","title":"Cross-database Queries","text":"<ul> <li> <p>With the data from your SQL database automatically stored in OneLake, you can write cross-database queries, joining data from other SQL databases, mirrored databases, warehouses, and the SQL analytics endpoint in a T-SQL query.</p> </li> <li> <p>All this is currently possible with queries on the SQL analytics endpoint of the SQL database, or lakehouse.</p> </li> <li> <p>Below is an example of how to write cross-database queries:</p> SQL<pre><code>SELECT * \nFROM ContosoWarehouse.dbo.ContosoSalesTable AS Contoso\nINNER JOIN AdventureWorksLT.SalesLT.Affiliation AS Affiliation\nON Affiliation.AffiliationId = Contoso.RecordTypeID;\n</code></pre> </li> </ul>"},{"location":"fabric_tools/sql_database/#creating-a-sql-database","title":"Creating a SQL Database","text":"<ol> <li> <p>In the workspace that you wnat to create a SQL Database in, select + New Item.</p> </li> <li> <p>Under Store Data, select the SQL database (preview) tile.</p> </li> <li> <p>Provide a name for the new database. Select Create.</p> </li> <li> <p>When the new database is provisioned, on the Home page for the database, notice the Explorer pane showing database objects.</p> </li> <li> <p>Under Build your database, three useful tiles can help you get your newly created database up and running.</p> <ul> <li> <p>Sample data: lets you import a sample data into your Empty database.</p> </li> <li> <p>T-SQL: gives you a web-editor that can be used to write T-SQL to create database object like schema, tables, views, and more.</p> </li> <li> <p>Connection strings: shows the SQL database connection string that is required when you want to connect using SQL Server Management Studio, the mssql extension with Visual Studio Code, or other external tools.</p> </li> </ul> </li> </ol>"},{"location":"fabric_tools/sql_database/#creating-a-table-with-t-sql-queries","title":"Creating a Table with T-SQL Queries","text":"<ol> <li> <p>Open your SQL Database.</p> </li> <li> <p>Select the New Query button in the main ribbon.</p> </li> <li> <p>Create the definition of your table in T-SQL with this sample:</p> SQL<pre><code>CREATE TABLE dbo.products ( \nproduct_id INT IDENTITY(1000,1) PRIMARY KEY, \nproduct_name VARCHAR(256), \ncreate_date DATETIME2 \n) \n</code></pre> </li> <li> <p>Once you have a table design you like, select Run in the toolbar of the query window.</p> </li> <li> <p>If the Object Explorer is already expanded to show tables, it will automatically refresh to show the new table upon create. If not, expand the tree to see the new table.</p> </li> </ol>"},{"location":"fabric_tools/sql_database/#query-the-database","title":"Query the Database","text":"<ul> <li> <p>For lightweight queries where you just want to see the structure of a table, the type of data in the table or even what columns the table has, just select on table's name in the Object Explorer. The top 1,000 rows will automatically be returned.</p> </li> <li> <p>If you want to write a new query with T-SQL, select the New Query button from the toolbar of the query editor.</p> </li> </ul> <p> Warehouse</p> <p>Dataflow </p>"},{"location":"fabric_tools/sql_endpoint/","title":"SQL Analytics Endpoint","text":"<ul> <li>In a SQL analytics endpoint, you can analyze data in Delta tables using T-SQL language, save functions, generate views, and apply SQL security.</li> </ul> SQL Analytics Endpoint Page (Microsoft, 2025) <ul> <li> <p>Creating a lakehouse creates a SQL Analytics endpoint, which points to the lakehouse Delta table storage. Once you create a Delta table in the lakehouse, it's available for querying using the SQL Analytics endpoint.</p> <ul> <li>Warehouse and SQL database also creates a SQL Analytics endpoint when created.</li> </ul> </li> <li> <p>SQL analytics endpoint operates in read-only mode over lakehouse Delta tables.</p> <ul> <li> <p>You can only read data from Delta tables using the SQL analytics endpoint.</p> </li> <li> <p>However, you also have the flexibility to create functions, define views, and implement SQL object-level security to manage access and structure data effectively.</p> </li> </ul> </li> <li> <p>External Delta tables created with Spark code won't be visible to the SQL analytics endpoint.</p> <ul> <li>To solve this, use shortcuts in Table space to make external Delta table svisible to the SQL analytics endpoint.</li> </ul> </li> </ul> <p> Lakehouse</p> <p>Notebook </p>"},{"location":"fabric_tools/warehouse/","title":"Warehouse","text":""},{"location":"fabric_tools/warehouse/#creating-a-warehouse","title":"Creating a Warehouse","text":"<p>To decide between the different data storage options, refer to Warehouse, SQL Database, or Lakehouse?</p> <ul> <li> <p>You can start creating your warehouse from the workspace. Select + New Item and look for the Warehouse card under the Store Data section.</p> </li> <li> <p>An empty warehouse is created for you to start creating objects in the warehouse.</p> </li> </ul> Warehouse Cards (Microsoft, 2025) <ul> <li>Another option available to create your warehouse is through the Create button in the navigation pane. Look for the Warehouse under Data Warehouse.</li> </ul>"},{"location":"fabric_tools/warehouse/#data-ingestion-options","title":"Data Ingestion Options","text":"<ul> <li> <p>COPY (Transact-SQL): the COPY statement offers flexible, high-throughput dadta ingestion from an external Azure storage account. You can use the COPY statement as part of your existing ELT/ETL logic in Transact-SQL code.</p> </li> <li> <p>Data pipelines: pipelines offer a code-free or low-code experience for data ingestion. Using pipelines, you can orchestrate robust workflows for a full Extract, Transform, Load (ETL) experience that includes activities to help prepare the destination environment, run custom Transact-SQL statements, perform lookups, or copy data from a source to a destination.</p> </li> <li> <p>Dataflows: an alternative to pipelines, dataflows enable easy data preparation, cleaning, and transformation using a code-free experience. </p> <ul> <li>Since the majority of our files are currently stored in Sharepoint folders, this is the default method to bring in our files.</li> </ul> </li> <li> <p>Cross-warehouse ingestion: data ingestion from workspace sources is also possible. This scenario might be required when there's the need to create a new table with a subset of a different table, or as a result of joining different tables in the warehouse and in the lakehouse. For cross-warehouse ingetsion, in addition to the options mentioned, Transact-SQL features such as INSERT...SELECT, SELECT INTO, or CREATE TABLE AS SELECT (CTAS) work cross-warehouse within the same workspace.</p> </li> </ul>"},{"location":"fabric_tools/warehouse/#deciding-on-data-ingestion-tool","title":"Deciding on Data Ingestion Tool","text":"<ul> <li> <p>COPY (Transact-SQL): for code-rich data ingetsion operations, for the highest data ingestion throughput possible, or when you need to add data ingestion as part of a T-SQL logic. For syntax, see COPY INTO(T-SQL).</p> </li> <li> <p>Data pipelines: for code-free or low-code, robust data ingestion workflows that run repeatedly, at a schedule, or that involces large volumes of data. For more information, see Data Pipelines.</p> </li> <li> <p>Dataflows: for a code-free experience that sllows custom transformations to source data before it's ingested. These transformations include (but aren't limited to) changing data types, adding or removing columns, or using functions to produce calculated columns. For more information, see Dataflow.</p> </li> <li> <p>Cross-warehouse ingestion: for code-rich experiences to create new tables with source data within the same workspace.</p> </li> </ul>"},{"location":"fabric_tools/warehouse/#analyzing-data-in-warehouse","title":"Analyzing Data in Warehouse","text":""},{"location":"fabric_tools/warehouse/#notebook","title":"Notebook","text":""},{"location":"fabric_tools/warehouse/#t-sql-notebook","title":"T-SQL Notebook","text":"<ol> <li> <p>Open the workspace with your warehouse that you want to analyze, and open the warehouse.</p> </li> <li> <p>On the Home ribbon, open the New SQL query dropdown listm and then select New SQL query in notebook.</p> <p> New SQL Query in Notebook (Microsoft, 2025) </p> </li> <li> <p>In the Explorer pane, select Warehouses to reveal the objects of the warehouse.</p> </li> <li> <p>To generate a SQL template to explore data, to the right of the table that you want to analyze, select the ellipse (...), and then select SELECT TOP 100.</p> <p> SELECT TOP 100 (Microsoft, 2025) </p> </li> <li> <p>To run the T-SQL code in this cell, select the Run cell button for the code cell.</p> <p> Run T-SQL Code Cell (Microsoft, 2025) </p> </li> <li> <p>Review the query result in the results pane.</p> </li> </ol>"},{"location":"fabric_tools/warehouse/#lakehouse-shortcut","title":"Lakehouse Shortcut","text":"<ol> <li> <p>Open the desired workspace landing page.</p> </li> <li> <p>Select the + New Item to display the full list of available item types.</p> </li> <li> <p>From the list, in the Store data section, select the Lakehouse item type.</p> </li> <li> <p>In the New lakehouse window, entire the name that you want to name your lakehouse.</p> </li> <li> <p>Select Create.</p> </li> <li> <p>When the new lakehouse opens, in the landing page, select the New shortcut option.</p> <p> Lakehouse: new Shortcut (Microsoft, 2025) </p> </li> <li> <p>In the New shortcut window, select the Microsoft OneLake option.</p> </li> <li> <p>In the Select a data source type window, select the warehouse that you want to create a shortcut to, and then select Next.</p> </li> <li> <p>In the OneLake object browser, expand Tables, expand the dbo schema, and then select the checkbox for the tables that you want to analyze.</p> </li> <li> <p>Select Next.</p> </li> <li> <p>Select Create.</p> </li> <li> <p>In the Explorer pane, select a table to preview the data, and then review the data retrieved from the table in the warehouse.</p> </li> <li> <p>To create a notebook to query the table, on the Home ribbon, in the Open notebook dropdown list, select New notebook.</p> </li> <li> <p>In the Explorer pane, select Lakehouses.</p> </li> <li> <p>Drag the table you want to analyze to the open notebook cell.</p> <p> Drag Table to Cell (Microsoft, 2025) </p> </li> <li> <p>Notice the PySpark query that was added to the notebook cell. This query retrieves the first 1,000 rows from the shortcut.</p> <ul> <li>This notebook can also be opened in VS Code.</li> </ul> </li> <li> <p>On the Home ribbon, select the Run all button.</p> </li> <li> <p>Review the query result in the results pane.</p> </li> </ol>"},{"location":"fabric_tools/warehouse/#connect-using-power-bi","title":"Connect Using Power BI","text":"<ol> <li> <p>Select the Warehouse.</p> </li> <li> <p>Choose entities.</p> </li> <li> <p>Load Data - choose a data connectivity mode: Import or DirectQuery.</p> </li> </ol> <p>For more information, see Using Fabric Tools: Reports.</p> <p> Semantic Model</p> <p>SQL Database </p>"},{"location":"finance_terms/finance_accounting_terms/","title":"Financial Terms Overview","text":"<p>This document explains the core accounting terms, financial statements, and sign conventions used in reporting. It also highlights where these terms appear in the Financial KPI Documentation.</p>"},{"location":"finance_terms/finance_accounting_terms/#1-core-financial-statements","title":"1. Core Financial Statements","text":""},{"location":"finance_terms/finance_accounting_terms/#11-pl-profit-loss-statement-income-statement","title":"1.1 P&amp;L (Profit &amp; Loss Statement / Income Statement)","text":"<p>Shows the company\u2019s financial performance over a period (monthly, quarterly, annual).</p> <p>Key Components: - Revenue \u2013 Total sales or income earned from operations.   - Used in: Gross Profit Margin, Net Profit Margin, Cash Flow Margin. - COGS (Cost of Goods Sold) \u2013 Direct costs of goods/services sold.   - Used in: Gross Margin, Inventory Turnover, CCC (DIO, DPO). - Gross Margin = Revenue \u2212 COGS. - OPEX (Operating Expenses) \u2013 Costs related to operations. Indirect costs not tied to production:   - Selling &amp; Distribution   - General &amp; Administrative (G&amp;A)   - Research &amp; Development (R&amp;D) - Operating Profit (EBIT: Earnings Before Interest and Taxes) = Gross Margin \u2212 OPEX.   - Used in: Operating Profit Margin, Interest Coverage Ratio. - Net Income (Profit After Tax) = EBIT \u2212 Interest Expense \u2212 Taxes \u00b1 Non-operating items.   - Used in: Net Profit Margin, ROA, ROE. - Non-Operating Income/Expense \u2013 Items outside core business (e.g., investment gains, one-time losses, financing costs). - CapEx (Capital Expenditure) \u2013 Cash spent on acquiring or upgrading assets.   - Note: CapEx does not appear in the P&amp;L; only depreciation (the allocation of CapEx over time) is recorded as an expense. CapEx is shown in the Cash Flow Statement (Investing Activities).</p>"},{"location":"finance_terms/finance_accounting_terms/#2-bs-balance-sheet","title":"2. BS (Balance Sheet)","text":"<p>Trial Balance Equation: [ \\textbf{Assets = Liabilities + Equity} ]</p> <p>Key Components: - Assets \u2013 Resources owned that provide future benefits.   - Example: cash, accounts receivable, inventory, equipment   - Used in: ROA, Current Ratio, Quick Ratio, CCC (DIO, DSO). - Liabilities \u2013 Obligations or debt owed to third parties.    - Example: loans, accounts payable   - Used in: Debt-to-Equity, Debt-to-Assets, Current Ratio, Quick Ratio. - Equity \u2013 Residual interest after liabilities are deducted. Ownership interest, has two main components: 1. Capital Contributions and 2. Retained Earnings.   - Used in: ROE, Debt-to-Equity.</p>"},{"location":"finance_terms/finance_accounting_terms/#21-assets","title":"2.1 Assets","text":"<p>Definition: Resources owned or controlled by the company that are expected to deliver future economic benefits.  </p> <p>Types: - Current Assets \u2013 Realized/used within 12 months.   - Examples:     - Cash and Cash Equivalents \u2013 Highly liquid funds.     - Accounts Receivable (AR): Money customers owe to the company for credit sales.     - Inventory \u2013 Goods held for sale.     - Prepaid Expenses \u2013 Payments made in advance for future benefits. - Non-Current Assets \u2013 Long-term resources.   - Examples: Property, Plant &amp; Equipment (PPE), buildings, Intangibles (patents, goodwill).</p>"},{"location":"finance_terms/finance_accounting_terms/#22-liabilities","title":"2.2 Liabilities","text":"<p>Definition: Present financial obligations or debts to transfer/owed resources to external parties due to past events. They typically require settlement in the form of cash, goods, or services.</p> <p>Types: - Current Liabilities \u2013 Due within 12 months.   - Examples:     - Accounts Payable (AP): Money the company owes to suppliers for goods/services purchased on credit.     - Short-term Loans \u2013 Borrowings repayable within a year.     - Accrued Expenses \u2013 Expenses incurred but not yet paid (e.g., wages, taxes). - Non-Current Liabilities \u2013 Due after 12 months.   - Examples: Long-term loans, Bonds payable, Lease obligations.</p>"},{"location":"finance_terms/finance_accounting_terms/#23-equity","title":"2.3 Equity","text":"<p>Definition: The residual value in the company after liabilities are subtracted from assets. Represents shareholders\u2019 ownership value.  </p> <p>Components: 1. Share Capital \u2013 Funds contributed by owners/shareholders. 2. Retained Earnings (RE) \u2013 Accumulated profits reinvested into the business.    - Current Year Net Income - The company\u2019s net profit for the current fiscal year \u2192 closes into RE in the Balance Sheet. 3. Additional Paid-In Capital (APIC) \u2013 Funds received above the per value of share. 4. Other Comprehensive Income (OCI) \u2013 Gains/losses bypassing Net Income (e.g., FX, revaluations).  </p>"},{"location":"finance_terms/finance_accounting_terms/#3-cf-cash-flow-statement","title":"3. CF (Cash Flow Statement)","text":"<p>Tracks cash inflows/outflows across three sections.</p> <ol> <li>Operating Activities (OCF) \u2013 Cash from day-to-day core operations.  </li> <li>Example: sales </li> <li>Used in: Operating Cash Flow, Free Cash Flow, Cash Flow Margin.  </li> <li>Investing Activities \u2013 CAPEX, asset purchases/sales, investments.  </li> <li>Financing Activities \u2013 Borrowings/repayments, equity issuance, dividends.  </li> </ol> <p>Example: CAPEX Purchase ($100 equipment) - Balance Sheet: Equipment (Asset) \u2191 $100 - Cash Flow (Investing): Cash \u2193 $100 - P&amp;L: No immediate expense \u2192 depreciation spreads cost over time  </p>"},{"location":"finance_terms/finance_accounting_terms/#4-alore-and-sign-conventions","title":"4. ALORE and Sign Conventions","text":"<p>Accounting balances follow Debit (Dr) and Credit (Cr) rules summarized by ALORE:</p> <ul> <li>A \u2013 Assets </li> <li>L \u2013 Liabilities </li> <li>O \u2013 Owners\u2019 Equity </li> <li>R \u2013 Revenue </li> <li>E \u2013 Expenses</li> </ul>"},{"location":"finance_terms/finance_accounting_terms/#41-debit-vs-credit-rules","title":"4.1 Debit vs Credit Rules","text":"<ul> <li>Debits (Dr): Increase Assets &amp; Expenses; decrease Liabilities, Equity, Revenue.  </li> <li>Credits (Cr): Increase Liabilities, Equity, Revenue; decrease Assets &amp; Expenses.  </li> </ul>"},{"location":"finance_terms/finance_accounting_terms/#42-system-display-conventions","title":"4.2 System Display Conventions","text":"<p>Some ERP systems display balances as positive/negative: - Assets &amp; Liabilities \u2192 shown as positive. [because they represent the total amounts owned (Assets) or owed (Liabilities).] - Revenue &amp; Equity \u2192 may show as negative when increasing (credit-normal). [because they are credit accounts, but negative here means an increase in those accounts. Revenue increases equity.] - Expenses \u2192 shown as positive when increasing (debit-normal). [because they increase with debits (they reduce equity). Expenses reduce equity]</p> <p>This ensures the accounting equation (Assets = Liabilities + Equity) stays balanced.</p>"},{"location":"finance_terms/finance_accounting_terms/#43-how-to-read-signs","title":"4.3 How to Read Signs","text":"<ul> <li>Positive Asset: The company owns value (e.g., $100 in cash).</li> <li>Positive Liability: The company owes value (e.g., $50 loan).</li> <li>Negative Revenue: This means revenue is increasing (a credit).</li> <li>Positive Expense: Expenses are increasing (a debit).</li> </ul>"},{"location":"finance_terms/finance_accounting_terms/#44-quick-reference-display-signs","title":"4.4 Quick Reference: Display Signs","text":"Category Displayed As Negative Means Assets (A) Positive Asset decreased Liabilities (L) Positive Liability decreased Equity (O) Often negative Equity increased Revenue (R) Often negative Revenue increased Expenses (E) Positive Refund/reversal"},{"location":"finance_terms/finance_accounting_terms/#45-quick-reference-debitcredit-effects","title":"4.5 Quick Reference: Debit/Credit Effects","text":"Category Debit (Dr) Credit (Cr) Assets (A) Increase Decrease Liabilities (L) Decrease Increase Equity (O) Decrease Increase Revenue (R) Decrease Increase Expenses (E) Increase Decrease"},{"location":"finance_terms/finance_kpi/","title":"Financial KPI Documentation (add GL accounting terms)","text":""},{"location":"finance_terms/finance_kpi/#1-overview","title":"1. Overview","text":"<p>Financial Key Performance Indicators (KPIs) are metrics used to evaluate a company\u2019s financial health, operational efficiency, and overall performance. These KPIs are essential for tracking progress against strategic objectives, identifying areas for improvement, and making informed financial decisions.</p>"},{"location":"finance_terms/finance_kpi/#2-key-kpi-formulas","title":"2. Key KPI Formulas","text":""},{"location":"finance_terms/finance_kpi/#21-cash-conversion-cycle-ccc","title":"2.1 Cash Conversion Cycle (CCC)","text":"<p>The Cash Conversion Cycle (CCC) measures the time (in days) it takes for a company to convert its investments in inventory and other resources into cash flows from sales. It is measured using dollar amounts.</p> <p>Formula: <code>CCC = DIO + DSO - DPO</code></p> <ul> <li>DIO (Days Inventory Outstanding): Average number of days inventory is held before it\u2019s sold.  </li> <li>DSO (Days Sales Outstanding): Average number of days it takes to collect payment after a sale.  </li> <li>DPO (Days Payable Outstanding): Average number of days the company takes to pay its suppliers.</li> </ul> Component Formula GL Accounts Used Purpose DIO (Days Inventory Outstanding) (Average Inventory \u00f7 COGS) \u00d7 365 Inventory (Current Assets), COGS (Expense) Measures how long inventory is held. DSO (Days Sales Outstanding) (Accounts Receivable \u00f7 Net Credit Sales) \u00d7 365 Accounts Receivable (Current Assets), Revenue (Sales GL) Measures how quickly receivables are collected. DPO (Days Payable Outstanding) (Accounts Payable \u00f7 COGS) \u00d7 365 Accounts Payable (Current Liabilities), COGS (Expense) Measures how long supplier payments are delayed. <p>Interpretation: Shorter CCC = better liquidity and efficiency.</p>"},{"location":"finance_terms/finance_kpi/#22-dsi-days-sales-of-inventory","title":"2.2 DSI (Days Sales of Inventory)","text":"<p>This is primarily a Supply Chain Management (SCM) metric, used to measure how long (in days) it takes for inventory to be sold or used. It is typically measured using units.</p> <p>Formula: <code>DSI = (Average Inventory / Cost of Goods Sold (COGS)) * 365</code></p> <ul> <li>Often derived using Inventory on Hand (IOL units) for SCM analysis.</li> <li>GL Accounts Used: Inventory (Balance Sheet), COGS (Income Statement).  </li> <li>Interpretation: Measures how quickly inventory is sold or consumed.</li> </ul>"},{"location":"finance_terms/finance_kpi/#23-vaw-value-added-wages","title":"2.3 VAW (Value Added Wages)","text":"<p>Value Added Wages (VAW) refers to the wages paid to employees as part of the value-added process in production or services. It reflects the company\u2019s investment in human capital and can be used to assess labor productivity and contribution to value creation.</p> <p>Formula: <code>VAW = (Wages \u00f7 Value Added) \u00d7 100</code> </p> <ul> <li>GL Accounts Used: Payroll Expense (Operating Expense GL), Benefits Expense.  </li> <li>Interpretation: Shows the share of value added distributed to employees, reflecting labor productivity.</li> </ul>"},{"location":"finance_terms/finance_kpi/#3-categories-of-financial-kpis","title":"3. Categories of Financial KPIs","text":""},{"location":"finance_terms/finance_kpi/#31-profitability-kpis","title":"3.1 Profitability KPIs","text":"KPI Formula GL Accounts Used Purpose Gross Profit Margin (Gross Profit \u00f7 Revenue) \u00d7 100 Revenue (Sales), COGS (Expense) Profitability after direct costs. Operating Profit Margin (Operating Profit \u00f7 Revenue) \u00d7 100 Revenue, Operating Expenses (SG&amp;A, R&amp;D) Efficiency from operations. EBIT (Earnings Before Interest and Taxes) Revenue \u2212 COGS \u2212 Operating Expenses \u2212 Depreciation &amp; AmortizationorNet Income + Interest Expense + Taxes Revenue (Sales), COGS, Operating Expenses (SG&amp;A, R&amp;D, Depreciation/Amortization) Core measure of operating profitability before financing and taxes. Net Profit Margin (Net Profit \u00f7 Revenue) \u00d7 100 Revenue, All Expenses (Operating, Interest, Taxes) Overall profitability. ROA (Return on Assets) (Net Income \u00f7 Total Assets) \u00d7 100 Net Income (P&amp;L), Total Assets (Balance Sheet) Measures efficiency of asset use. ROE (Return on Equity) (Net Income \u00f7 Shareholder\u2019s Equity) \u00d7 100 Net Income, Equity Accounts Profitability for shareholders. <p>Interpretation: Higher margins, ROA, and ROE generally indicate stronger profitability, but should be benchmarked against industry peers.</p>"},{"location":"finance_terms/finance_kpi/#32-liquidity-kpis","title":"3.2 Liquidity KPIs","text":"KPI Formula GL Accounts Used Purpose Current Ratio Current Assets \u00f7 Current Liabilities Cash, AR, Inventory vs. AP, Accruals, Short-Term Debt Measures short-term solvency. Quick Ratio (Acid Test) (Current Assets \u2212 Inventory) \u00f7 Current Liabilities Cash, AR, Short-Term Investments vs. CL Tests immediate liquidity without relying on inventory. Cash Conversion Cycle (CCC) DIO + DSO \u2212 DPO Inventory, AR, AP, COGS, Sales Efficiency of working capital."},{"location":"finance_terms/finance_kpi/#33-efficiency-kpis","title":"3.3 Efficiency KPIs","text":"KPI Formula GL Accounts Used Purpose Inventory Turnover COGS \u00f7 Average Inventory COGS, Inventory Efficiency of inventory management. AR Turnover Net Credit Sales \u00f7 Avg. Accounts Receivable (AR) Sales, Accounts Receivable Collection efficiency. AP Turnover COGS \u00f7 Avg. Accounts Payable (AP) COGS, Accounts Payable Supplier payment speed."},{"location":"finance_terms/finance_kpi/#34-solvency-kpis","title":"3.4 Solvency KPIs","text":"KPI Formula GL Accounts Used Purpose Debt-to-Equity Total Liabilities \u00f7 Shareholder\u2019s Equity Total Liabilities, Equity Accounts Capital structure health. Debt-to-Assets Total Liabilities \u00f7 Total Assets Liabilities, Assets Measures leverage. Interest Coverage EBIT \u00f7 Interest Expense Operating Profit (EBIT), Interest Expense Ability to cover debt costs."},{"location":"finance_terms/finance_kpi/#35-revenue-kpis","title":"3.5 Revenue KPIs","text":"KPI Formula GL Accounts Used Purpose Total Revenue Sum of Sales Sales Revenue GL Sales performance. Revenue Growth Rate ((Current Revenue \u2212 Previous Revenue) \u00f7 Previous Revenue) \u00d7 100 Sales GL (Period over Period) Growth tracking. Sales per Employee Total Revenue \u00f7 Number of Employees Sales GL, HR Headcount (non-GL) Productivity measure."},{"location":"finance_terms/finance_kpi/#36-cash-flow-kpis","title":"3.6 Cash Flow KPIs","text":"KPI Formula GL Accounts Used Purpose Operating Cash Flow (OCF) Cash generated from Operations GL: Operating Cash Flow (Cash Flow Statement) Core business cash generation. Free Cash Flow (FCF) Operating Cash Flow \u2212 Capital Expenditures Operating Cash Flow, Capital Expenditures Cash available after investments. Cash Flow Margin (Operating Cash Flow \u00f7 Revenue) \u00d7 100 Operating Cash Flow, Sales GL Proportion of revenue turning into cash."},{"location":"finance_terms/finance_kpi/#4-why-financial-kpis-matter","title":"4. Why Financial KPIs Matter","text":""},{"location":"finance_terms/finance_kpi/#41-performance-monitoring","title":"4.1 Performance Monitoring","text":"<p>Financial KPIs provide a clear picture of how a business is performing financially over time. It tracks trends in revenue, profitability and liquidity.</p>"},{"location":"finance_terms/finance_kpi/#42-strategic-alignment","title":"4.2 Strategic Alignment","text":"<p>They help ensure that financial strategies are aligned with overall business goals. </p>"},{"location":"finance_terms/finance_kpi/#43-early-warning-system","title":"4.3 Early Warning System","text":"<p>It detects inefficiencies or financial risks early. Businesses can identify potential problems early on and take corrective actions.  </p>"},{"location":"finance_terms/finance_kpi/#44-informed-decision-making","title":"4.4 Informed Decision Making","text":"<p>Financial KPIs provide data-driven insights that support better decision-making in areas like investment, pricing, and resource allocation.  </p>"},{"location":"finance_terms/finance_kpi/#45-investor-relations","title":"4.5 Investor Relations","text":"<p>These provide transparency and confidence to stakeholder. They are essential for communicating a company\u2019s financial health to investors and stakeholders.  </p>"},{"location":"general_terminology/big_data/","title":"Big Data","text":"<p>To understand some concepts behind Big Data, there are some terminologies that should be properly defined. Below are some of the most common terms that should come in handy.</p>"},{"location":"general_terminology/big_data/#big-data_1","title":"Big Data","text":"<ul> <li> <p>A large collection of data characterized by the three V\u2019s: volume, velocity, and variety.</p> <ul> <li> <p>Volume refers to the amount of data\u2014big data deals with high volumes of data</p> </li> <li> <p>Velocity refers to the rate at which data is collected\u2014big data is collected at a high velocity and often streams directly into memory</p> </li> <li> <p>Variety refers to the range of data formats\u2014big data tends to have a high variety of structured, semi-structured, and unstructured data, as well as a variety of formats such as numbers, text strings, images, and audio.</p> </li> </ul> </li> </ul>"},{"location":"general_terminology/big_data/#as-a-service-infrastructure","title":"As-a-service Infrastructure","text":"<ul> <li> <p>Some examples: Data-as-a-service, software-as-a-service, platform-as-a-service</p> </li> <li> <p>All of them refer to the idea that rather than selling data, licences to use data, or platforms for running Big Data technology, it can be provided \"as a service\", rather than as a product.</p> </li> </ul>"},{"location":"general_terminology/big_data/#data-lake","title":"Data Lake","text":"<ul> <li> <p>A storage repository that holds a vast amount of raw data in its native format until it's required.</p> </li> <li> <p>Every data element within a data lake is assigned a unique identifier and set of extended metadata tags. When a business question arises, users can access the data lake to retrieve any relevant, supporting data. </p> </li> </ul>"},{"location":"general_terminology/big_data/#data-mining","title":"Data Mining","text":"<ul> <li> <p>Process of discovering insights from data.</p> </li> <li> <p>In terms of Big Data, because it is so large, this is generally done by computational methods in an automated way using methods such as decision trees, clustering analysis and, most recently, machine learning.</p> </li> </ul>"},{"location":"general_terminology/big_data/#apache","title":"Apache","text":"<ul> <li>The name of a prominent open-source web server and also a non-profit organization that supports many open-source projects, including the web server.</li> </ul>"},{"location":"general_terminology/big_data/#apache-hadoop","title":"Apache Hadoop","text":"<ul> <li> <p>A Java-based framework for Big Data computing which has been released into the public domain as a open source software, and so can freely be used by anyone.</p> </li> <li> <p>It is mainly used for distributed storage and processing of large datasets and analytics jobs, breaking workloads down into smaller workloads that can be run at the same.</p> </li> </ul> <p>For more information, refer to: Google Cloud - What is Apache Hadoop?.</p>"},{"location":"general_terminology/big_data/#apache-spark","title":"Apache Spark","text":"<ul> <li> <p>A unified analytics engine for large-scale data processing, offering built-in modules for SQL, streaming, machine learning, and graph processing, and can run on various platforms and against diverse data sources.</p> </li> <li> <p>More recently developed and more suited to handling cutting-edge Big Data tasks involving real time analytics and machine learning.</p> </li> <li> <p>Does not include its own file system, but is designed to work with Hadoop's HDFS or a number of other options. </p> </li> <li> <p>However, for certain data related processes it is able to calculate at over 100 times the speed of Hadoop, thanks to its in-memory processing capability.</p> </li> </ul> <p>For more information, refer to Amazon Web Services - Whta is Apache Spark?.</p>"},{"location":"general_terminology/big_data/#apache-hadoop-vs-apache-spark","title":"Apache Hadoop vs. Apache Spark","text":"Feature Apache Spark Apache Hadoop (MapReduce) Processing Model In-memory processing Disk-based batch processing Speed Faster due to in-memory computation Slower due to frequent disk I/O Ease of Use Easier with APIs in Python, Scala, Java More complex, primarily Java-based Data Processing Type Supports batch, streaming, and ML workloads Primarily batch processing Fault Tolerance RDD lineage and DAG recovery Data replication via HDFS Resource Management Integrates with YARN, Mesos, or Kubernetes Uses YARN for resource management Machine Learning Support Built-in MLlib library No native ML support, relies on tools like Mahout Streaming Support Yes, via Spark Streaming or Structured Streaming Not inherently designed for streaming Data Caching Yes, supports in-memory data caching No, data read/written from disk each time Use Cases Real-time analytics, ML pipelines, ETL Long-running batch jobs, ETL <p>For more information, refer to Spark vs. Hadoop.</p>"},{"location":"general_terminology/big_data/#mapreduce","title":"MapReduce","text":"<ul> <li> <p>A computing procedure for working with large datasets, which was devised due to difficulty of reading and analysing really Big Data using conventional computing methodologies.</p> </li> <li> <p>As its name suggests, it consists of two procedures \u2013 mapping (sorting information into the format needed for analysis \u2013 i.e. sorting a list of people according to their age) and reducing (performing an operation, such checking the age of everyone in the dataset to see who is over 21).</p> </li> </ul>"},{"location":"general_terminology/big_data/#nosql","title":"NoSQL","text":"<ul> <li> <p>A database format designed to hold more than data which is simply arranged into tables, rows, and columns, as is the case in a conventional relational database.</p> </li> <li> <p>This database format has proven very popular in Big Data applications because Big Data is often messy, unstructured and does not easily fit into traditional database frameworks.</p> </li> </ul> <p> Programming Languages</p> <p>Data Engineering </p>"},{"location":"general_terminology/calendar_types/","title":"Calendar Systems and Accounting Periods","text":"<p>Different teams, regions, and systems use different calendar definitions for budgeting, forecasting, and reporting.  </p> <p>Misalignment between calendar definitions is a common cause of: - Budget vs actual mismatches - Incorrect variance analysis - Confusion around year-to-date (YTD) reporting</p>"},{"location":"general_terminology/calendar_types/#type-of-calender-terminology","title":"Type of Calender Terminology","text":""},{"location":"general_terminology/calendar_types/#calendar-year","title":"Calendar Year","text":"<ul> <li>Runs from 1 January to 31 December</li> <li>Aligns exactly with the Gregorian calendar</li> <li>Commonly used for:</li> <li>External reporting</li> <li>Public datasets</li> <li>Many US-based companies</li> </ul>"},{"location":"general_terminology/calendar_types/#fiscal-year-fy","title":"Fiscal Year (FY)","text":"<ul> <li>A company-defined 12-month reporting period</li> <li>Does not necessarily align with the calendar year</li> <li>Typically named by its ending year</li> <li>Example: FY25 ends in 2025</li> </ul> <p>Fiscal years are used for: - Budgeting - Forecasting - Internal performance evaluation</p>"},{"location":"general_terminology/calendar_types/#accounting-period","title":"Accounting Period","text":"<ul> <li>A subdivision of a fiscal year</li> <li>Often monthly, but system-dependent</li> <li>Used for:</li> <li>Financial closing</li> <li>Budget tracking</li> <li>Variance analysis</li> </ul>"},{"location":"general_terminology/calendar_types/#1-standard-calendar-calendar","title":"1. Standard (Calendar) Calendar","text":""},{"location":"general_terminology/calendar_types/#definition","title":"Definition","text":"<p>The standard calendar follows the Gregorian calendar: - January (1) through December (12) - Year boundaries align with the calendar year</p>"},{"location":"general_terminology/calendar_types/#typical-usage","title":"Typical Usage","text":"<ul> <li>Most US companies</li> <li>External-facing financial communication</li> <li>Simplified time-based analysis</li> </ul>"},{"location":"general_terminology/calendar_types/#advantages","title":"Advantages","text":"<ul> <li>Intuitive and widely understood</li> <li>Aligns with public benchmarks and datasets</li> <li>No year-crossing months</li> </ul>"},{"location":"general_terminology/calendar_types/#limitations","title":"Limitations","text":"<ul> <li>May not reflect business seasonality</li> <li>Less suitable for companies with non-calendar operating cycles</li> </ul>"},{"location":"general_terminology/calendar_types/#2-fiscal-calendars-non-standard","title":"2. Fiscal Calendars (Non-Standard)","text":"<p>Fiscal calendars shift the start of the year to better align with operational or regional business cycles.</p>"},{"location":"general_terminology/calendar_types/#japan-fiscal-calendar-april-to-march","title":"Japan Fiscal Calendar (April to March)","text":"<p>Structure: - Fiscal Year starts in April - Ends in March - Example:   - FY25 = April 2024 to March 2025</p> <p>Common Usage: - Japanese companies - Subsidiaries reporting into Japan headquarters</p> <p>Implications: - Q1 = April to June - Fiscal year crosses calendar year boundaries - Calendar-based data must be remapped for fiscal reporting</p>"},{"location":"general_terminology/calendar_types/#singapore-regional-fiscal-calendar-july-to-june","title":"Singapore / Regional Fiscal Calendar (July to June)","text":"<p>Structure: - Fiscal Year starts in July - Ends in June - Example:   - FY25 = July 2024 to June 2025</p> <p>Common Usage: - Some Singapore-based or APAC regional organisations - Government-linked or education-related entities</p> <p>Implications: - Budget cycles reset mid-calendar year - Year-over-year comparisons require fiscal alignment</p>"},{"location":"general_terminology/calendar_types/#3-sap-accounting-periods","title":"3. SAP Accounting Periods","text":""},{"location":"general_terminology/calendar_types/#what-sap-periods-represent","title":"What SAP Periods Represent","text":"<p>SAP uses posting periods rather than relying purely on calendar months.</p> <p>A fiscal year in SAP typically consists of: - 12 normal operating periods - Additional adjustment periods</p>"},{"location":"general_terminology/calendar_types/#sap-period-structure","title":"SAP Period Structure","text":"<ul> <li>Periods 1\u201312 </li> <li>Correspond to standard operating months within the fiscal year</li> <li>Periods 13\u201316 </li> <li>Special adjustment periods</li> <li>Used for year-end and post-closing entries</li> </ul> <p>These additional periods do not represent new calendar months.</p>"},{"location":"general_terminology/calendar_types/#purpose-of-sap-adjustment-periods-1316","title":"Purpose of SAP Adjustment Periods (13\u201316)","text":"<p>Adjustment periods exist to: - Allow audit and accrual postings - Prevent reopening closed operational periods - Maintain clean month-end and year-end closes</p> <p>Example: - Period 12 is closed - Adjustments are posted to Period 13\u201316 instead</p>"},{"location":"general_terminology/calendar_types/#key-distinction","title":"Key Distinction","text":"<p>SAP Period \u2260 Calendar Month</p> <p>SAP periods are accounting constructs and should not be assumed to map one-to-one with calendar months.</p>"},{"location":"general_terminology/calendar_types/#example-fiscal-year-and-sap-period-mapping","title":"Example: Fiscal Year and SAP Period Mapping","text":"<p>Based on the current budgeting setup:</p> <ul> <li>Fiscal Year FY25 starts in April 2025</li> <li>Period mapping:</li> <li>Period 1 \u2192 April 2025</li> <li>Period 2 \u2192 May 2025</li> <li>\u2026</li> <li>Period 9 \u2192 December 2025</li> <li>Period 10 \u2192 January 2026</li> <li>Period 11 \u2192 February 2026</li> <li>Period 12 \u2192 March 2026</li> <li>Period 16 \u2192 Year-end adjustment (March 2026)</li> </ul>"},{"location":"general_terminology/calendar_types/#key-observation","title":"Key Observation","text":"<ul> <li>FY25 spans two calendar years</li> <li>January to March 2026 still belong to FY25</li> <li>This is expected behaviour under a fiscal calendar</li> </ul>"},{"location":"general_terminology/calendar_types/#why-calendar-definitions-matter-for-budgeting","title":"Why Calendar Definitions Matter for Budgeting","text":""},{"location":"general_terminology/calendar_types/#budget-vs-actual-alignment","title":"Budget vs Actual Alignment","text":"<ul> <li>Budgets are set by fiscal year and fiscal period</li> <li>Actuals may be recorded by calendar date</li> <li>Misalignment leads to incorrect comparisons</li> </ul>"},{"location":"general_terminology/calendar_types/#forecasting-implications","title":"Forecasting Implications","text":"<ul> <li>Rolling forecasts must respect fiscal boundaries</li> <li>Period-based forecasts cannot assume calendar alignment</li> </ul>"},{"location":"general_terminology/calendar_types/#variance-analysis","title":"Variance Analysis","text":"<ul> <li>Comparing calendar-month actuals to fiscal-period budgets can result in misleading variance conclusions</li> </ul>"},{"location":"general_terminology/calendar_types/#best-practices-when-working-with-multiple-calendars","title":"Best Practices When Working with Multiple Calendars","text":""},{"location":"general_terminology/calendar_types/#always-state-the-calendar-definition","title":"Always State the Calendar Definition","text":"<p>Every report or dataset should clearly specify: - Calendar type - Fiscal year start month - Period definition (SAP vs calendar)</p>"},{"location":"general_terminology/calendar_types/#avoid-implicit-calendar-mixing","title":"Avoid Implicit Calendar Mixing","text":"<p>Do not: - Assume Period 1 is January - Aggregate SAP periods as if they are calendar months - Mix regional fiscal calendars without reconciliation</p>"},{"location":"general_terminology/calendar_types/#maintain-a-calendar-mapping-table","title":"Maintain a Calendar Mapping Table","text":"<p>A central calendar reference should include: - Calendar date - Calendar month and year - Fiscal year - Fiscal period - SAP posting period</p>"},{"location":"general_terminology/calendar_types/#forecasting-periods-and-budgeting-cycle-expectations","title":"Forecasting Periods and Budgeting Cycle Expectations","text":"<p>Budgeting and forecasting follow a rolling period-based approach, where each period transitions from Forecast to Actual as financial results are closed.</p> <p>The expectations for each period differ depending on whether the submission is: - For Review, or - Final Submission</p> <p>The figure below illustrates how Actuals and Forecasts should be populated across the fiscal year for each budgeting cycle.</p> Submission Requirement"},{"location":"general_terminology/calendar_types/#key-definitions","title":"Key Definitions","text":""},{"location":"general_terminology/calendar_types/#actual","title":"Actual","text":"<ul> <li>Financial results that have been:</li> <li>Posted</li> <li>Reviewed</li> <li>Closed for the period</li> <li>Actuals should not change after period close</li> </ul>"},{"location":"general_terminology/calendar_types/#forecast","title":"Forecast","text":"<ul> <li>Forward-looking estimates based on:</li> <li>Latest business assumptions</li> <li>Known risks and opportunities</li> <li>Forecast values are updated each budgeting cycle until the period closes</li> </ul>"},{"location":"general_terminology/calendar_types/#rolling-forecast","title":"Rolling Forecast","text":"<ul> <li>As each month closes:</li> <li>That period becomes Actual</li> <li>Remaining future periods stay Forecast</li> <li>The forecast horizon shifts forward month by month</li> </ul>"},{"location":"general_terminology/calendar_types/#for-review-submissions","title":"For Review Submissions","text":""},{"location":"general_terminology/calendar_types/#expectations","title":"Expectations","text":"<ul> <li>Only periods that are fully closed should be marked as Actual</li> <li>All open and future periods remain Forecast</li> <li>The cutoff reflects the latest available actuals at the time of review</li> </ul>"},{"location":"general_terminology/calendar_types/#final-submissions","title":"Final Submissions","text":"<p>Final submissions represent the official budgeting or forecasting position.</p>"},{"location":"general_terminology/calendar_types/#expectations_1","title":"Expectations","text":"<ul> <li>All periods up to and including the current reporting month must be Actual</li> <li>Only periods after the cutoff month remain Forecast</li> <li>No forecast values should remain in closed periods</li> </ul>"},{"location":"general_terminology/data_analytics/","title":"Data Analytics","text":""},{"location":"general_terminology/data_analytics/#data-analysis","title":"Data Analysis","text":"<ul> <li> <p>The process of working with data to derive useful information, which can then be used to make data-informed decisions.</p> </li> <li> <p>Generally consists of a six-step process:</p> <ul> <li> <p>Ask a question</p> </li> <li> <p>Prepare your raw data</p> </li> <li> <p>Process your data for analysis</p> </li> <li> <p>Analyze your data</p> </li> <li> <p>Share your results</p> </li> <li> <p>Act in accordance with your data</p> </li> </ul> </li> </ul>"},{"location":"general_terminology/data_analytics/#four-types-of-data-analytics","title":"Four types of data analytics","text":"<ul> <li> <p>Decriptive analytics: what happened?</p> </li> <li> <p>Diagnostic analytics: why did it happen?</p> </li> <li> <p>Predictive analytics: what is going to happen next?</p> </li> <li> <p>Prescriptive analytics: what should we be doing?</p> </li> </ul>"},{"location":"general_terminology/data_analytics/#data-analysts","title":"Data Analysts","text":"<ul> <li> <p>Data professionals who gather, clean, study or interpret data to solve business problems.</p> </li> <li> <p>They tend to work alongside other data analytics professionals, such as data scientists and engineers.</p> </li> <li> <p>Tasks:</p> <ul> <li> <p>Creating reports and dahsboards</p> </li> <li> <p>Analyzing patterns and trends</p> </li> </ul> </li> </ul>"},{"location":"general_terminology/data_analytics/#attribute","title":"Attribute","text":"<ul> <li>When working in a spreadsheet or database, an attribute is a common descriptor used to label a column.</li> </ul> <p>To see the standard naming conventions in the Finance dataset, refer to Naming Convention.</p>"},{"location":"general_terminology/data_analytics/#data-architecture","title":"Data Architecture","text":"<ul> <li> <p>The plan for an organisation's data management system.</p> </li> <li> <p>This can include all touchpoints in the data lifecycle, including how the data is gathered, organized, utilized, and discarded. </p> </li> <li> <p>Data architects design the blueprints thst prganizations use for their data management systems.</p> </li> </ul>"},{"location":"general_terminology/data_analytics/#business-intelligence-bi","title":"Business Intelligence (BI)","text":"<ul> <li> <p>Data analytics used to empower organizations to make data-driven business decisions.</p> </li> <li> <p>BI analysts analyze business data like revenue, sales, or customer data, and offer recommendations based on their analysis.</p> </li> </ul>"},{"location":"general_terminology/data_analytics/#dashboard","title":"Dashboard","text":"Dashboard in Power BI (Microsoft, 2024) <ul> <li> <p>Tool used to monitor and display live data. </p> </li> <li> <p>Dashboards are typically connected to databases and feature visualizations that automatically update to reflect the most current data in the database.</p> </li> </ul>"},{"location":"general_terminology/data_analytics/#data-mining","title":"Data Mining","text":"<ul> <li> <p>Closely examining data to identify patterns and glean insights.</p> </li> <li> <p>A central aspect of data analytics; the insights you find during the mining process will inform your business recommendations.</p> </li> </ul>"},{"location":"general_terminology/data_analytics/#data-source","title":"Data Source","text":"<ul> <li>refers to the origin of a specific set of information. As businesses increasingly generate data year over year, data analysts rely on different data sources to measure business success and offer strategic recommendations.</li> </ul>"},{"location":"general_terminology/data_analytics/#application-programming-interface-api","title":"Application Programming Interface (API)","text":"<ul> <li> <p>It is a way for two pieces of code (whole applications or components) to interact. The API will tell a user what requests they can make of teh code, how to make them and what they can expect in return.</p> </li> <li> <p>Specifically for data, an API may specify how to retrieve data from an application.</p> </li> </ul>"},{"location":"general_terminology/data_analytics/#data-visualization","title":"Data Visualization","text":"<ul> <li> <p>The representation of information and data using charts, graphs, maps, and other visual tools. </p> </li> <li> <p>With strong data visualizations, you can foster storytelling, make your data accessible to a wider audience, identify patterns and relationships, and explore your data further.</p> </li> </ul> <p>Refer to Data Visualization for a list of common data visualizations used.</p>"},{"location":"general_terminology/data_analytics/#data-wrangling","title":"Data Wrangling","text":"<ul> <li> <p>The process of converting raw data into a usable form.</p> </li> <li> <p>There are four stages to the wrangling procesS: discovery, data transformation, data validation, and publishing.</p> <ul> <li>The data transformation stage can be broken down further into tasks like dat astructuring, data normalization or denormalization, data cleaning, and data enrichment.</li> </ul> </li> </ul>"},{"location":"general_terminology/data_analytics/#database","title":"Database","text":"<ul> <li> <p>An organised collection of information that can be searched, sorted, and updated.</p> <ul> <li> <p>This data is often stored electronically in a computer system called a datavase management system (DBMS).</p> </li> <li> <p>Oftentimes you'll need to use a programming language, such as SQL, to interact with your database.</p> </li> </ul> </li> </ul>"},{"location":"general_terminology/data_analytics/#metadata","title":"Metadata","text":"<ul> <li> <p>Data about data.</p> </li> <li> <p>It describes various characteristics of your data, such as how it was collected, where it\u2019s stored, its file type, or creation date. Metadata can be particularly useful for verification and tracking purposes.</p> </li> </ul>"},{"location":"general_terminology/data_analytics/#qualitative-data","title":"Qualitative Data","text":"<ul> <li> <p>Data that describes qualities or characteristics.</p> </li> <li> <p>It\u2019s generally non-numeric data and can be subjective, for example eye color or emotions.</p> </li> </ul>"},{"location":"general_terminology/data_analytics/#quantitative-data","title":"Quantitative Data","text":"<ul> <li> <p>Objective data with a specific numeric value.</p> </li> <li> <p>It\u2019s generally something you can count or measure, such as height or speed.</p> </li> </ul>"},{"location":"general_terminology/data_analytics/#relational-database","title":"Relational Database","text":"<ul> <li> <p>A database that contains several tables with related information. Even though data is stored in separate tables, you can access related data across several tables witha  single query.</p> </li> <li> <p>For example, a relational database may have one table for inventory and another table for customer orders/ When you look up a specific product in your relational database, you can retrieve both inventory and customer order information at the same time.</p> </li> </ul>"},{"location":"general_terminology/data_analytics/#structured-data","title":"Structured Data","text":"<ul> <li> <p>Formatted data, for example data that is organized into rows and columns.</p> </li> <li> <p>Structured data is more readily analyzed than unstructured data because of its tidy formatting.</p> </li> </ul>"},{"location":"general_terminology/data_analytics/#unstructured-data","title":"Unstructured Data","text":"<ul> <li> <p>Unstructured data is data that is organzed in any apparent way.</p> </li> <li> <p>In order to analyze unstructured data, you'll typically need to implement some type of organization.</p> </li> </ul> <p>For a comprehensive definition list for data and analytics, refer to Data and Analytics Dictionary.</p> <p> Data Engineering</p> <p>Data Visualization </p>"},{"location":"general_terminology/data_engineering/","title":"Data Engineering","text":""},{"location":"general_terminology/data_engineering/#data-engineering","title":"Data Engineering","text":"<ul> <li> <p>The process of making data accessible for analysis.</p> </li> <li> <p>Data engineers build systems that collect, manager, and convert raw data into usable information.</p> </li> <li> <p>Some common tasks include developing algorithms to transform data into a more useful form, building database pipeline architectures, and creating new data analysis tools.</p> <ul> <li>In our case, this will mainly be done on Microsoft Fabric.</li> </ul> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-engineer","title":"Data Engineer","text":"<ul> <li> <p>Designs, builds, and maintains the infrastructure and systems needed to collect, store, and process data for various business applications.</p> </li> <li> <p>Skills: Python, ETL tools, big data, cloud platforms</p> </li> <li> <p>Task:</p> <ul> <li> <p>Cleaning and transform raw data</p> </li> <li> <p>Setting up data warehouses/lakes</p> </li> </ul> </li> </ul>"},{"location":"general_terminology/data_engineering/#extract-transform-load-etl","title":"Extract, Transform, Load (ETL)","text":"ETL Process (Rivery, 2024) <ul> <li> <p>A process that involves extracting data from source systems, transforming it into a suitable format, and loading it into a target data store, transforming it into a suitable format, and loading it into a target data store.</p> </li> <li> <p>Older method ideal for complex transformations of smaller data sets.</p> <ul> <li>Great for those prioritizing data security.</li> </ul> </li> </ul>"},{"location":"general_terminology/data_engineering/#extract-load-transform-elt","title":"Extract, Load, Transform (ELT)","text":"ELT Process (Rivery, 2024) <ul> <li> <p>Performs data transformations directly within the data warehouse itself.</p> </li> <li> <p>ELT allows for raw data to be sent directly to the data warehouse, eliminating the need for staging processes.</p> </li> <li> <p>A newer technology that provides more flexibility to analysts and is perfect for processing both structured and unstructured data.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#etl-vs-elt","title":"ETL vs. ELT","text":"Category ETL (Extract, Transform, Load) ELT (Extract, Load, Transform) Definition Data is extracted from a source system, transformed on a secondary processing server, and loaded into a destination system. Data is extracted from a source system, loaded into a destination system, and transformed inside the destination system. Extract Raw data is extracted using API connectors. Raw data is extracted using API connectors. Transform Raw data is transformed on a processing server. Raw data is transformed inside the target system. Load Transformed data is loaded into a destination system. Raw data is loaded directly into the target system. Speed ETL is a time-intensive process; data is transformed before loading into a destination system. ELT is faster by comparison; data is loaded directly into a destination system and transformed in-parallel. Code-Based Transformations Performed on secondary server. Best for compute-intensive transformations and pre-cleansing. Performed in-database; supports simultaneous load and transform; improves speed and efficiency. Maturity Mature technology with 20+ years of development; practices and protocols are well-documented. Relatively newer approach with evolving best practices and tooling. Privacy Pre-load transformation allows PII removal before loading; useful for compliance (e.g., HIPAA). Direct loading requires strong in-database privacy safeguards and governance. Maintenance Requires maintenance of separate processing infrastructure. Lower maintenance burden due to fewer systems and unified architecture. Costs Can incur higher costs due to separate processing servers and staging environments. More cost-efficient due to simplified architecture and modern cloud services. Requeries Raw data is not stored in the destination system, limiting the ability to requery unprocessed data. Raw data is stored directly in the target system, allowing flexible and repeated querying. Data Lake Compatibility Not compatible with data lake architectures. Compatible with data lakes and modern data platforms. Data Output Typically structured data. Supports structured, semi-structured, and unstructured data. Data Volume Ideal for smaller datasets with complex transformation requirements. Designed for large-scale data volumes with high throughput needs. Use Cases Legacy systems, on-premise data platforms, heavy pre-processing needs. Cloud-native data platforms, real-time analytics, big data and data lake environments. Tool Examples Informatica, Talend, Apache Nifi, Microsoft SSIS. dbt, Azure Data Factory (pushdown), Google BigQuery SQL, Snowflake with native SQL."},{"location":"general_terminology/data_engineering/#similarities","title":"Similarities","text":"<ul> <li> <p>Both involve Extracting, Transforming, and Loading data.</p> </li> <li> <p>Both aim to make data available for analytics and reporting.</p> </li> <li> <p>Both can be scheduled or orchestrated using data pipelines.</p> </li> <li> <p>Both support data integration across multiple sources.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#use-cases","title":"Use Cases","text":"Scenario Use ETL Use ELT Legacy systems and on-premise databases Yes No Cloud-native data warehousing No Yes Real-time or near-real-time analytics No (higher latency) Yes (faster ingestion and transformation) Heavy pre-processing required Yes No Data lake architecture with large datasets No Yes"},{"location":"general_terminology/data_engineering/#data-governance","title":"Data Governance","text":"<ul> <li> <p>The formal plan for the way an organization manages company data.</p> </li> <li> <p>Data governance emcompassess rules for the way data is accessed and used, and can include acountability and compliance rules.</p> <ul> <li>Defining and implementing policies, standards, and practices for managing and ensuring the quality, integrity, and security of data within an organization.</li> </ul> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-integrity","title":"Data Integrity","text":"<ul> <li> <p>Emcompasses the accuracy, reliability, and consistency of data over time.</p> </li> <li> <p>It involves maintaining the quality and reliability of data by implementing safeguards against unauthorized modifications, errors, or data loss. </p> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-lake","title":"Data Lake","text":"<ul> <li> <p>A centralized repository designed to capture and store a large amount of structured, semi-structured, and unstructured raw data.</p> </li> <li> <p>Unlike a data warehouse, a data lake does not impose a structure on the data before storage, providing flexibility for diverse analytics.</p> </li> <li> <p>Data scientists use the data in data lakes for machine learning or AI algorithms and models, or they can process the data and transfer it to a data warehouse.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-modeling","title":"Data Modeling","text":"<ul> <li> <p>The process of mapping and building data pipelines that connect data sources for analysis.</p> <ul> <li> <p>Define structure of data and its relationships in a database or system.</p> </li> <li> <p>It helps in understanding and designing how data will be stored and accessed.</p> </li> </ul> </li> <li> <p>A data model is a tool that implements those pipelines and organizes data across data sources.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-pipeline","title":"Data Pipeline","text":"<ul> <li> <p>A series of processes hat move data from one system to another, typically involving multiple stages such as extraction, transformation, and loading.</p> <ul> <li>These pipelines can be designed to operate in real-time (stream processing) or in scheduled intervals (batch processing).</li> </ul> </li> <li> <p>It ensures a smooth flow of data from source to destination.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-warehouse","title":"Data Warehouse","text":"<ul> <li> <p>A centralized data repository that stores processed, organized data from multiple sources.</p> </li> <li> <p>Data warehouses may contain a combination of current and historical data that has been extracted, transformed, and loaded from internal and external databases.</p> </li> <li> <p>Characteristics:</p> <ul> <li> <p>Usually contains large amounts of historical data.</p> </li> <li> <p>considerable time and effort to create and maintain the warehouse.</p> </li> <li> <p>(Optional) Can create Data Marts for better performance during exploitation by end users.</p> </li> </ul> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-lakehouse","title":"Data Lakehouse","text":"<ul> <li> <p>A data lakeouse is an architectural approach that combines the flexibility of a data lake with the reliability and performance of a data warehouse.</p> <ul> <li>Provides a unified platform for both analytical and transactional workloads.</li> </ul> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-mart","title":"Data Mart","text":"<ul> <li> <p>A subset of a data warehouse that houses all processed data relevant to a specific department.</p> </li> <li> <p>While a data warehouse may contain data pertaining to the finance, marketing, sales, and human resources teams, a data mart may isolate the finance team data.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#schema","title":"Schema","text":"<ul> <li>Defines the structure of a database or data warehouse, including tables, columns, relationships, and constraints. It serves as a blueprint for organizing and representing data.</li> </ul>"},{"location":"general_terminology/data_engineering/#distributed-systems","title":"Distributed Systems","text":"<ul> <li> <p>Distributed systems involve the coordination and communication of multiple interconnected components across different machines.</p> </li> <li> <p>In the contet of data engineering, this is essential for scalability and fault tolerance.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-scalability","title":"Data Scalability","text":"<ul> <li> <p>The ability of a system to handle growing amounts of data or increase workload.</p> </li> <li> <p>Data engineers design systems that can scale horizontally or vertically to meet performance requirements as data volumes increase.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-integration","title":"Data Integration","text":"<ul> <li> <p>Involves combining data from different sources to provide a unified view.</p> </li> <li> <p>It ensures that diverse datasets can work together seamlessly, often through ETL processes, to support analytics and reporting.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-migration","title":"Data Migration","text":"<ul> <li> <p>Data migration is the process of transferring data from one system to another.</p> </li> <li> <p>Data engineers need to plan and execute migrations carfully to ensure data integrity and minimize downtime.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-orchestration","title":"Data Orchestration","text":"<ul> <li>Involves coordinating and managing the flow of data across various systems, services, and processes. It ensures that data workflows are executed in a controlled and organized manner.</li> </ul>"},{"location":"general_terminology/data_engineering/#data-mesh","title":"Data Mesh","text":"<ul> <li> <p>Data Mesh is a decentralized approach to data architecture that emphasizes domain-oriented dencentralized data ownership and infrastructure as code. </p> </li> <li> <p>It aims to address scalability and agility in data systems.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-ingestion","title":"Data Ingestion","text":"<ul> <li> <p>Data ingestion is the process of collecting and importing data into a data system or storage layer.</p> </li> <li> <p>It involves azquiring data from various sources, such as databases, logs, or external APIs, for further processing.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-bias","title":"Data Bias","text":"<ul> <li> <p>Data bias occurs when datasets used for analysis or machine learning models contain systematic errors or favor specific groups, leading to biased results.</p> </li> <li> <p>Data engineers and data scientists must be aware of and address bias in data.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-redundancy","title":"Data Redundancy","text":"<ul> <li> <p>Data redundancy occurs when the same piece of data is unnecessarily duplicated and stored in multiple places within a database.</p> </li> <li> <p>While redundancy can be intentional for performance or data retrieval purposes, excessive redundancy can lead to inefficiencies and data integrity issues.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#normalization","title":"Normalization","text":"<ul> <li> <p>Normalization is a database design technique that aims to minimize data redundancy and dependency by organizing data into separate tables.</p> </li> <li> <p>It involves breaking down a large table into smaller, related tables and establishing relationships between them.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#data-workflow","title":"Data Workflow","text":"<ul> <li> <p>A data workflow refers to the series of steps, processes, and tasks involved in the end-to-end management and movement of data within an organization.</p> </li> <li> <p>This includes the collection, processing, storage, analysis, and distribution of data throughout its lifecycle.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#online-transaction-processing-oltp","title":"Online Transaction Processing (OLTP)","text":"<ul> <li> <p>OLTP systems manage transaction-oriented applications, typically involving large numbers of short online transactions such as insert, updatem and delete operations.</p> </li> <li> <p>These systems are optimized for fast query processing and maintaining data integrity in environments where multiple users perform transactions simultaneously.</p> </li> <li> <p>OLTP is commonly used in applications like banking, order processing, and retail, where real-time data access and reliability are critical.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#online-analytical-processing-olap","title":"Online Analytical Processing (OLAP)","text":"<ul> <li> <p>Software technology you can use to analyze business data from different points of view.</p> </li> <li> <p>OLAP combines and groups the data into categories to provide actionable insights for strategic planning.</p> </li> </ul>"},{"location":"general_terminology/data_engineering/#oltp-vs-olap","title":"OLTP vs. OLAP","text":"Feature OLTP (Online Transaction Processing) OLAP (Online Analytical Processing) Purpose Handles day-to-day transactional data Supports complex analysis and decision-making Data Operations Insert, Update, Delete, Short Queries Read-heavy, Aggregate, Analytical Queries Data Volume Handles large number of short online transactions Handles large volumes of historical data Users Operational users (e.g., cashiers, clerks) Business analysts, decision makers Data Structure Highly normalized schema (e.g., 3NF) Denormalized schema (e.g., star, snowflake) Query Complexity Simple queries with fast response times Complex queries involving joins, aggregations, and summarizations Response Time Milliseconds to seconds Seconds to minutes Data Freshness Real-time or near real-time Periodically updated (e.g., hourly, daily) Examples Banking transactions, order entry, retail sales Sales trend analysis, forecasting, market segmentation Database Design Goal Speed and efficiency of transactions Flexibility and speed of analytical queries System Type Row-based databases Columnar or hybrid storage databases Backup and Recovery Essential for data consistency and reliability May be less frequent, often relies on data snapshots <p> Big Data</p> <p>Data Analytics </p>"},{"location":"general_terminology/data_science/","title":"Data Science","text":""},{"location":"general_terminology/data_science/#data-science_1","title":"Data Science","text":"<ul> <li> <p>The scientific study of data.</p> </li> <li> <p>Data science combines math and statistics, specialized programming, advanced analytics, artificial intelligence (AI) and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organization\u2019s data. These insights can be used to guide decision making and strategic planning.</p> </li> </ul>"},{"location":"general_terminology/data_science/#data-scientist","title":"Data Scientist","text":"<ul> <li> <p>Ask questions and find ways to answer those questions with data.</p> </li> <li> <p>They may work on capturning data, transforming raw data into a usable form, analyzing data, and creating predictive models.</p> </li> <li> <p>Skills: Python/R, machine learning, statistics, data visualization</p> </li> <li> <p>Tasks:</p> <ul> <li> <p>Creating ML models</p> </li> <li> <p>Predictive and prescriptive data</p> </li> </ul> </li> </ul>"},{"location":"general_terminology/data_science/#algorithm","title":"Algorithm","text":"<ul> <li> <p>A set of instructions or rules to follow in order to complete a specific task.</p> </li> <li> <p>Data analysts may use algorithms to organize or analyze data, while data scientists may use algorithms to make predictions or build models.</p> </li> </ul>"},{"location":"general_terminology/data_science/#classification","title":"Classification","text":"<ul> <li> <p>A machine learning problem that organizes data into categories.</p> <ul> <li>You may use this to create email spam filters, for example.</li> </ul> </li> <li> <p>Some examples of algorithms commonly used to create classification models are logistic regression, decision trees, K-nearest neighbor (KNN), and random forest.</p> </li> </ul>"},{"location":"general_terminology/data_science/#deep-learning","title":"Deep Learning","text":"<ul> <li> <p>A machine learning technique that layers algorithms and computing units\u2014or neurons\u2014into what is called an artificial neural network (ANN).</p> </li> <li> <p>Unlike machine learning, deep learning algorithms can improve incorrect outcomes through repetition without human intervention. These deep neural networks take inspiration from the structure of the human brain.</p> </li> </ul>"},{"location":"general_terminology/data_science/#machine-learning","title":"Machine Learning","text":"<ul> <li> <p>A subset of AI in which algorithms mimic human learning while processing data.</p> </li> <li> <p>With machine learning, algorithms can improve over time, becoming increasingly accurate when making predictions or classifications.</p> </li> </ul>"},{"location":"general_terminology/data_science/#regression","title":"Regression","text":"<ul> <li> <p>A machine learning problem that uses data to predict future outcomes.</p> </li> <li> <p>Some examples of algorithms commonly used to create regression models are linear regression and ridge regression.</p> </li> </ul>"},{"location":"general_terminology/data_science/#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li> <p>A type of machine learning that learns by interacting with its environment and receiving positive reinforcement for correct predictions and negative reinforcement for incorrect predictions.</p> </li> <li> <p>This type of machine learning may be used to develop autonomous vehicles. Common algorithms are temporal difference, deep adversarial networks, and Q-learning.</p> </li> </ul>"},{"location":"general_terminology/data_science/#supervised-learning","title":"Supervised Learning","text":"<ul> <li> <p>A type of machine learning that learns from labeled historical input and output data.</p> </li> <li> <p>This type of machine learning may be used to predict real estate prices or find disease risk factors.</p> </li> <li> <p>Common algorithms used during supervised learning are neural networks, decision trees, linear regression, and support vector machines.</p> </li> </ul>"},{"location":"general_terminology/data_science/#unsupervised-learning","title":"Unsupervised Learning","text":"<ul> <li> <p>A machine learning type that looks for data patterns.</p> </li> <li> <p>Unlike supervised learning, unsupervised learning doesn\u2019t learn from labeled data.</p> </li> <li> <p>This type of machine learning is often used to develop predictive models and to create clusters. </p> <ul> <li>For example, you can use unsupervised learning to group customers based on purchase behavior, and then make product recommendations based on the purchasing patterns of similar customers.</li> </ul> </li> <li> <p>Hidden Markov models, k-means, hierarchical clustering, and Gaussian mixture models are common algorithms used during unsupervised learning.</p> </li> </ul> <p>For more information and code for each data science and analytics tool, refer to Data Science Glossary on Kaggle.</p> <p> Data Visualization</p> <p>Fabric Terminology: Get Data </p>"},{"location":"general_terminology/data_viz/","title":"Data Visualization","text":""},{"location":"general_terminology/data_viz/#charts","title":"Charts","text":""},{"location":"general_terminology/data_viz/#bar-column-charts","title":"Bar &amp; Column Charts","text":"Bar and Column Chart Example (Anapedia, 2024) <ul> <li> <p>Bar charts: reserved for charts where the categories appear on the vertical axis.</p> </li> <li> <p>Column charts: those where categories appear on the horizontal axis.</p> <ul> <li> <p>In either case, the chart has a series of categories along one axis.</p> </li> <li> <p>Extending rightwards (or upwards) from each category is a rectangle whose width (height) is proportional to the value associated with this category.</p> </li> </ul> </li> <li> <p>Sometimes the bars are clustered to allow multiple series to be charted side-by-side.</p> </li> </ul>"},{"location":"general_terminology/data_viz/#bubble-charts","title":"Bubble Charts","text":"Bubble Chart Example (NetSuite, 2021) <ul> <li> <p>Used to display three dimensions of data on a two dimensional chart.</p> </li> <li> <p>A circle is placed with its centre at a value on the horizontal and vertical axes according to the first two dimensions of data, and the area of the circle reflects the third dimension.</p> <ul> <li>Research suggests that humans are more attuned to the comparing areas of circles than say their diameters.</li> </ul> </li> </ul>"},{"location":"general_terminology/data_viz/#cartograms","title":"Cartograms","text":"Cartogram Example (Max Roser - Our World in Data) <ul> <li> <p>A geographic map where areas of map sections are changed to be proportional to some other value; resulting in a distorted map.</p> </li> <li> <p>In the diagram, we can see that the mosaic cartogram shows the distribution of the global population. Each of the 15,266 pixels represents the home country of 500,000 people. </p> </li> </ul>"},{"location":"general_terminology/data_viz/#histograms","title":"Histograms","text":"Histogram Example (James Chen - Investopedia, 2024) <ul> <li> <p>A type of Bar Chart (categories along the horizontal axis) where the categories are bins (or buckets) and the bars are proportional to the number of items falling into a bin.</p> </li> <li> <p>In the diagram, there are 8 bins, with each bar representing the frequency of the data falling into each bin. Thus, from the histogram we can see that '41-50' has the highest frequency of occurrence.</p> </li> </ul>"},{"location":"general_terminology/data_viz/#line-charts","title":"Line Charts","text":"Line Chart Example (BusinessQ, 2019) <ul> <li> <p>These typically have categories across the horizontal axis and could be considered as a set of line segments joining up the tops off what would be the rectangles on a Bar chart.</p> </li> <li> <p>Multiple lines, associted with multiple series, can be plotted simultaneously without the need to cluster rectangles as is required with Bar Chart.</p> <ul> <li>Multiple series shown in the diagram above.</li> </ul> </li> </ul>"},{"location":"general_terminology/data_viz/#map-charts","title":"Map Charts","text":"Map Chart Example (GapMinder) <ul> <li> <p>Place data on top of geographic maps.</p> </li> <li> <p>For example, if we use a map of the US, the degree of shading of each state could be proportional to some state-related data (e.g.: average income quartile of residents). Or more simply, figures could appear against each state.</p> <ul> <li> <p>Alternatively, similar to the diagram above, bubbles could be placed at the location of major cities (or maybe a bubble per country or state etc.) with their size relating to some aspect of the locale (e.g. population).</p> </li> <li> <p>Data can also be overlaid on a map, for example coloured bands showing different intensities of rainfall in different areas.</p> </li> </ul> </li> </ul>"},{"location":"general_terminology/data_viz/#pie-charts","title":"Pie Charts","text":"Pie Chart Example (Moqups) <ul> <li> <p>These circular charts normally display a single series of categories with values, showing the proportion each category value is of the total.</p> <ul> <li>Sometimes the segments are \"exploded\" away from each other.</li> </ul> </li> <li> <p>For example a series might be the nations that make up the United Kingdom and their populations: England 55.62 million people, Scotland 5.43 million, Wales 3.13 million and Northern Ireland 1.87 million.</p> <ul> <li> <p>The whole circle represents the total of all the category values (e.g. the UK population of 66.05 million people).</p> </li> <li> <p>The ratio of a segment\u2019s angle to 360\u00b0 (i.e. the whole circle) is equal to the percentage of the total represented by the linked category\u2019s value (e.g. Scotland is 8.2% of the UK population and so will have a segment with an angle of just under 30\u00b0).</p> </li> </ul> </li> </ul>"},{"location":"general_terminology/data_viz/#waterfall-chart","title":"Waterfall Chart","text":"Waterfall Chart Example (Eugenia Anello - DataCamp, 2025) <ul> <li> <p>A data visualization technique that shows how an initial value can be affected by the cumulative effect of sequential positive and negative values.</p> <ul> <li> <p>This chart can be used to show either sequential or categorical data.</p> </li> <li> <p>It uses a series of bars that show gains and losses, clearly showing how an opening figure was changed by events and led to the closing figure.</p> </li> </ul> </li> </ul>"},{"location":"general_terminology/data_viz/#international-business-communication-standards-ibcs","title":"International Business Communication Standards (IBCS)","text":"<ul> <li> <p>There are 3 pillars to IBCS:</p> <ul> <li> <p>Conceptual rules</p> <ul> <li>How to organise content such that the message gets across</li> </ul> </li> <li> <p>Perceptual rules</p> <ul> <li>Which visualizations to use in each case</li> </ul> </li> <li> <p>Semantic rules</p> <ul> <li> <p>Actual standardization of business communication.</p> </li> <li> <p>Terminology, descriptions, dimensions, analyses, indicators, etc.</p> </li> </ul> </li> </ul> </li> </ul>"},{"location":"general_terminology/data_viz/#top-5-ibcs-recommendations","title":"Top 5 IBCS Recommendations","text":"<ul> <li> <p>Present time series with horizontal axis</p> <p> IBCS Axis Example (Zebra BI) </p> <ul> <li> <p>2 types of series: time and structure</p> <ul> <li> <p>Time series: horizontal axis (E.g.: months, years, quarters, days, etc.)</p> </li> <li> <p>Structural comparison: vertical axis (E.g.: cities/location, projects, type of revenue, etc.)</p> </li> </ul> </li> </ul> </li> <li> <p>Unify the titles of pages, charts, and tables</p> <p> IBCS Title Example (Zebra BI) </p> <ul> <li> <p>Subject: company, a division, country, or store name</p> </li> <li> <p>Measure with units: measure is the quantity that you are showing.</p> <ul> <li>E.g.: your profits, loss statements, net sales, etc.</li> </ul> </li> <li> <p>Time period: period of which you are showig the data (e.g.: January 2021)</p> </li> </ul> </li> <li> <p>Identify scenarios by fill pattern</p> <p> IBCS Fill Example (Zebra BI) </p> <ul> <li> <p>Solid fill: actual values</p> </li> <li> <p>Hatched: forecast values</p> </li> <li> <p>Outlined: planned</p> </li> </ul> </li> <li> <p>Highlight variances with green and red</p> <p> IBCS Color Example (Zebra BI) </p> <ul> <li> <p>Usually cimple charts in black and white, colors are only reserved for important things</p> </li> <li> <p>Red: negative variances, green: positive variances</p> </li> <li> <p>Negative variances does not necessarily mean negative number</p> <ul> <li>An increase in costs is a positive value that is negative variance</li> </ul> </li> </ul> </li> <li> <p>Use highlighting to get message across</p> <p> IBCS Highlight Example (Zebra BI) </p> <ul> <li>E.g.: highlighting differences between highest and lowest values</li> </ul> <p> Data Analytics</p> <p>Data Science </p> </li> </ul>"},{"location":"general_terminology/languages/","title":"Programming Languages","text":"<p>The programming languages defined below are some of the most commonly utilised languages in this documentation.</p>"},{"location":"general_terminology/languages/#python","title":"Python","text":"<ul> <li> <p>Python is a high-level, easy-to-read programming language used for building websites, analyzing data, automating tasks, and more.</p> </li> <li> <p>It has simple and clear syntax that makes it great for beginners and widely used by professionals.</p> </li> <li> <p>Python supports many libraries and frameworks, which makes it powerful for fields like data science, web development, and machine learning.</p> </li> </ul> <p>For official documentation, refer to Python Documentation.</p>"},{"location":"general_terminology/languages/#numpy-package","title":"NumPy Package","text":"<ul> <li> <p>NumPy is used for numerical computing with powerful tools for working with arrays and matrices.</p> </li> <li> <p>It speeds up math operations using optimized C code behind the scenes.</p> </li> <li> <p>Essential for data science, machine learning, and scientific computing.</p> </li> </ul> <p>Refer to NumPy Documentation for more information.</p>"},{"location":"general_terminology/languages/#pandas-package","title":"Pandas Package","text":"<ul> <li> <p>Pandas makes it easy to work with tables and time series data.</p> </li> <li> <p>It lets you clean, filter, and analyze data with simple commands.</p> </li> <li> <p>Great for Excel-style data manipulation in Python.</p> </li> </ul> <p>For more information, refer to Pandas Website.</p>"},{"location":"general_terminology/languages/#matplotlib-package","title":"Matplotlib Package","text":"<ul> <li> <p>Matplotlib is used for creating charts and plots.</p> </li> <li> <p>You can visualize trends in your data with lines, bars, scatterplots, and more.</p> </li> <li> <p>Highly customizable and works well with Pandas and NumPy.</p> </li> </ul> <p>For more information, refer to Matplotlib Website.</p>"},{"location":"general_terminology/languages/#seaborn-package","title":"Seaborn Package","text":"<ul> <li> <p>Seaborn is built on top of Matplotlib for prettier and easier statistical plots.</p> </li> <li> <p>It comes with themes and functions for common visualizations like heatmaps and boxplots.</p> </li> <li> <p>Great for exploring data relationships visually.</p> </li> </ul> <p>More information available at: Seaborn Website.</p>"},{"location":"general_terminology/languages/#scikit-learn-package","title":"Scikit-learn Package","text":"<ul> <li> <p>Scikit-learn is a simple and efficient tool for machine learning.</p> </li> <li> <p>It includes ready-to-use models for classification, regression, and clustering.</p> </li> <li> <p>Works smoothly with Pandas and NumPy.</p> </li> </ul> <p>Refer to Scikit-learn Website for more information.</p>"},{"location":"general_terminology/languages/#tensorflow-package","title":"Tensorflow Package","text":"<ul> <li> <p>TensorFlow is a deep learning framework developed by Google.</p> </li> <li> <p>It lets you build and train complex neural networks.</p> </li> <li> <p>Used for applications like image recognition, NLP, and more.</p> </li> </ul> <p>More information available at Tensorflow Website.</p>"},{"location":"general_terminology/languages/#keras-package","title":"Keras Package","text":"<ul> <li> <p>Keras is a user-friendly interface for building deep learning models.</p> </li> <li> <p>It runs on top of TensorFlow for quick prototyping.</p> </li> <li> <p>Great for beginners starting with neural networks.</p> </li> </ul> <p>For more information, refer to Keras website.</p>"},{"location":"general_terminology/languages/#pytorch-package","title":"PyTorch Package","text":"<ul> <li> <p>PyTorch is a flexible deep learning framework developed by Facebook.</p> </li> <li> <p>Popular in research and production for dynamic computation graphs.</p> </li> <li> <p>Easy to debug and integrates well with Python code.</p> </li> </ul> <p>For more information, refer to; PyTorch Website.</p>"},{"location":"general_terminology/languages/#beautifulsoup-package","title":"BeautifulSoup Package","text":"<ul> <li> <p>BeautifulSoup helps extract data from HTML and XML files.</p> </li> <li> <p>Used in web scraping to pull content from websites.</p> </li> <li> <p>Simple to learn and works well with other libraries like requests.</p> </li> </ul> <p>Refer to BeautifulSoup Website for more information.</p>"},{"location":"general_terminology/languages/#requests","title":"Requests","text":"<ul> <li> <p>Requests makes it easy to send HTTP requests in Python.</p> </li> <li> <p>Use it to interact with APIs or access web data.</p> </li> <li> <p>Simplifies web communication without needing complex setup.</p> </li> </ul> <p>More information available at Requests Website.</p>"},{"location":"general_terminology/languages/#pyspark-package","title":"PySpark Package","text":"<ul> <li> <p>PySpark is the Python API for Apache Spark, a big data processing engine.</p> </li> <li> <p>It lets you work with large datasets using SQL-like operations and distributed computing.</p> </li> <li> <p>Ideal for data analysis and machine learning on massive data across clusters.</p> </li> </ul> <p>For more information, refer to PySpark Website.</p>"},{"location":"general_terminology/languages/#structured-query-language-sql","title":"Structured Query Language (SQL)","text":"<ul> <li> <p>A domain-specific language used to manage data, especially in a relational database management system.</p> </li> <li> <p>It is particularly usedul in handling structured data, i.e. data incorporating relations among entities and variables.</p> </li> </ul> <p>Refer to PostgreSQL Website for more information on PostgreSQL language.</p>"},{"location":"general_terminology/languages/#transact-sql-t-sql","title":"Transact-SQL (T-SQL)","text":"<ul> <li> <p>An extension of the SQL language used primarily within Microsoft SQL Server.</p> <ul> <li>It provides all the functionality of SQL but with some added extras.</li> </ul> </li> </ul> <p>For more information, refer to Microsoft: Transact-SQL reference (Database engine).</p>"},{"location":"general_terminology/languages/#r","title":"R","text":"<ul> <li> <p>An open source programming language and environment for developing statistical computing and graphics.</p> </li> <li> <p>The language of choice for statistical analysis, which is a very important feature in Data Science.</p> </li> <li> <p>R\u2019s popularity comes from the fact that most statistical methods developed in research environments lead to the production of ready-to-use freely available R packages.</p> </li> </ul> <p>Big Data </p>"},{"location":"power_bi/best_design_pracs/","title":"Best Dashboard Design Practices","text":""},{"location":"power_bi/best_design_pracs/#understand-the-business-context-and-audience","title":"Understand the Business Context and Audience","text":"<ul> <li> <p>Executive Dashboards: Focus on high-level KPIs like revenue growth, profit margins, or customer satisfaction trends. Keep visualizations concise and impactful.</p> </li> <li> <p>Operational Dashboards: Highlight process metrics, such as daily production efficiency or supply chain performance, with real-time data analytics.</p> </li> <li> <p>Analytical Dashboards: Offer drill-down capabilties and granular data for power users to explore.</p> </li> </ul>"},{"location":"power_bi/best_design_pracs/#simplify-the-user-experience-with-intuitive-navigation","title":"Simplify the User Experience with Intuitive Navigation","text":"<ul> <li> <p>Multi-page reports and complex datasets can overwhelm users, so thoughtful navigation is essential.</p> </li> <li> <p>Filters or bookmarks can be used to make navigation between pages or segments of data easier.</p> <ul> <li> <p>Filters: allow executives to focus on different segments of data with a single click.</p> </li> <li> <p>Bookmarks and buttons: allow executives to navigative between different pages easily.</p> </li> </ul> </li> </ul>"},{"location":"power_bi/best_design_pracs/#traditional-vs-optimized-navigation","title":"Traditional vs. Optimized Navigation","text":"Traditional Navigation Optimized Navigation Overloaded with filters Contextual filters only Unclear tab names Descriptive, user-friendly labels Data visual clutter Logical grouping of visuals"},{"location":"power_bi/best_design_pracs/#create-a-clear-visual-hierarchy","title":"Create a Clear Visual Hierarchy","text":"<ul> <li> <p>Effective Power BI UI design ensures users can see the big picture first and dive deeper into details when needed. A clear visual hierarchy organizes information logically.</p> </li> <li> <p>Example layout:</p> <ul> <li> <p>Top Section: high-level KPIs (e.g.: revenue, expenses, net profit).</p> </li> <li> <p>Middle Section: supporting charts (e.g.: sales trends, regional performance).</p> <ul> <li> <p>This should represent trend-based data including activity-based metrics, and visuals that demonstrate data over time. </p> </li> <li> <p>Best suited for larger visuals.</p> </li> </ul> </li> <li> <p>Bottom Section: detailed data tables or supplementary visuals (e.g.: KPIs, or Tables).</p> </li> </ul> </li> </ul>"},{"location":"power_bi/best_design_pracs/#design-checklist","title":"Design Checklist","text":"<ul> <li> <p>Ensure you are visualizing 6-10 insights per page.</p> </li> <li> <p>Match the right insights with the correct visual.</p> <p> Power BI Visualizations by Category (Numerro, 2022) </p> </li> <li> <p>Use a logical layout by grouping related metrics.</p> <p> Power BI Logical Layout (Numerro, 2022) </p> </li> <li> <p>Ensure the correct sizing of visuals.</p> <p> Power BI Visual Sizing Guide (Numerro, 2022) </p> </li> <li> <p>Apply suitable margin and padding.</p> <ul> <li> <p>When working with boxes, be generous with the margin outside of the boxes between visuals, and then padding within each visuals box.</p> </li> <li> <p>This allows the content in the box more room to breathe and becomes more legible for the end user.</p> </li> </ul> </li> <li> <p>Apply a visual hierarchy.</p> <ul> <li> <p>Visual hierarchy is about making elements of different importance stand out accordingly on the dashboard. </p> </li> <li> <p>In this example, the numeric value is more important than the textual description, so you can increase the font size and the font-weight to make it stand out. </p> <ul> <li>The subheading can be de-emphasized through smaller font size, lighter colour, and wider letter spacing.</li> </ul> </li> </ul> <p> Power BI Example of Visual Hierarchy (Numerro, 2022) </p> </li> <li> <p>Use a consistent colour palette.</p> <ul> <li>Build your own colour palette to stay consistent when using colours across your report. Usng a colour palette helps you understand which colour to use for certain aspects of a report.</li> </ul> </li> <li> <p>Use distinguishable colours with enough contrast.</p> <ul> <li>Use colour that are easy to distinguish between one another, e.g. using contrasting colours compared to shades of the same colour, as this can make it hard to tell the difference. </li> </ul> <p> Power BI Example Color Contrast (Numerro, 2022) </p> </li> <li> <p>Use a consistent theme.</p> <ul> <li>Ensure you stay consistent with you theme's colour palette across your visuals, and limit the number of colours per visual, maximum 6.</li> </ul> </li> <li> <p>Add clear headings and labels.</p> <ul> <li> <p>Helps to add clarity and context to the information you've provided.</p> </li> <li> <p>This principle will also give your audience the ability to extract valuable insights at a glance, eliminating any confusion.</p> </li> </ul> </li> <li> <p>Round your numbers where necessary.</p> </li> </ul>"},{"location":"power_bi/best_design_pracs/#consistency-in-branding-and-design","title":"Consistency in Branding and Design","text":"<ul> <li> <p>Consistency reinforces credibility and ensures a professional look and feel. Your dashboard should align with the organization's branding standards.</p> </li> <li> <p>Branding elements to include:</p> <ul> <li> <p>Company logo, placed subtly.</p> </li> <li> <p>Brand colors for charts, graphs, and accents.</p> </li> <li> <p>Fonts and typography consistent with corporate identity.</p> </li> </ul> </li> <li> <p>Use Power BI's theme customization feature as shown below to apply consistent branding across all reports.</p> <p> Themes for Dashboards (aufaitUX, 2025) </p> </li> </ul>"},{"location":"power_bi/best_design_pracs/#design-for-mobile-and-desktop-experiences","title":"Design for Mobile and Desktop Experiences","text":"<ul> <li> <p>Use Power BI's Mobile Layout to create device-specific views.</p> </li> <li> <p>Prioritize essential metrics and visuals for smaller screens.</p> </li> <li> <p>Test the dashboard on different devices to ensure responsiveness.</p> </li> </ul>"},{"location":"power_bi/best_design_pracs/#mobile-vs-desktop-dashboard-design","title":"Mobile vs. Desktop Dashboard Design","text":"Mobile Design Desktop Design Minimal visuals, focus on KPIs Comprehensive data visualization Large buttons for touch screens Interactive slicers and tooltips Simplified navigation Detailed multi-page navigation"},{"location":"power_bi/best_design_pracs/#optimize-use-of-white-space","title":"Optimize Use of White Space","text":"<ul> <li> <p>White space isn't wasted space; it's a design element that creates balance and reduces visual clutter.</p> </li> <li> <p>Why white space matters:</p> <ul> <li> <p>Enhances readability and focus.</p> </li> <li> <p>Prevents data overload by separating visuals.</p> </li> <li> <p>Creates a clean, professional aesthetic.</p> </li> </ul> </li> <li> <p>Use gridlines and alignment tools in Power BI to space out visuals evenly.</p> </li> </ul>"},{"location":"power_bi/best_design_pracs/#colour-with-purpose","title":"Colour with Purpose","text":"<ul> <li> <p>Best practices for color:</p> <ul> <li> <p>Use a limited palette to maintain coherence.</p> </li> <li> <p>Apply consistent colours for recurring categories (e.g.: blue for revenue, green for growth).</p> </li> <li> <p>Leverage contrast for reability (e.g.: dark text on light backgrounds).</p> </li> </ul> </li> <li> <p>What to avoid:</p> <ul> <li> <p>Overuse of bright, distracting colours.</p> </li> <li> <p>Relying on colour alone to convey information (ensure accessibility for colourblind users).</p> </li> </ul> </li> </ul>"},{"location":"power_bi/best_design_pracs/#maximize-interactivity","title":"Maximize Interactivity","text":"<ul> <li> <p>Interactivity transforms static dashboards into dynamic tools that empower users to explore data.</p> </li> <li> <p>Interactive features to include:</p> <ul> <li> <p>Slicers and filters: allow users to segment data by time, region, or category.</p> </li> <li> <p>Drill-throughs: enable deep dives into specific data points.</p> </li> <li> <p>Tooltips: provide additional context without cluttering the visuals.</p> </li> </ul> </li> <li> <p>Keep interactivity intuitive and avoid overloading users with too many options.</p> </li> </ul>"},{"location":"power_bi/best_design_pracs/#ensure-data-accuracy-and-clarity","title":"Ensure Data Accuracy and Clarity","text":"<ul> <li> <p>Data integrity is non-negotiable. Even the most visually stunning dashboard loses credibility if the data is flawed. </p> </li> <li> <p>Tips for accurate data presentation:</p> <ul> <li> <p>Use consistent formatting for dates, numbers, and currencies.</p> </li> <li> <p>Validate data sources and refresh schedules.</p> </li> <li> <p>Avoid overloading visuals with excessive data points.</p> </li> </ul> </li> </ul>"},{"location":"power_bi/best_design_pracs/#test-iterate-and-improve","title":"Test, Iterate, and Improve","text":"<ul> <li> <p>Dashboards evolve with user feedback and business needs. Regular testing ensures your Power BI designs remain relevant and effective.</p> </li> <li> <p>Testing checklist:</p> <ul> <li> <p>Are objectives being met?</p> </li> <li> <p>Is navigation seamless?</p> </li> <li> <p>Are visuals loading quickly?</p> </li> <li> <p>Is data accurate and up-to-date?</p> </li> </ul> </li> <li> <p>Use Power BI's usage analytics to monitor how users interact with your dashboard.</p> </li> </ul>"},{"location":"power_bi/vertipaq/","title":"VertiPaq Engine","text":"<ul> <li> <p>Can be understood as the \"brain and muscles\" of the system behind not only Power BI, but also SSAS Tabular and Excel Power Pivot. </p> </li> <li> <p>When you send the query to get data for your Power BI report, here is what happens:</p> <ul> <li> <p>Formula Engine (FE) accepts the request, process it, generates the query plan and finally executes it.</p> </li> <li> <p>Storage Engine (SE) pulls the data out of Tabular model to stisfy the request issed within the query generated by the Formula Engine.</p> </li> </ul> </li> <li> <p>Storage Engine works in two different ways in order to retrieve requested data: VertiPaq keeps the snapshot of the data in-memory, This snapshot can be refreshed from time to time, from the original data source.</p> <ul> <li>DirectQuery doesn't store any data. It just forwards the query straight to the data source for every single request.</li> </ul> </li> </ul>"},{"location":"power_bi/vertipaq/#formula-engine","title":"Formula Engine","text":"<ul> <li> <p>Formula Engine accepts the query, and since it's able to \"understand\" DAX, it \"translates\" DAX into a specific query plan, consisting of physical operations that need to be executed in order to get resuts back.</p> </li> <li> <p>Physical operations: joins between multiple tables, filtering, or aggregations.</p> <ul> <li>Requests to Storage Engine are always being sent sequentially.</li> </ul> </li> </ul>"},{"location":"power_bi/vertipaq/#storage-engine","title":"Storage Engine","text":"<ul> <li> <p>Once the query has been generated and executed by the Formula Engine, the Storage Engine comes into the scene. It physically goes through the data stored within the Tabular model (VertiPaq) or goes directly to a different data source (E.g.: if DirectQuery mode is in place, SQL Server).</p> </li> <li> <p>There are 3 options when specifiying the storage engine for the table:</p> <ul> <li> <p>Import mode: based on VertiPaq. Table data is being stored in-memory as a snapshot. Data can be refrehed periodically.</p> </li> <li> <p>DirectQuery mode: data is being retrieved from the data source at the query time. Data resides in its original source before, during and after the query execution.</p> </li> <li> <p>Dual mode: combination of the first two options. Data from the table is being loaded into memory, but at the query time it can be also retrieved directly from the source.</p> </li> </ul> </li> <li> <p>As opposed to the Formula Engine that doesn't support parallelism, the Storage Engine can work asynchronously.</p> </li> </ul>"},{"location":"power_bi/vertipaq/#vertipaq-storage-engine","title":"VertiPaq Storage Engine","text":"<ul> <li> <p>When we choose Import mode for our Power BI tables, VertiPaq performs the following actions:</p> <ul> <li> <p>Reads the data source, transforms data into columnar structure, encodes and compresses data within each of the columns.</p> </li> <li> <p>Establishes dictionary and index for each of the columns.</p> </li> <li> <p>Prepares and establishes relationships.</p> </li> <li> <p>Computes all calculated columns and calculated tables and compresses them.</p> </li> </ul> </li> <li> <p>Two main characteristics of VertiPaq are:</p> <ul> <li> <p>VertiPaq is a columnar database.</p> </li> <li> <p>VertiPaq is an in-memory database.</p> <p> Row-Store vs. Column-Store (Data Mozart, 2021) </p> <ul> <li>Columnar databases are optimized for vertical data scanning, which means that every column is structured in its own way and physically separated from other columns.</li> </ul> </li> </ul> </li> <li> <p>With columnar databases, single column access is fast and effective. Once the computation starts to involve multiple columns, things become more complex, as intermediary steps' results need to be temporarily stored in some way.</p> <ul> <li>Simply said, columnar databases are more CPU exhaustive, while row-store databases increase I/O, because of many scans of useless data.</li> </ul> </li> </ul>"},{"location":"programming_tools/git/","title":"Git","text":""},{"location":"programming_tools/git/#installing-git-and-github","title":"Installing Git and Github","text":"<p>Follow this link to find the steps to install Git.</p> <p>Once Git has been installed, follow this link to create a Github account.</p>"},{"location":"programming_tools/git/#git-vs-github","title":"Git vs. Github","text":"<ul> <li> <p>Git is an open-source, version control tool created in 2005 by developers working on the Linux operating system.</p> </li> <li> <p>Github is a company foudned in 2008 that makes tools which integrate with Git.</p> </li> <li> <p>You do not need Github to use Git, but you cannot use GitHub without using Git.</p> </li> </ul>"},{"location":"programming_tools/git/#step-1-create-a-local-git-repository","title":"Step 1: Create a local git repository","text":"<ul> <li> <p>You typically obtain a Git repository in one of two ways:</p> <ul> <li> <p>You can take a local directory that is currently not under version control, and turn it into a Git repository, or</p> </li> <li> <p>You can <code>clone</code> an existing Git repository from elsewhere.</p> </li> </ul> </li> </ul>"},{"location":"programming_tools/git/#existing-directory","title":"Existing Directory","text":"<ul> <li> <p>If you have a project directory that is currently not under version control and you want to start controlling it with Git, you first need to go to that project's directory. </p> <p>PowerShell<pre><code>cd /home/user/my_project\n</code></pre> - After that, type the command below to initialize a git repository in the root of the folder:</p> PowerShell<pre><code>git init\n</code></pre> </li> </ul>"},{"location":"programming_tools/git/#cloning-an-existing-repository","title":"Cloning an Existing Repository","text":"<ul> <li> <p>You clone a repository with <code>git clone &lt;url&gt;</code>. For example, if you want to clone the Git linkable library called <code>libgit2</code>, you can do so like this:</p> <p>PowerShell<pre><code>git clone https://github.com/libgit2/libgit2\n</code></pre> - That creates a directory named <code>libgit2</code>, initializes a <code>.git</code> directory inside it, pulls down all the data for that repository, and checks out a working copy of the latest version.</p> <ul> <li>If you go into the new <code>libgit2</code> directory that was just created, you'll see the project files in there, ready to be worked on or used.</li> </ul> </li> <li> <p>If you want to clone the repository into a directory named something other than <code>libgit2</code>, you can specify the new directory name as an additional argument:</p> PowerShell<pre><code>git clone https://github.com/libgit2/libgit2 mylibgit\n</code></pre> </li> </ul>"},{"location":"programming_tools/git/#step-2-add-a-new-file-to-the-repo","title":"Step 2: Add a new file to the repo","text":"<ul> <li> <p>To create a new file, run a touch command.</p> <ul> <li><code>touch newfile.txt</code> just creates and saves a blank file named newfile.txt.</li> </ul> </li> <li> <p>Once you've added or modified files in a folder containing the git repo, git will notice that the file exists inside the repo.</p> <ul> <li> <p>Git will not track the file unless you explicitly tell it to. </p> </li> <li> <p>Git only saves/manages changes to files that it tracks, so we need to send a command to confirm that we want Git to track our new file.</p> </li> </ul> </li> <li> <p>After creating the new file, you can use the git status command to see which files Git knows exist.</p> </li> </ul>"},{"location":"programming_tools/git/#step-3-add-a-file-to-the-staging-environment","title":"Step 3: Add a file to the staging environment","text":"<ul> <li> <p>Add a file to the staging environment using the git add command.</p> </li> <li> <p>If you rerun the git status command, you'll see that git has added the file to the staging environment. </p> </li> </ul> <p>The file has not yet been added to a commit, but it's about to be.</p>"},{"location":"programming_tools/git/#step-4-create-a-commit","title":"Step 4: Create a commit","text":"<ul> <li> <p>Run:</p> PowerShell<pre><code>git commit -m \"Your message about the commit\"\n</code></pre> </li> <li> <p>The commit message should be something related to what the commit contains.</p> <ul> <li>When a clear explanation is given, it helps future programmers figure out why some changes were made years later.</li> </ul> </li> </ul>"},{"location":"programming_tools/git/#step-5-create-a-new-branch","title":"Step 5: Create a new branch","text":"<ul> <li> <p>Branches allow you to move back and forth between 'states' of a project.</p> <ul> <li>Official Git docs: \"A branch in Git is simply a lightweight movable pointer to one of these commits\".</li> </ul> </li> <li> <p>When you create a new branch, Git keeps track of which commit your branch 'branched' off of, so it knows the history behind all the files.</p> </li> <li> <p>To create a new branch, run:</p> PowerShell<pre><code>git checkout -b &lt;branch-name&gt;\n</code></pre> <ul> <li>This command automatically creates a new branch and then 'checks you out' on it, meaning Git will move you to that branch, off of the primary branch.</li> </ul> </li> <li> <p>After running to command above, you can use <code>git branch</code> command to confirm that your branch was created.</p> <ul> <li>The branch name with the asterisk next to it indicates which branch you're on at the given time. </li> </ul> </li> </ul>"},{"location":"programming_tools/git/#step-6-create-a-new-repository-on-github","title":"Step 6: Create a new repository on Github","text":"<ul> <li> <p>If you only want to keep track of your code locally, you don't need to use Github.</p> </li> <li> <p>If you want to work with a team, you can use Github to collaboratively modify the project's code.</p> </li> <li> <p>To create a new repository on Github, log in and go to the Github home page. </p> <ul> <li> <p>You can find the \"New repository\" option under the \"+\" sign next to your profile picture, in the top right corner of the navbar.</p> <p> Create A New Repository in Github (HubSpot, 2020) </p> </li> <li> <p>After clicking the button, Github will ask you to name your repository and provide a brief description:</p> <p> Creating a Repository (HubSpot, 2020) </p> <ul> <li>When you're done filling out the information, press the 'Create repository' button to make your new repository.</li> </ul> </li> <li> <p>Github will ask if you want to create a new repository from scratch or if you want to add a repository you have created locally. </p> <ul> <li> <p>If you already have an existing local repository, follow the '...or push an existing repository from the command line' section.</p> </li> <li> <p>Generally, the code would be: <code>git remote add origin {link_from_github}</code>.</p> </li> </ul> </li> </ul> </li> </ul>"},{"location":"programming_tools/git/#step-7-push-a-branch-to-github","title":"Step 7: Push a branch to Github","text":"<ul> <li> <p>This allows other people to see the changes you've made.</p> <ul> <li>If they're approved by the repository's owner, the changes can then be merged into the primary branch.</li> </ul> </li> <li> <p>To push changes onto a new branch on Github, you'll want to run <code>git push origin {your_branch_name}</code>.</p> <ul> <li>Github will automatically create the branch for you on the remote repository. </li> </ul> </li> <li> <p>If you refresh the Github page, you'll see a note saying that you have just pushed a branch into the repository.</p> <ul> <li> <p>You can also click the 'branches' link to see your branch listed there.</p> <p> Compare &amp; pull request (HubSpot, 2020) </p> </li> </ul> </li> </ul>"},{"location":"programming_tools/git/#step-8-create-a-pull-request-pr","title":"Step 8: Create a pull request (PR)","text":"<ul> <li> <p>A pull request (or PR) is a way to alert a repo's owners that you want to make some changes to their code.</p> <ul> <li>It allows them to review the code and make sure it looks good before putting your changes on the primary branch.</li> </ul> </li> <li> <p>This is what the PR page looks like before you've submitted it:</p> <p> Open a pull request (HubSpot, 2020) </p> </li> <li> <p>This is what it looks like once you've submitted the PR request:</p> <p> Submitted pull request (HubSpot, 2020) </p> </li> </ul>"},{"location":"programming_tools/git/#step-9-merge-a-pr","title":"Step 9: Merge a PR","text":"<ul> <li> <p>Click the 'Merge pull request' button. This will merge your changes into the primary branch.</p> <p> Merged pull request (HubSpot, 2020) </p> </li> <li> <p>You can double check that your commits were merged by clicking on the 'Commits' link on the first page of your new repository.</p> <p> Commits (HubSpot, 2020) </p> <ul> <li>This will show you a list of all commits in that branch. </li> </ul> </li> </ul>"},{"location":"programming_tools/git/#step-10-get-changes-on-github-back-to-your-computer","title":"Step 10: Get changes on Github back to your computer","text":"<ul> <li> <p>In order to get the most recent changes that you or others have merged on Github, use <code>git pull origin main</code> command.</p> </li> <li> <p>This shows you all the files that have changed and how they've changed.</p> </li> <li> <p>if you want to update your local main file, you can change to the main branch by using <code>git checkout main</code> command.</p> </li> </ul>"},{"location":"terminology/analyze_train_data/","title":"Analyze and Train Data","text":"Fabric Items Under \"Analyze and Train Data\" <p>In this section, we will be defining the Environment and ML Model items, since Notebook and Spark Job Definition were already defined previously.</p>"},{"location":"terminology/analyze_train_data/#environment","title":"Environment","text":"<ul> <li> <p>Logical container in Microsoft Fabric used to group and manage related resources like Lakehouses, Warehouses, Pipelines, and Notebooks.</p> </li> <li> <p>Controls access, security, and compute settings across all items within the environment.</p> <ul> <li> <p>Once it is set up, you can attch it to your Spark Job definition, as well as Notebooks. Then, ass the libraries and the configurations will be there when the session gets started.</p> </li> <li> <p>All the configuration libraries can be patched together, seamless code experience.</p> </li> </ul> </li> <li> <p>Supports collaboration and governance, making it easier to organize projects and apply consistent policies.</p> </li> </ul>"},{"location":"terminology/analyze_train_data/#ml-model","title":"ML Model","text":"<ul> <li> <p>Machine Learning (ML) models are trained algorithms that learn patterns from data to make predictions or classifications.</p> </li> <li> <p>In Fabric, ML models can be created, trained, and deployed within Notebooks, Pipelines, or integrated environments.</p> </li> <li> <p>They support a range of use cases such as forecasting, classification, recommendation systems, and anomaly detection.</p> </li> </ul> <p>Refer to Using Fabric Tools: ML Model.</p> <p> Prepare Data</p> <p>OneLake </p>"},{"location":"terminology/get_data/","title":"Get Data","text":"Fabric Items Under \"Get Data\" <p>While there are many items that can be utilized under \"Get Data\", the main ones that are used are: copy job, data pipelines, dataflow Gen2, and notebook. Thus, these items will be defined and explained below.</p>"},{"location":"terminology/get_data/#data-pipelines","title":"Data Pipelines","text":"<ul> <li> <p>Data orchestration tool in Microsoft Fabric that automates the movement and transformation of data across different sources and destinations.</p> </li> <li> <p>Supports low-code and code-first experiences, allowing users to create dataflows with drag-and-drop activities or advanced custom logic.</p> </li> <li> <p>Integrates seamlessly with Fabric services like Lakehouse, Warehouse, and Real-Time Analytics for end-to-end data management and analysis.</p> <ul> <li>Efficient at importing data from the cloud (e.g., ADLS, Azure SQL).</li> </ul> </li> <li> <p>Can trigger a variety of actions in Fabric:</p> <ul> <li> <p>Dataflow Gen2</p> </li> <li> <p>Notebooks</p> </li> <li> <p>Stored Procedures</p> </li> <li> <p>KQL script</p> </li> <li> <p>Generic webhooks</p> </li> <li> <p>Azure Functions</p> </li> <li> <p>Azure Databricks</p> </li> <li> <p>Azure ML</p> </li> </ul> </li> <li> <p>Con: Cannot access on-premises data.</p> </li> <li> <p>Con: Does not have ability to do transformation on its own but can embed Notebooks or Dataflows to perform that task.</p> </li> <li> <p>Con: Does not work cross-workspace as of now.</p> </li> </ul> <p>For more information on Data Pipelines, refer to Using Fabric Tools: Data Pipelines.</p>"},{"location":"terminology/get_data/#copy-job","title":"Copy Job","text":"<ul> <li> <p>Data movement activity that efficiently copies data from a source to a destination within the Microsoft Fabric ecosystem.</p> </li> <li> <p>Supports a wide range of connectors, enabling transfers between databases, cloud storage, APIs, and Fabric-native stores.</p> </li> <li> <p>Optimized for high performance, handling large-scale data loads with options for incremental copy, parallelism, and schema mapping.</p> </li> <li> <p>Copy Job is a simplified, user-friendly way to move data, while a Data Pipeline is a more complex, flexible solution that can handle data ingestion and more.</p> </li> <li> <p>Does incremental copy of the data. This means that it initially copies all data, and subsequent runs copy only changes.</p> <ul> <li>Does not capture deleted rows.</li> </ul> </li> </ul>"},{"location":"terminology/get_data/#dataflow-gen2","title":"Dataflow Gen2","text":"<ul> <li> <p>Data preparation tool that enables users to ingest, transform, and cleanse data using a visual, low-code interface.</p> <ul> <li>Can do Extract, Transform and Load tasks using familiar Power Query interface.</li> </ul> </li> <li> <p>Built on Fabric\u2019s unified data platform, allowing direct output to Lakehouses, Warehouses, and other Fabric items.</p> </li> <li> <p>Supports complex transformations with features like incremental refresh, computed columns, and integration with pipelines.</p> <ul> <li>Can combine more than one dataset in a dataflow.</li> </ul> </li> <li> <p>Since most files are stored in Sharepoint, this is the main method to bring in data.</p> </li> <li> <p>Con: Struggles with large datasets, which can be costly if you are transforming loads of data regularly.</p> </li> </ul> <p>More information is available at Using Fabric Tools: Dataflow Gen2</p>"},{"location":"terminology/get_data/#notebook","title":"Notebook","text":"<ul> <li> <p>Interactive development environment for writing and executing code in languages like Python, SQL, and Spark.</p> </li> <li> <p>Integrated with Fabric\u2019s compute and storage, allowing direct access to Lakehouses and other data assets.</p> <ul> <li>Can be used to bring data into Fabric via connecting to API\u2019s or client Python libraries.</li> </ul> </li> <li> <p>Supports data exploration, machine learning, and automation with rich outputs like charts, tables, and visualizations.</p> <ul> <li>Can do data validation on incoming data.</li> </ul> </li> </ul> <p>Refer to Using Fabric Tools: Notebook for more information.</p>"},{"location":"terminology/get_data/#comparing-all-methods","title":"Comparing All Methods","text":"Category Pipeline Copy Activity Dataflow Gen 2 Spark Notebook Use case Data lake and data warehouse migration, data ingestion, lightweight transformation Data ingestion, data transformation, data wrangling, data profiling Data ingestion, data transformation, data processing, data profiling Data exploration, transformation, machine learning, automation Primary developer persona Data engineer, data integrator Data engineer, data integrator, business analyst Data integrator, data engineer Data scientist, data engineer, analyst Primary developer skill set ETL, SQL, JSON ETL, M, SQL Spark (Scala, Python, Spark SQL, R) Python, SQL, Spark, machine learning Code written No code, low code No code, low code Code Code Data volume Low to high Low to high Low to high Low to high Development interface Wizard, canvas Power Query Notebook, Spark job definition Notebook Sources 30+ connectors 150+ connectors Hundreds of Spark libraries Lakehouse, Warehouse, Spark tables, external sources via code Destinations 18+ connectors Lakehouse, Azure SQL Database, Azure Data Explorer, Azure Synapse Analytics Hundreds of Spark libraries Lakehouse, Warehouse, visualization outputs Transformation complexity Low: lightweight - type conversion, column mapping, merge/split files, flatten hierarchy Low to high: 300+ transformation functions Low to high: support for native Spark and open-source libraries Low to high: custom transformations using Python, SQL, Spark <p> General Terminology: Data Science</p> <p>Store Data </p>"},{"location":"terminology/onelake/","title":"OneLake","text":"<ul> <li> <p>OneLake is a single, unified, logical data lake for your whole organization. A data lake processes large volumes of data from various sources. Like OneDrive, OneLake comes automatically with every Microsoft Fabric tenant and is designed to be the single place for all your analytics data.</p> </li> <li> <p>OneLake is the core of Microsoft Fabric - it is essentially the storage account for all of your data utilized within Fabric, be that within Azure or from another cloud.</p> </li> </ul>"},{"location":"terminology/onelake/#shortcuts","title":"Shortcuts","text":"<ul> <li> <p>They allow you to connect you to your data which reside elsewhere. This could be files/tables from other Fabric workspaces, or external ADLS Gen2/AWS S3 accounts.</p> </li> <li> <p>This will reduce the need for duplication and unnecessary data copy, but only if your source data is stored in one of the above data stores. Maybe other supported \"shortcut\" sources will come in the future.</p> </li> <li> <p>Shortcuts can be created from any type of file, or a \"Delta\" table.</p> <ul> <li>If you create a shortcut to a Delta table (created from Databricks, for example) and you place this shortcut at the top level of the \"Tables\" directory in OneLake*, then that Delta table should be automatically recognized and made available for querying in your Lakehouse.</li> </ul> </li> </ul>"},{"location":"terminology/onelake/#exploring-onelake","title":"Exploring OneLake","text":""},{"location":"terminology/onelake/#lakehouse-view","title":"Lakehouse view","text":"<ul> <li> <p>This can be viewed within the Fabric UI. Tables and files can be browsed using a familiar folder structure, as per the below screenshot. </p> </li> <li> <p>Certain file types can be previewed (like CSV/JSON), other file types can't (e.g. parquet).</p> <p> OneLake UI (endjin, 2023) </p> </li> </ul>"},{"location":"terminology/onelake/#onelake-file-explorer","title":"OneLake File Explorer","text":"<ul> <li> <p>This mounts your OneLake to your Windows file explorer, just like the OneDrive File Explorer. </p> <ul> <li>This makes it easy to upload/browse/interact with files locally. </li> </ul> </li> <li> <p>If you need to inteact with a file, then you must double click it to have it downloaded. </p> </li> </ul>"},{"location":"terminology/onelake/#key-benefits-and-drawback-of-onelake","title":"Key benefits and Drawback of OneLake","text":""},{"location":"terminology/onelake/#benefits","title":"Benefits","text":"<ul> <li> <p>No infrastructure to manage (since it's SaaS).</p> </li> <li> <p>All organizational data stored in same, logical storage account.</p> </li> <li> <p>Each worksapce can have its own admins/access policies/region assignment.</p> </li> <li> <p>Unified governance policies easier to enforce.</p> </li> <li> <p>Compatibility with most ADLS Gen2 APIs (and a subset of blob storage APIs).</p> </li> <li> <p>OneLake File Explorer.</p> </li> <li> <p>Interoperability (enabled by using the Delta open table format).</p> </li> </ul>"},{"location":"terminology/onelake/#drawbacks","title":"Drawbacks","text":"<ul> <li> <p>Less configurable than ADLS.</p> </li> <li> <p>One OneLake per tenant might make strict multi-tenanted Fabric scenarios harder to sell to the security team. A separated OneLake would require a separate AAD tenant (just like Power BI).</p> </li> </ul> <p> Analyze and Train Data</p>"},{"location":"terminology/prepare_data/","title":"Prepare Data","text":"Fabric Items Under \"Prepare Data\" <p>Since Copy Job, Data Pipeline, Dataflow Gen2, and Notebooks have already been defined under Get Data, we will only define Apache Airflow Job and Spark Job Definition in this section.</p>"},{"location":"terminology/prepare_data/#apache-airflow-job","title":"Apache Airflow Job","text":"<ul> <li> <p>Powered by Apache Airflow.</p> </li> <li> <p>Task orchestration framework that schedules and manages complex workflows as Directed Acyclic Graphs (DAGs).</p> </li> <li> <p>Code-first approach where developers define workflows using Python scripts, enabling dynamic pipeline generation.</p> <ul> <li>Ideal if you have experience with Apache Airflow or you prefer code-centric approach.</li> </ul> </li> <li> <p>Integrates with a wide range of systems (databases, APIs, cloud services) for ETL, data processing, and automation tasks.</p> </li> </ul>"},{"location":"terminology/prepare_data/#spark-job-definition","title":"Spark Job Definition","text":"<ul> <li> <p>Microsoft Fabric code item that allows you to submit batch/streaming jobs to Spark clusters.</p> <ul> <li>To run a Spark job definition item, you must have a main definition file and default lakehouse context.</li> </ul> </li> <li> <p>By uploading the binary files from the compilation output of different languages (for example, .jar from Java), you can apply different transformation logic to the data hosted on a lakehouse.</p> <ul> <li>Besides the binary file, you can further customize the behavior of the job by uploading more libraries and command line arguments.</li> </ul> </li> <li> <p>To run a Spark job definition, you must have at least one lakehouse associated with it. This default lakehouse context serves as the default file system for Spark runtime. For any Spark code using a relative path to read/write data, the data is served from the default lakehouse.</p> </li> </ul> <p> Store Data</p> <p>Analyze and Train Data </p>"},{"location":"terminology/store_data/","title":"Store Data","text":"<p>While there are many items that can be utilised to store data, the main ones that are used are Lakehouses, SQL databases, semantic model and warehouses.</p> <p>In this section, these terms will be defined.</p>"},{"location":"terminology/store_data/#lakehouse","title":"Lakehouse","text":"<ul> <li> <p>A collection of files, folders, and tables that represent a database over a data lake used by Apache Spark engine and SQL engine for big data processing.</p> </li> <li> <p>Contains files in various formats (structured and unstructured) organised in folders and subfolders.</p> </li> </ul> <p>For more information, refer to Using Fabric Tools: Lakehouse.</p>"},{"location":"terminology/store_data/#sql-analytics-endpoint","title":"SQL Analytics Endpoint","text":"<ul> <li> <p>Enables the querying of data in the Lakehouse using T-SQL language and TDS protocol.</p> </li> <li> <p>Every Lakehouse has one SQL Analytics Endpoint.</p> </li> <li> <p>This can be used to connect to external sources such as Excel and Power BI reports.</p> </li> </ul> <p>For more information, refer to SQL Analytics Endpoint.</p>"},{"location":"terminology/store_data/#fabric-sql-database","title":"Fabric SQL Database","text":"<ul> <li> <p>Developer-friendly transactional database, based on Azure SQL Database, that allows you to easily create your operational database in Fabric.</p> </li> <li> <p>Stores data in open-source Delta Lake format.</p> </li> <li> <p>SQL database in Fabric creates 3 items in your Fabric workspace:</p> <ul> <li> <p>Data in your SQL database is automatically replicated into the OneLake and converted to Parquet, in an analytics-ready format.</p> </li> <li> <p>A SQL analytics endpoint</p> </li> <li> <p>A default semantic model</p> </li> </ul> </li> </ul>"},{"location":"terminology/store_data/#warehouses","title":"Warehouses","text":"<ul> <li> <p>Lake-centric warehouse built on an enterprise-grade distributed processing engine.</p> </li> <li> <p>Not a traditional enterprise data warehouse, it is a lake warehouse that supports two distinct warehousing items:</p> <ul> <li> <p>Fabric data warehouse</p> </li> <li> <p>SQL analytics endpoint</p> </li> </ul> </li> </ul> <p>For more information, refer to Using Fabric Tools: Warehouse.</p>"},{"location":"terminology/store_data/#warehouse-sql-database-or-lakehouse","title":"Warehouse, SQL Database, or Lakehouse?","text":"Data Storage Lakehouse Warehouse SQL Database Data Format Open data formats (Delta/Parquet) Proprietary formats optimized for analytics Traditional relational formats (e.g., row-based) Storage OneLake (data lake-based) OneLake (optimized tables) OneLake (relational tables) Schema Enforcement Schema-on-read (flexible) Schema-on-write (strict) Schema-on-write Performance Good for large-scale data, not fastest Highly optimized for analytics (fast queries) Moderate, optimized for transactional access Data Type Semi-structured, unstructured, structured Structured only Structured only Best For Big data, ML pipelines, raw data BI, dashboards, OLAP workloads OLTP workloads, simple app-level storage Compute Engine Spark-based engine SQL-based compute (dedicated pools) Lightweight SQL engine Real-Time Queries Limited support Strong support Moderate support Integration with BI Good, but may need performance tuning Excellent (Power BI native) Moderate Versioning &amp; Lineage Built-in Delta Lake versioning Limited Limited Data Volume Petabyte-scale Terabyte-scale (optimized) Gigabyte-scale Cost Efficiency Efficient for storage-heavy, compute-light Efficient for performance Efficient for lightweight operations Use Cases Data science, ML, raw ingestion, archiving Business intelligence, reporting, dashboards Lightweight apps, transactional needs <ul> <li> <p>Choose a data warehouse when you need an enterprise-scale solution with open standard format, no knobs performance, and minimal setup. Best suited for semi-structured and structured data formats, the data warehouse is suitable for both beginner and experienced data professionals, offering simple and intuitive experiences.</p> </li> <li> <p>Choose a lakehouse when you need a large repository of highly unstructured data from heterogeneous sources, leveraging low-cost object storage and want to use SPARK as your primary development tool. Acting as a 'lightweight' data warehouse, you always have the option to use the SQL endpoint and T-SQL tools to deliver reporting and data intelligence scenarios in your lakehouse.</p> </li> </ul>"},{"location":"terminology/store_data/#semantic-model","title":"Semantic Model","text":"<ul> <li> <p>A semantic model represents a source of data ready for reporting, visualization, discovery, and consumption.</p> </li> <li> <p>Typically a star schema with facts that represent a domain, and dimensions that allow you to analyze, or slice and dice the domain to drill down, filter, and calculate different analyses.</p> </li> <li> <p>The semantic model is created automatically for you, and you choose which tables, relationships, and measures are to be added, and the business logic gets inherited from the parent lakehouse or Warehouse respectively.</p> </li> </ul> <p>For more information, refer to Using Fabric Tools: Semantic Model.</p> <p> Get Data</p> <p>Prepare Data </p>"}]}